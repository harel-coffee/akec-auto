index,Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
0,Depicting Decision-Making: A Type-2 Fuzzy Logic Based Explainable Artificial Intelligence System for Goal-Driven Simulation in the Workforce Allocation Domain,E. Ferreyra; H. Hagras; M. Kern; G. Owusu,"The Computational Intelligence Centre, School of Computer Science and Electronic Engineering, University of Essex, Colchester, UK; The Computational Intelligence Centre, School of Computer Science and Electronic Engineering, University of Essex, Colchester, UK; Applied Research, BT, Adastral Park, Martlesham Heath, Ipswich, UK; Applied Research, BT, Adastral Park, Martlesham Heath, Ipswich, UK",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2019,,,1,6,"The recent years have witnessed a growing anticipation for the positive transformation of industries which adopt Artificial Intelligence (AI) for the core areas of their business activities. However, the effectiveness and reliability of such AI systems must comprise the ability to explain their data acquisition, the underlying algorithms operations and the final decisions to stakeholders, including regulators, risk managers, supervisors and end-users among others. There are plenty of areas where Explainable AI (XAI) holds the promise to be a major disruptor. Particularly, in Telecommunication Service Providers (TSPs) which is a core business activity relating to the workforce allocation domain, which, involves costly and time-consuming scheduling processes. This paper focuses on the construction of an XAI framework to assist workforce allocation based on a big bang- big crunch interval type-2 fuzzy logic system (BB-BC IT2FLS) for modelling and scaling goal-driven simulation (GDS) problems, specifically within the telecommunications industry. The obtained results reported the proposed XAI system produces similar results to opaque box models like Neural Networks (NNs) and LSTM Recurrent NNs while being able to explain the decision and operation of the employed system.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8858933,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858933,Explainable AI;interval type-2 fuzzy logic systems;goal-driven simulation;big bang-big crunch,Artificial intelligence;Fuzzy logic;Resource management;Fuzzy sets;Uncertainty;Computational modeling,artificial intelligence;business data processing;decision making;fuzzy logic;fuzzy set theory;fuzzy systems;recurrent neural nets;risk management;telecommunication services,workforce allocation domain;growing anticipation;business activities;AI systems;data acquisition;Explainable AI;BB-BC IT2FLS;XAI system;telecommunication service providers;scheduling processes;decision-making depiction;type-2 fuzzy logic based explainable artificial intelligence system;goal-driven simulation modelling;TSPs;big bang- big crunch interval type-2 fuzzy logic system;LSTM recurrent NNs;GDS;neural networks,,,32.0,,,,,IEEE,IEEE Conferences
1,Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI),A. Adadi; M. Berrada,"Computer and Interdisciplinary Physics Laboratory, Sidi Mohammed Ben Abdellah University, Fez, Morocco; Computer and Interdisciplinary Physics Laboratory, Sidi Mohammed Ben Abdellah University, Fez, Morocco",IEEE Access,,2018,6,,52138,52160,"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.",2169-3536,,10.1109/ACCESS.2018.2870052,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466590,Explainable artificial intelligence;interpretable machine learning;black-box models,Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling,artificial intelligence,AI-based systems;black-box nature;explainable AI;XAI;explainable artificial intelligence;fourth industrial revolution,,18.0,180.0,,,,,IEEE,IEEE Journals
2,explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning,T. Spinner; U. Schlegel; H. Schäfer; M. El-Assady,University of Konstanz; University of Konstanz; University of Konstanz; University of Konstanz,IEEE Transactions on Visualization and Computer Graphics,,2020,26,1.0,1064,1074,"We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.",1941-0506,,10.1109/TVCG.2019.2934629,European Union's Horizon 2020 research and innovation programme; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807299,Explainable AI;Interactive Machine Learning;Deep Learning;Visual Analytics;Interpretability;Explainability,Data models;Analytical models;Computational modeling;Pipelines;Machine learning;Monitoring,data analysis;data visualisation;interactive systems;learning (artificial intelligence),explAIner;visual analytics framework;interactive machine learning;explainable machine learning;AI methods;machine learning process;XAI pipeline,,2.0,86.0,,,,,IEEE,IEEE Journals
3,Improvement in Deep Networks for Optimization Using eXplainable Artificial Intelligence,J. h. Lee; I. h. Shin; S. g. Jeong; S. Lee; M. Z. Zaheer; B. Seo,University of Science and Technology; University of Science and Technology; University of Science and Technology; University of Science and Technology; University of Science and Technology; University of Science and Technology,2019 International Conference on Information and Communication Technology Convergence (ICTC),,2019,,,525,530,"With the recent advancements in the field of explainable artificial intelligence (XAI), which covers the domain of turning deep network architectures from black boxes to comprehensible structures, it became easy to understand what goes on inside a network when it predicts an output. Many researchers have successfully shown the `thought process' behind a network's decision making. This rich and interesting information has not been utilized beyond the scope of visualizations once the training finish. In this work, a novel idea to utilize this insight to the network as a training parameter is proposed. Layer-wise Relevance Propagation (LRP), which obtains the effect of each neuron towards the output of the whole network, is used as a parameter, along with learning rate and network weights, to optimize the training. Various intuitive formulations have been proposed, and the results of the experiments on MNIST and CIFAR-10 datasets have been reported in this paper. Our proposed methodologies show better or comparable performances against conventional optimization algorithms. This would open a new dimension of research to explore the possibility of using XAI in optimizing the training of neural networks.",2162-1233,978-1-7281-0893-3,10.1109/ICTC46691.2019.8939943,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939943,explainable artificial intelligence;neural network;optimization;layer-wise relevance propagation,,artificial intelligence;backpropagation;decision making;mathematics computing;neural nets;optimisation,decision making;Layer-wise Relevance Propagation;optimization algorithms;XAI;neural networks;deep networks;explainable artificial intelligence;deep network architectures;data visualizations;LRP;MNIST dataset;CIFAR-10 dataset,,,24.0,,,,,IEEE,IEEE Conferences
4,Explainable artificial intelligence: A survey,F. K. Došilović; M. Brčić; N. Hlupić,"University of Zagreb Faculty of Electrical Engineering and Computing, Zagreb, Croatia; University of Zagreb Faculty of Electrical Engineering and Computing, Zagreb, Croatia; University of Zagreb Faculty of Electrical Engineering and Computing, Zagreb, Croatia","2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",,2018,,,0210,0215,"In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.",,978-953-233-095-3,10.23919/MIPRO.2018.8400040,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8400040,explainable artificial intelligence;interpretability;explainability;comprehensibility,Predictive models;Machine learning;Support vector machines;Decision trees;Supervised learning;Optimization,learning (artificial intelligence),interpretability;healthcare;finance;explainable artificial intelligence;XAI;recent developments;supervised learning;artificial general intelligence;datasets;computing power;machine learning systems;(super)human performance;image recognition;speech analysis;strategic game planning;state-of-the-art models;transparency,,1.0,54.0,,,,,IEEE,IEEE Conferences
5,Explainable AI for Designers: A Human-Centered Perspective on Mixed-Initiative Co-Creation,J. Zhu; A. Liapis; S. Risi; R. Bidarra; G. M. Youngblood,"Drexel University, Philadelphia, USA; University of Malta, Msida, Malta; IT University of Copenhagen, Copenhagen, Denmark; Delft University of Technology, Delft, The Netherlands; Palo Alto Research Center, California, USA",2018 IEEE Conference on Computational Intelligence and Games (CIG),,2018,,,1,8,"Growing interest in eXplainable Artificial Intelligence (XAI) aims to make AI and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable AI for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the AI techniques and users' needs, and we identify key open challenges.",2325-4289,978-1-5386-4359-4,10.1109/CIG.2018.8490433,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8490433,explainable artificial intelligence;mixed-initiative co-creation;human-computer interaction;machine learning;game design,Games;Task analysis;Machine learning;Neurons;Visualization;Tools,computer games;human computer interaction;learning (artificial intelligence),game designers;AI/ML techniques;human-centered perspective;AI machine;human-centered approach;explainable artificial intelligence;mixed-initiative co-creation;XAI;machine learning;explainable AI for designers;XAID framework,,2.0,34.0,,,,,IEEE,IEEE Conferences
6,Explainable A.I.: The Promise of Genetic Programming Multi-run Subtree Encapsulation,D. Howard; M. A. Edwards,"Howard Sci. Ltd., Malvern, UK; Howard Sci. Ltd., Malvern, UK",2018 International Conference on Machine Learning and Data Engineering (iCMLDE),,2018,,,158,159,"Deep Learning and other Artificial Neural Network based solutions are rarely transparent, and white-box solutions are often called for. This paper explains how Multirun Subtree Encapsulation can provide equivalent white box solutions to facilitate Explainable Artificial Intelligence.",,978-1-7281-0404-1,10.1109/iCMLDE.2018.00037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614020,Explainable Artificial Intelligence;A.I.;Genetic Programming;Evolutionary Computation;modularization;Subtree Encapsulation;Automatically Defined Functions;Software Evolution;white box;black box;expression simplification;Deep Learning;Artificial Neural Networks;Multirun Subtree Encapsulation;subtree database,Encapsulation;Databases;Genetic programming;Standards;Deep learning;Artificial neural networks,data encapsulation;genetic algorithms;learning (artificial intelligence);neural nets;trees (mathematics),white-box solutions;artificial neural network;explainable artificial intelligence;genetic programming;deep learning;multirun subtree encapsulation;explainable AI,,,5.0,,,,,IEEE,IEEE Conferences
7,Explainable Machine Learning for Scientific Insights and Discoveries,R. Roscher; B. Bohn; M. F. Duarte; J. Garcke,"Institute of Geodesy and Geoinformation, University of Bonn, Bonn, Germany; Institute for Numerical Simulation, University of Bonn, Bonn, Germany; Department of Electrical and Computer Engineering, University of Massachusetts Amherst, Amherst, MA, USA; Institute for Numerical Simulation, University of Bonn, Bonn, Germany",IEEE Access,,2020,8,,42200,42216,"Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.",2169-3536,,10.1109/ACCESS.2020.2976199,Fraunhofer Cluster of Excellence Cognitive Internet Technologies; Deutsche Forschungsgemeinschaft; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007737,Explainable machine learning;informed machine learning;interpretability;scientific consistency;transparency,Machine learning;Data models;Mathematical model;Kernel;Biological system modeling;Approximation algorithms;Data mining,learning (artificial intelligence);natural sciences computing,scientific insights;scientific discoveries;machine learning methods;natural sciences;scientific outcome;explainability;scientific consistency,,,117.0,CCBY,,,,IEEE,IEEE Journals
8,Evolving Rule-Based Explainable Artificial Intelligence for Unmanned Aerial Vehicles,B. M. Keneni; D. Kaur; A. Al Bataineh; V. K. Devabhaktuni; A. Y. Javaid; J. D. Zaientz; R. P. Marinier,"Electrical Engineering and Computer Science Department, The University of Toledo, Toledo, OH, USA; Electrical Engineering and Computer Science Department, The University of Toledo, Toledo, OH, USA; Electrical Engineering and Computer Science Department, The University of Toledo, Toledo, OH, USA; Electrical Engineering and Computer Science Department, The University of Toledo, Toledo, OH, USA; Electrical Engineering and Computer Science Department, The University of Toledo, Toledo, OH, USA; Soar Technology, Inc., Ann Arbor, MI, USA; Soar Technology, Inc., Ann Arbor, MI, USA",IEEE Access,,2019,7,,17001,17016,"In this paper, an explainable intelligence model that gives the logic behind the decisions unmanned aerial vehicle (UAV) makes when it is on a predefined mission and chooses to deviate from its designated path is developed. The explainable model is on a visual platform in the format of if-then rules derived from the Sugeno-type fuzzy inference model. The model is tested using the data recorded from three different missions. In each mission, adverse weather, conditions and enemy locations are introduced at random locations along the path of the mission. There are two phases to the model development. In the first phase, the Mamdani fuzzy model is used to create rules to steer the UAV along the designated mission and the rules of engagement when it encounters weather and enemy locations along and near its chosen mission. The data are gathered as UAV traverses on each mission. In the second phase, the data gathered from these missions are used to create a reverse model using a Sugeno-type fuzzy inference system based on the subtractive clustering in the data. The model has seven inputs (time, x-coordinate, y-coordinate, heading direction, engage in attack, continue mission, and steer UAV) and two outputs (weather conditions and distance from the enemy). This model predicts the outputs regarding the weather conditions and enemy positions whenever UAV deviates from the predefined path. The model is optimized with respect to the number of rules and prediction accuracy by adjusting subtractive clustering parameters. The model is then fine-tuned with ANFIS. The final model has six rules and root mean square error value that is less than 0.05. Furthermore, to check the robustness of the model, the Gaussian random noise is added to a UAV path, and the prediction accuracy is validated.",2169-3536,,10.1109/ACCESS.2019.2893141,"Soar Technology, Inc.; Air Force Research Laboratory; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8612916,Explainable artificial intelligence (XAI);fuzzy logic;ANFIS;unmanned aerial vehicle (UAV);subtractive clustering,Fuzzy logic;Unmanned aerial vehicles;Predictive models;Machine learning;Data models;Fuzzy sets;Atmospheric modeling,autonomous aerial vehicles;fuzzy control;fuzzy logic;fuzzy reasoning;fuzzy set theory;fuzzy systems;inference mechanisms;knowledge based systems;mean square error methods;path planning,unmanned aerial vehicles;explainable intelligence model;Sugeno-type fuzzy inference model;different missions;enemy locations;model development;Mamdani fuzzy model;designated mission;chosen mission;UAV traverses;reverse model;Sugeno-type fuzzy inference system;steer UAV;weather conditions;UAV deviates;predefined path;final model;UAV path;evolving rule-based explainable artificial intelligence,,4.0,40.0,,,,,IEEE,IEEE Journals
9,"Evolutionary Fuzzy Systems for Explainable Artificial Intelligence: Why, When, What for, and Where to?",A. Fernandez; F. Herrera; O. Cordon; M. Jose del Jesus; F. Marcelloni,"Data Science and Computational Intelligence, University of Granada, Granada, Spain; Data Science and Computational Intelligence, University of Granada, Granada, Spain; Data Science and Computational Intelligence, University of Granada, Granada, Spain; Data Science and Computational Intelligence, University of Jaen, Spain; University of Pisa, Italy",IEEE Computational Intelligence Magazine,,2019,14,1.0,69,81,"Evolutionary fuzzy systems are one of the greatest advances within the area of computational intelligence. They consist of evolutionary algorithms applied to the design of fuzzy systems. Thanks to this hybridization, superb abilities are provided to fuzzy modeling in many different data science scenarios. This contribution is intended to comprise a position paper developing a comprehensive analysis of the evolutionary fuzzy systems research field. To this end, the ""4 W"" questions are posed and addressed with the aim of understanding the current context of this topic and its significance. Specifically, it will be pointed out why evolutionary fuzzy systems are important from an explainable point of view, when they began, what they are used for, and where the attention of researchers should be directed to in the near future in this area. They must play an important role for the emerging area of eXplainable Artificial Intelligence (XAI) learning from data.",1556-6048,,10.1109/MCI.2018.2881645,Ministerio de Ciencia y Tecnología; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610271,,"Fuzzy systems;Data models;Data science;Fuzzy sets;Computational modeling;Task analysis;Genetic algorithms;Zadeh, Lotfi",data handling;evolutionary computation;expert systems;fuzzy set theory;fuzzy systems;learning (artificial intelligence),explainable Artificial Intelligence;evolutionary algorithms;fuzzy modeling;eXplainable Artificial Intelligence;evolutionary fuzzy systems;computational intelligence;data science;4W questions,,3.0,81.0,,,,,IEEE,IEEE Magazines
10,Explainable AI For Dataset Comparison,A. Jain; J. Keller; M. Popescu,"Department of EECS, University of Missouri, Columbia, MO, USA; Department of EECS, University of Missouri, Columbia, MO, USA; Department of EECS, University of Missouri, Columbia, MO, USA",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2019,,,1,7,"With the increasing use of intelligent systems to make sense of data, lately, explainable AI systems are gaining a lot of traction. A distance measure that can distinguish data sets in linguistic terms can help AI systems in achieving explainability. We make use of Linguistic Protoform Summaries in tandem with Fuzzy Rules to design a system that can compare datasets numerically, as well as explain the difference in Natural Language. We validate our method with the help of synthetic data and show that it produces high correlation with the well-known Euclidean distance measure. We also employ the proposed method to explain changes in daily pulse rate measurements of an elderly resident living in a sensor equipped smart home. We postulate that the method will help in future endeavors to produce explainable pattern recognition systems.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8858911,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858911,Explainable AI;Distance measure;Linguistic Protoform Summaries;Fuzzy Rules;Natural Language Generation,Linguistics;Artificial intelligence;Senior citizens;Histograms;Fuzzy logic,artificial intelligence;biomedical measurement;distance measurement;fuzzy set theory;geriatrics;home automation;medical computing;natural language processing,explainable AI systems;linguistic terms;Linguistic Protoform Summaries;Fuzzy Rules;Natural Language;synthetic data;Euclidean distance measure;daily pulse rate measurements;explainable pattern recognition systems;dataset comparison;intelligent systems;sensor equipped smart home,,,13.0,,,,,IEEE,IEEE Conferences
11,XAI-CBIR: Explainable AI System for Content based Retrieval of Video Frames from Minimally Invasive Surgery Videos,D. R. Chittajallu; B. Dong; P. Tunison; R. Collins; K. Wells; J. Fleshman; G. Sankaranarayanan; S. Schwaitzberg; L. Cavuoto; A. Enquobahrie,"Kitware Inc., Carrboro, NC, USA; Kitware Inc., Carrboro, NC, USA; Kitware Inc., Carrboro, NC, USA; Kitware Inc., Carrboro, NC, USA; Baylor University Medical Center, Dallas, TX; Baylor University Medical Center, Dallas, TX; Baylor University Medical Center, Dallas, TX; University At Buffalo, Buffalo, NY; University At Buffalo, Buffalo, NY; Kitware Inc., Carrboro, NC, USA",2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),,2019,,,66,69,"In this paper, we present a human-in-the-loop explainable AI (XAI) system for content based image retrieval (CBIR) of video frames similar to a query image from minimally invasive surgery (MIS) videos for surgical education. It extracts semantic descriptors from MIS video frames using a self-supervised deep learning model. It then employs an iterative query refinement strategy where in a binary classifier trained online based on relevance feedback from the user is used to iteratively refine the search results. Lastly, it uses an XAI technique to generate a saliency map that provides a visual explanation of why the system considers a retrieved image to be similar to the query image. We evaluated the proposed XAI-CBIR system on the public Cholec80 dataset containing 80 videos of minimally invasive cholecystectomy surgeries with encouraging results.",1945-8452,978-1-5386-3641-1,10.1109/ISBI.2019.8759428,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759428,Minimally invasive surgery;Content based video retrieval;Deep learning;Explainable AI;Surgical data science;Laparoscopy;Self-supervised learning,Semantics;Content-based retrieval;Training;Deep learning;Minimally invasive surgery,content-based retrieval;feature extraction;image classification;image retrieval;learning (artificial intelligence);medical image processing;relevance feedback;surgery;video signal processing,minimally invasive surgery videos;human-in-the-loop explainable AI system;content based image retrieval;query image;surgical education;semantic descriptors;MIS video frames;self-supervised deep learning model;iterative query refinement strategy;XAI technique;retrieved image;XAI-CBIR system;binary classifier;relevance feedback;public Cholec80 dataset;minimally invasive cholecystectomy surgeries;visual explanation,,,15.0,,,,,IEEE,IEEE Conferences
13,On the Need of Interpretability for Biomedical Applications: Using Fuzzy Models for Lung Cancer Prediction with Liquid Biopsy,N. Potie; S. Giannoukakos; M. Hackenberg; A. Fernandez,"Dept. of Computer Science and A.I., University of Granada, Granada, Spain; Dept. of Genetics, University of Granada, Granada, Spain; Dept. of Genetics, University of Granada, Granada, Spain; Dept. of Computer Science and A.I., University of Granada, Granada, Spain",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2019,,,1,6,"In the latter years, we are witnessing a movement from the standard Data Mining towards a more profitable and challenging scenario known as Data Science. It can be defined as a set of quantitative and qualitative approaches that are applied to current relevant problems. In order to be able to ""dig"" to the deepest level considering the whole information available, the knowledge domain and the analysis of the data must have a strong synergy.There are many fields of application where it is necessary, if not essential, to give an explanation of the phenomenon under study. It is no longer enough to simply apply a Machine Learning model, but it must be comprehensible in order to provide a real decision support system. For this reason, a strong movement has emerged in favour of the eXplainable Artificial Intelligence that aims to respond to the ""how"" and ""why"" of the operation of automatic models.In this work, our objective is to show the benefits of one of the learning paradigms of Computational Intelligence: Fuzzy Rule Based Systems and Evolutionary Fuzzy Systems. To this end, we focus on biomedical applications by presenting a case study based on lung cancer prediction from samples taken by liquid biopsy. Liquid biopsy enable us to study genomic alterations for each individual independently, a step towards personalised medicine. The results show the goodness of the solution based on Evolutionary Fuzzy Systems in terms of interpretability and comprehensibility, obtaining a low number of rules with less than 3 fuzzy linguistic labels per antecedent.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8858976,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858976,eXplainable Artificial Intelligence;Evolutionary Fuzzy Systems;Lung Cancer;Liquid Biopsy;Interpretability,Cancer;Lung;Biopsy;Liquids;Blood;Tumors;Data mining,cancer;data mining;decision support systems;evolutionary computation;fuzzy set theory;fuzzy systems;genomics;knowledge based systems;learning (artificial intelligence);lung;medical computing,Computational Intelligence;Fuzzy Rule Based Systems;Evolutionary Fuzzy Systems;biomedical applications;lung cancer prediction;liquid biopsy;3 fuzzy linguistic labels;fuzzy models;standard Data Mining;Data Science;Machine Learning model;decision support system;eXplainable Artificial Intelligence;genomic alterations,,,29.0,,,,,IEEE,IEEE Conferences
14,Design of an explainable machine learning challenge for video interviews,H. J. Escalante; I. Guyon; S. Escalera; J. Jacques; M. Madadi; X. Baró; S. Ayache; E. Viegas; Y. Güçlütürk; U. Güçlü; M. A. J. van Gerven; R. van Lier,"ChaLearn, California, USA; ChaLearn, California, USA; Computer Vision Center, UAB, Barcelona, Spain; Computer Vision Center, UAB, Barcelona, Spain; Computer Vision Center, UAB, Barcelona, Spain; Computer Vision Center, UAB, Barcelona, Spain; Aix Marseille Univ, CNRS, LIF, Marseille, France; Microsoft Research, USA; Radboud University, Donders Institute for Brain, Cognition and Behaviour, Nijmegen, the Netherlands; Radboud University, Donders Institute for Brain, Cognition and Behaviour, Nijmegen, the Netherlands; Radboud University, Donders Institute for Brain, Cognition and Behaviour, Nijmegen, the Netherlands; Radboud University, Donders Institute for Brain, Cognition and Behaviour, Nijmegen, the Netherlands",2017 International Joint Conference on Neural Networks (IJCNN),,2017,,,3688,3695,"This paper reviews and discusses research advances on “explainable machine learning” in computer vision. We focus on a particular area of the “Looking at People” (LAP) thematic domain: first impressions and personality analysis. Our aim is to make the computational intelligence and computer vision communities aware of the importance of developing explanatory mechanisms for computer-assisted decision making applications, such as automating recruitment. Judgments based on personality traits are being made routinely by human resource departments to evaluate the candidates' capacity of social insertion and their potential of career growth. However, inferring personality traits and, in general, the process by which we humans form a first impression of people, is highly subjective and may be biased. Previous studies have demonstrated that learning machines can learn to mimic human decisions. In this paper, we go one step further and formulate the problem of explaining the decisions of the models as a means of identifying what visual aspects are important, understanding how they relate to decisions suggested, and possibly gaining insight into undesirable negative biases. We design a new challenge on explainability of learning machines for first impressions analysis. We describe the setting, scenario, evaluation metrics and preliminary outcomes of the competition. To the best of our knowledge this is the first effort in terms of challenges for explainability in computer vision. In addition our challenge design comprises several other quantitative and qualitative elements of novelty, including a “coopetition” setting, which combines competition and collaboration.",2161-4407,978-1-5090-6182-2,10.1109/IJCNN.2017.7966320,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966320,,Computational modeling;Computer vision;Machine learning;Interviews;Predictive models;Natural language processing;Analytical models,computer vision;decision making;learning (artificial intelligence);recruitment;video signal processing,explainable machine learning challenge;video interviews;computer vision communities;Looking at People thematic domain;LAP thematic domain;first impressions analysis;personality analysis;computational intelligence;explanatory mechanisms;computer-assisted decision making applications;recruitment automation;personality traits;undesirable negative biases;learning machines;coopetition setting,,7.0,48.0,,,,,IEEE,IEEE Conferences
15,"Toward Human-Understandable, Explainable AI",H. Hagras,University of Essex,Computer,,2018,51,9.0,28,36,"Recent increases in computing power, coupled with rapid growth in the availability and quantity of data have rekindled our interest in the theory and applications of artificial intelligence (AI). However, for AI to be confidently rolled out by industries and governments, users want greater transparency through explainable AI (XAI) systems. The author introduces XAI concepts, and gives an overview of areas in need of further exploration-such as type-2 fuzzy logic systems-to ensure such systems can be fully understood and analyzed by the lay user.",1558-0814,,10.1109/MC.2018.3620965,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481251,Future of AI;artificial intelligence;AI;intelligent systems;explainable artificial intelligence;machine leaning;Type-2 Fuzzy Logic Systems,Artificial intelligence;Machine learning;Learning systems;Fuzzy logic;Intelligent systems,artificial intelligence;fuzzy logic,computing power;artificial intelligence;XAI;type-2 fuzzy logic systems;AI systems;human-understandable,,6.0,19.0,,,,,IEEE,IEEE Magazines
16,An Explainable Artificial Intelligence Model for Clustering Numerical Databases,O. Loyola-González; A. E. Gutierrez-Rodríguez; M. A. Medina-Pérez; R. Monroy; J. F. Martínez-Trinidad; J. A. Carrasco-Ochoa; M. García-Borroto,"Tecnologico de Monterrey, Puebla, Mexico; Tecnologico de Monterrey, San Antonio Buenavista, Mexico; Tecnologico de Monterrey, Estado de Mexico, Mexico; Tecnologico de Monterrey, Estado de Mexico, Mexico; Instituto Nacional de Astrofísica, Óptica y Electrónica, Puebla, Mexico; Instituto Nacional de Astrofísica, Óptica y Electrónica, Puebla, Mexico; Instituto Superior Politécnico José Antonio Echeverría, Habana, Mexico",IEEE Access,,2020,8,,52370,52384,"Nowadays, the international scientific community of machine learning has an enormous campaign in favor of creating understandable models instead of black-box models. The main reason is that experts in the application area are showing reluctance due to black-box models cannot be understood by them, and consequently, their results are difficult to be explained. In unsupervised problems, where experts have not labeled objects, obtaining an explanation of the results is necessary because specialists in the application area need to understand both the applied model as well as the obtained results for finding the rationale behind each obtained clustering from a practical point of view. Hence, in this paper, we introduce a clustering based on decision trees (eUD3.5), which builds several decision trees from numerical databases. Unlike previous solutions, our proposal takes into account both separation and compactness for evaluating a feature split without decreasing time efficiency and with no empirical parameter to control the depth of the trees. We tested eUD3.5 on 40 numerical databases of UCI Machine Learning Repository, showing that our proposal builds a set of high-quality unsupervised decision trees for clustering, allowing us to obtain the best average ranking compared with other popular state-of-the-art clustering solutions. Also, from the collection of unsupervised decision trees induced by our proposal, a set of high-quality patterns are extracted for showing the main feature-value pairs describing each cluster.",2169-3536,,10.1109/ACCESS.2020.2980581,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035478,Explainable model;clustering;unsupervised decision trees;numerical databases,Decision trees;Clustering algorithms;Databases;Proposals;Numerical models;Machine learning;Biological system modeling,decision trees;pattern classification;pattern clustering;unsupervised learning,explainable artificial intelligence model;international scientific community;black-box models;UCI Machine Learning Repository;numerical database clustering;unsupervised decision trees;eUD3.5,,,68.0,CCBY,,,,IEEE,IEEE Journals
17,Interval Type-2 Fuzzy Logic Based Stacked Autoencoder Deep Neural Network For Generating Explainable AI Models in Workforce Optimization,R. Chimatapu; H. Hagras; A. Starkey; G. Owusu,"The Computational Intelligence Centre, University of Essex, Colchester, UK; The Computational Intelligence Centre, University of Essex, Colchester, UK; Business Modelling and Operational Transformation Practice BT, Ipswich, UK; Business Modelling and Operational Transformation Practice BT, Ipswich, UK",2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2018,,,1,8,"In Utility based industries that employ a large mobile workforce, efficient utilization of field engineers is key to optimal service delivery. The utilization of the engineers can be improved by predicting the future performance of work areas by using machine learning tools such as Deep Neural Networks (DNNs).The dramatic success of DNNs has led to an explosion of its applications. However, the effectiveness of DNNs can be limited by the inability to explain how the models arrived at their predictions.In this paper, we present a novel Type-2 Fuzzy Logic System (FLS) whose inputs are preprocessed by a Stacked Autoencoder Neural Network to add some interpretability to a Deep Neural Network model. The proposed type-2 FLS will contain a small rule set with a small number of antecedents per rule to maximize the model's interpretability. We also present an algorithm which can be used to efficiently train the proposed model.We will compare the proposed model with a Standard Stacked Autoencoder Deep Neural Network, a Multi-Layer Perceptron (MLP) neural network and an Interval Type-2 Fuzzy Logic System.The results show that even though the Standard Stacked Autoencoder and MLP Neural Networks have better performance, they do not provide any insight into the reasoning behind the predictions. The Proposed model, on the other hand, provides better result than the standalone type-2 FLS and a comparable performance to the neural networks and provides a little bit of insight into the decision-making process. Without this insight, we cannot be sure why there is a drop in the performance and we need to further analyze the WA before we can take any decision. This leads to quicker decision making and potentially improving the efficiency of the engineers.",,978-1-5090-6020-7,10.1109/FUZZ-IEEE.2018.8491679,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8491679,Type-2 fuzzy logic;Big Bang - Big Crunch;Deep Neural Networks;Explainable Artificial Intelligence,Fuzzy logic;Neural networks;Fuzzy sets;Optimization;Task analysis;Predictive models,decision making;fuzzy logic;fuzzy set theory;learning (artificial intelligence);multilayer perceptrons;optimisation,workforce optimization;mobile workforce;field engineers;optimal service delivery;machine learning tools;DNNs;standalone type-2 FLS;multilayer perceptron neural network;interval type-2 fuzzy logic system;MLP neural networks;utility based industries;explainable AI models;standard stacked autoencoder deep neural network;decision-making process,,,30.0,,,,,IEEE,IEEE Conferences
18,LEAFAGE: Example-based and Feature importance-based Explanations for Black-box ML models,A. Adhikari; D. M. J. Tax; R. Satta; M. Faeth,"Data Science Department, TNO, The Hague, The Netherlands; EEMCS Delft University of Technology, Delft, The Netherlands; Data Science Department, TNO, The Hague, The Netherlands; Data Science Department, TNO, The Hague, The Netherlands",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2019,,,1,7,"Explainable Artificial Intelligence (XAI) is an emergent research field which tries to cope with the lack of transparency of AI systems, by providing human understandable explanations for the underlying Machine Learning models. This work presents a new explanation extraction method called LEAFAGE. Explanations are provided both in terms of feature importance and of similar classification examples. The latter is a well known strategy for problem solving and justification in social science. LEAFAGE leverages on the fact that the reasoning behind a single decision/prediction for a single data point is generally simpler to understand than the complete model; it produces explanations by generating simpler yet locally accurate approximations of the original model. LEAFAGE performs overall better than the current state of the art in terms of fidelity of the model approximation, in particular when Machine Learning models with non-linear decision boundaries are analysed. LEAFAGE was also tested in terms of usefulness for the user, an aspect still largely overlooked in the scientific literature. Results show interesting and partly counter-intuitive findings, such as the fact that providing no explanation is sometimes better than providing certain kinds of explanation.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8858846,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858846,eXplainable AI;example-based reasoning;empirical study,Cognition;Training;Data models;Predictive models;Data science;Machine learning;Euclidean distance,feature extraction;learning (artificial intelligence);pattern classification,AI systems;human understandable explanations;machine learning models;explanation extraction method;similar classification examples;LEAFAGE leverages;single data point;locally accurate approximations;model approximation;nonlinear decision boundaries;example-based explanations;feature importance-based explanations;black-box ML models;explainable artificial intelligence;emergent research field,,,18.0,,,,,IEEE,IEEE Conferences
19,Visualization of Deep Reinforcement Learning using Grad-CAM: How AI Plays Atari Games?,H. Joo; K. Kim,"Institute of Integrated Technology GIST, Gwangju, South Korea; Institute of Integrated Technology GIST, Gwangju, South Korea",2019 IEEE Conference on Games (CoG),,2019,,,1,2,"Deep Reinforcement Learning (DRL) allows agents to learn strategies to solve complex tasks. It has been applied to solve various problems such as natural language processing, games, etc. However, it is still difficult to apply DRL to certain real-world problems because each action is not predictable, and we cannot know why the results are coming out. For this reason, a technology called eXplainable Artificial Intelligence (XAI) has been recently developed. As this technology shows a visualization of the AI process, people can easily understand the results of AI. In this paper, we proposed to use Grad-CAM, one of the XAI techniques, when we visualize the behaviors of AI players trained by DRL. Our experimental results show which part of the input state is focused on when one well-trained agent takes action.",2325-4289,978-1-7281-1884-0,10.1109/CIG.2019.8847950,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8847950,Explainable AI;Grad-CAM;Deep Reinforcement Learning,Visualization;Games;Reinforcement learning;Task analysis;Natural language processing;Insurance,computer games;learning (artificial intelligence);multi-agent systems;neural nets,DRL;AI process;Grad-CAM;AI players;explainable artificial intelligence;Atari games;deep reinforcement learning;XAI,,,5.0,,,,,IEEE,IEEE Conferences
20,Explainable AI for Understanding Decisions and Data-Driven Optimization of the Choquet Integral,B. Murray; M. A. Islam; A. J. Pinar; T. C. Havens; D. T. Anderson; G. Scott,"Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA; Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA; Electrical and Computer Engineering, Michigan Technological University, Houghton, MI, USA; Electrical and Computer Engineering, Michigan Technological University, Houghton, MI, USA; Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA",2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2018,,,1,8,"To date, numerous ways have been created to learn a fusion solution from data. However, a gap exists in terms of understanding the quality of what was learned and how trustworthy the fusion is for future-i.e., new-data. In part, the current paper is driven by the demand for so-called explainable AI (XAI). Herein, we discuss methods for XAI of the Choquet integral (ChI), a parametric nonlinear aggregation function. Specifically, we review existing indices, and we introduce new data-centric XAI tools. These various XAI-ChI methods are explored in the context of fusing a set of heterogeneous deep convolutional neural networks for remote sensing.",,978-1-5090-6020-7,10.1109/FUZZ-IEEE.2018.8491501,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8491501,Choquet Integral;Fuzzy Integral;Explainable AI;Machine Learning,Frequency modulation;Indexes;Remote sensing;Optimization;Artificial intelligence;Electronic mail;Convolutional neural networks,convolution;feedforward neural nets;learning (artificial intelligence);optimisation;remote sensing;sensor fusion,data-driven optimization;Choquet integral;fusion solution;parametric nonlinear aggregation function;data-centric XAI tools;XAI-ChI methods;explainable AI;heterogeneous deep convolutional neural networks;remote sensing,,1.0,26.0,,,,,IEEE,IEEE Conferences
22,Integrating Machine Learning with Symbolic Reasoning to Build an Explainable AI Model for Stroke Prediction,N. Prentzas; A. Nicolaides; E. Kyriacou; A. Kakas; C. Pattichis,"University of Cyprus; University of Cyprus; Frederic University, Cyprus; University of Cyprus; University of Cyprus",2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE),,2019,,,817,821,"Despite the recent recognition of the value of Artificial Intelligence and Machine Learning in healthcare, barriers to further adoption remain, mainly due to their ""black box"" nature and the algorithm's inability to explain its results. In this paper we present and propose a methodology of applying argumentation on top of machine learning to build explainable AI (XAI) models. We compare our results with Random Forests and an SVM classifier that was considered best for the same dataset in [1].",2471-7819,978-1-7281-4617-1,10.1109/BIBE.2019.00152,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941679,"argumentation, explainability, inTrees, random forests, XAI",,health care;learning (artificial intelligence);support vector machines,machine learning;symbolic reasoning;explainable AI model;stroke prediction;artificial intelligence;black box nature;explainable AI models;healthcare;random forests;SVM classifier,,,13.0,,,,,IEEE,IEEE Conferences
23,An Adversarial Approach for Explainable AI in Intrusion Detection Systems,D. L. Marino; C. S. Wickramasinghe; M. Manic,"Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA",IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,,2018,,,3237,3243,"Despite the growing popularity of modern machine learning techniques (e.g, Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs) An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.",2577-1647,978-1-5090-6684-1,10.1109/IECON.2018.8591457,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8591457,Adversarial Machine Learning;Adversarial samples;Explainable AI;cyber-security,Machine learning;Intrusion detection;Mathematical model;Visualization;Estimation,learning (artificial intelligence);multilayer perceptrons;neural nets;pattern classification;security of data,adversarial approach;explainable AI;cyber-security applications;adversarial machine learning;multilayer perceptron classifiers;machine learning techniques;deep neural networks;data-driven intrusion detection systems;IDSs,,5.0,24.0,,,,,IEEE,IEEE Conferences
24,Towards A Rigorous Evaluation Of XAI Methods On Time Series,U. Schlegel; H. Arnout; M. El-Assady; D. Oelke; D. A. Keim,"University of KonstanzKonstanz, Germany; Siemens CT, TU Munich; University of Konstanz; Siemens CT; University of Konstanz",2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,2019,,,4197,4201,"Explainable Artificial Intelligence (XAI) methods are typically deployed to explain and debug black-box machine learning models. However, most proposed XAI methods are black-boxes themselves and designed for images. Thus, they rely on visual interpretability to evaluate and prove explanations. In this work, we apply XAI methods previously used in the image and text-domain on time series. We present a methodology to test and evaluate various XAI methods on time series by introducing new verification techniques to incorporate the temporal dimension. We further conduct preliminary experiments to assess the quality of selected XAI method explanations with various verification methods on a range of datasets and inspecting quality metrics on it. We demonstrate that in our initial experiments, SHAP works robust for all models, but others like DeepLIFT, LRP, and Saliency Maps work better with specific architectures.",2473-9944,978-1-7281-5023-9,10.1109/ICCVW.2019.00516,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022428,Time-Series;explainable-ai;explainable-ai-evaluation,Time series analysis;Perturbation methods;Task analysis;Heating systems;Predictive models;Machine learning;Data models,image processing;learning (artificial intelligence);text analysis;time series,SHAP;image domain;text-domain;black-box machine learning models;explainable artificial intelligence methods;verification methods;XAI methods;time series,,,26.0,,,,,IEEE,IEEE Conferences
25,Learning Fuzzy Relations and Properties for Explainable Artificial Intelligence,R. Pierrard; J. Poli; C. Hudelot,"Data Analysis and System Intelligence Laboratory, CEA, LIST, Gif-sur-Yvette cedex, 91191, France; Data Analysis and System Intelligence Laboratory, CEA, LIST, Gif-sur-Yvette cedex, 91191, France; Mathematics Interacting with Computer Science CentraleSupélec, Paris-Saclay University, Gif-sur-Yvette, 91190, France",2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2018,,,1,8,"The goal of explainable artificial intelligence is to solve problems in a way that humans can understand how it does it. However, few approaches have been proposed so far and some of them lay more emphasis on interpretability than on explainability. In this paper, we propose an approach that is based on learning fuzzy relations and fuzzy properties. We extract frequent relations from a dataset to generate an explained decision. Our approach can deal with different problems, such as classification or annotation. A model was built to perform explained classification on a toy dataset that we generated. It managed to correctly classify examples while providing convincing explanations. A few areas for improvement have been spotted, such as the need to filter relations and properties before or while learning them in order to avoid useless computations.",,978-1-5090-6020-7,10.1109/FUZZ-IEEE.2018.8491538,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8491538,,Fuzzy logic;Task analysis;Decision trees;Training;Knowledge representation;Partitioning algorithms;Neural networks,fuzzy set theory;learning (artificial intelligence);pattern classification,fuzzy relations;fuzzy properties;artificial intelligence;fuzzy relations learning;frequent relations extraction;classification,,2.0,35.0,,,,,IEEE,IEEE Conferences
26,The role of emotion in self-explanations by cognitive agents,F. Kaptein; J. Broekens; K. Hindriks; M. Neerincx,"Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands",2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),,2017,,,88,93,"Artificial Intelligence (AI) systems, including intelligent agents, are becoming increasingly complex. Explainable AI (XAI) is the capability of these systems to explain their behaviour, in a for humans understandable manner. Cognitive agents, a type of intelligent agents, typically explain their actions with their beliefs and desires. However, humans also take into account their own and other's emotions in their explanations, and humans explain their emotions. We refer to using emotions in XAI as Emotion-aware eXplainable Artificial Intelligence (EXAI). Although EXAI should also include awareness of the other's emotions, in this work we focus on how the simulation of emotions in cognitive agents can help them self-explain their behaviour. We argue that emotions simulated based on cognitive appraisal theory enable (1) the explanation of these emotions, (2) using them as a heuristic to identify important beliefs and desires for the explanation, and (3) the use of emotion words in the explanations themselves.",,978-1-5386-0680-3,10.1109/ACIIW.2017.8272595,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8272595,,Artificial intelligence;Appraisal;Computational modeling;Training;Psychology;Games;Cognition,artificial intelligence;cognition;emotion recognition;multi-agent systems;software agents,cognitive agents;intelligent agents;explainable AI;humans understandable manner;explanation;emotion words;artificial intelligence systems;emotion-aware explainable artificial intelligence;EXAI;cognitive appraisal theory,,2.0,36.0,,,,,IEEE,IEEE Conferences
28,IMPROVED EXPLAINABILITY OF CAPSULE NETWORKS: RELEVANCE PATH BY AGREEMENT,A. Shahroudnejad; P. Afshar; K. N. Plataniotis; A. Mohammadi,"Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada",2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP),,2018,,,549,553,"Recent advancements in signal processing domain have resulted in a surge of interest in deep neural networks (DNNs) due to their unprecedented performance and high accuracy for challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in medical applications), it is of paramount importance to understand, trust, and in one word ""explain"" the rational behind deep models' decisions. Generally, DNNs are considered as black-box systems, which do not provide any clue on their internal processing actions. Although some recent efforts have been initiated to explain behavior/decisions of deep networks, explainable artificial intelligence (XAI) domain is still in its infancy. In this regard, we consider capsule networks (referred to as CapsNets), which are novel deep structures; recently proposed as an alternative counterpart to convolutional neural networks (CNNs), and posed to change the future of machine intelligence. In this paper, we investigate and analyze structure and behavior of CapsNets and illustrate potential explainability properties of such networks. Furthermore, we show possibility of transforming deep architectures in to transparent networks via incorporation of capsules in different layers instead of convolution layers of the CNNs.",,978-1-7281-1295-4,10.1109/GlobalSIP.2018.8646474,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8646474,Explainable Machine Learning;Capsule Networks;Deep Neural Networks;Convolutional Neural Networks,Feature extraction;Predictive models;Neural networks;Machine learning;Training;Couplings;Computer architecture,convolutional neural nets;learning (artificial intelligence);signal processing,capsule networks;relevance path;signal processing domain;deep neural networks;unprecedented performance;significant engineering importance;deep learning architectures;critical decisions;human lives;medical applications;deep models;black-box systems;internal processing actions;deep networks;explainable artificial intelligence domain;CapsNets;deep structures;convolutional neural networks;potential explainability properties;deep architectures;transparent networks;DNN;XAI;CNN,,4.0,28.0,,,,,IEEE,IEEE Conferences
29,Interpretable Machine Learning in Healthcare,M. A. Ahmad; A. Teredesai; C. Eckert,NA; NA; NA,2018 IEEE International Conference on Healthcare Informatics (ICHI),,2018,,,447,447,"This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.",2575-2634,978-1-5386-5377-7,10.1109/ICHI.2018.00095,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419428,interpretable machine learning;explainable artificial intelligence,Machine learning;Machine learning algorithms;Prediction algorithms;Tutorials;Predictive models;Cancer,health care;learning (artificial intelligence),interpretable machine learning models;healthcare;interpretable machine learning algorithm,,2.0,0.0,,,,,IEEE,IEEE Conferences
31,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,M. Carletti; C. Masiero; A. Beghi; G. A. Susto,"University of Padova,Department of Information Engineering,Padova,Italy; Statwolf Data Science SRL,Padova,Italy; University of Padova,Department of Information Engineering,Padova,Italy; University of Padova,Department of Information Engineering,Padova,Italy","2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",,2019,,,21,26,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",2577-1655,978-1-7281-4569-3,10.1109/SMC.2019.8913901,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913901,Anomaly Detection;Industry 4.0;Interpretability;Isolation Forest;Machine Learning,Feature extraction;Task analysis;Monitoring;Anomaly detection;Machine learning;Computational modeling;Industries,condition monitoring;design engineering;feature selection;industrial plants;learning (artificial intelligence);process monitoring;production engineering computing;quality control,machine learning;industry 4.0;feature selection;anomaly detection;root cause analysis;industrial plants;quality monitoring;design engineering;isolation forests;smart monitoring solution,,,20.0,,,,,IEEE,IEEE Conferences
33,Explainable Machine Learning for Breast Cancer Diagnosis,T. Brito-Sarracino; M. Rocha dos Santos; E. Freire Antunes; I. Batista de Andrade Santos; J. Coelho Kasmanas; A. C. Ponce de Leon Ferreira de Carvalho,University of São Paulo; University of São Paulo; University of São Paulo; University of São Paulo; University of São Paulo; University of São Paulo,2019 8th Brazilian Conference on Intelligent Systems (BRACIS),,2019,,,681,686,"Cancer is already the leading cause of death in most Brazilian cities and in the world. The understanding of its internal mechanisms and the design of computational models capable of improving its diagnosis will have strong benefits for humanity. New technologies have made available a wealth of data, which can be used to improve the diagnosis of cancer. As a manual analysis of this data is impracticable, many black-box machine learning algorithms have been employed successfully for cancer diagnosis. Despite their high accuracy prediction abilities, black-box models sacrifice transparency and accountability. In contrast, interpretable machine learning algorithms are powerful tools for understanding the underlying mechanism present within a large corpus of data. In this work, Linear Projections and Radviz were used as visualization techniques for data exploration and feature selection. Further, Decision Tree induction algorithms were used to create models that are able to differentiate between Malignant and Benign breast tumors from breast mass images. These models can be considered white-box models which means their inner workings are easier to explain and interpret. The result shows Classification and Regression Trees achieved an accuracy of 96% in predicting breast cancer.",2643-6264,978-1-7281-4253-1,10.1109/BRACIS.2019.00124,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8923961,machine learning;breast cancer diagnosis;white-box model;data visualization,Feature extraction;Breast cancer;Data visualization;Machine learning algorithms;Neural networks;Logistics,cancer;data mining;data visualisation;decision trees;learning (artificial intelligence);medical diagnostic computing;medical image processing;patient diagnosis;pattern classification;regression analysis;tumours,breast cancer diagnosis;Brazilian cities;internal mechanisms;computational models;strong benefits;manual analysis;black-box machine learning algorithms;high accuracy prediction abilities;black-box models sacrifice transparency;accountability;interpretable machine learning algorithms;underlying mechanism;data exploration;feature selection;decision tree induction algorithms;breast mass images;white-box models,,,26.0,,,,,IEEE,IEEE Conferences
35,NOVA - A tool for eXplainable Cooperative Machine Learning,A. Heimerl; T. Baur; F. Lingenfelser; J. Wagner; E. André,"Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany; Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany; Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany; Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany; Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany",2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII),,2019,,,109,115,"In this paper, we introduce a next-generation annotation tool called NOVA, which implements a workflow that interactively incorporates the `human in the loop'. In particular, NOVA offers a collaborative annotation backend where multiple annotators join their workforce. A main aspect of NOVA is the possibility of applying semi-supervised active learning where Machine Learning techniques are used already during the annotation process by giving the possibility to pre-label data automatically. Furthermore, NOVA implements recent eXplainable AI (XAI) techniques to provide users with both, a confidence value of the automatically predicted annotations, as well as visual explanation. This way, annotators get to understand whether they can trust their ML models, or more annotated data is necessary.",2156-8111,978-1-7281-3888-6,10.1109/ACII.2019.8925519,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8925519,annotation tools;cooperative machine learning;explainable AI,3G mobile communication;TV,learning (artificial intelligence),automatically predicted annotations;annotated data;next-generation annotation tool;collaborative annotation backend;active learning;machine learning techniques;annotation process;explainable cooperative machine learning;NOVA tool;explainable AI techniques,,,34.0,,,,,IEEE,IEEE Conferences
36,I see what you did there: Understanding when to trust a ML model with NOVA,T. Baur; A. Heimerl; F. Lingenfelser; E. André,"Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany; Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany; Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany; Human Centered Multimedia Lab, Augsburg University Germany,Augsburg,Germany",2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),,2019,,,77,78,"In this demo paper we present NOVA, a machine learning and explanation interface that focuses on the automated analysis of social interactions. NOVA combines Cooperative Machine Learning (CML) and explainable AI (XAI) methods to reduce manual labelling efforts while simultaneously generating an intuitive understanding of the learning process of a classification system. Therefore, NOVA features a semi-automated labelling process in which users are provided with immediate visual feedback on the predictions, which gives insights into the strengths and weaknesses of the underlying classification system. Following an interactive and exploratory workflow, the performance of the model can be improved by manual revision of the predictions.",,978-1-7281-3891-6,10.1109/ACIIW.2019.8925214,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8925214,annotation tools;cooperative machine learning;explainable AI,Machine learning;Predictive models;Manuals;Task analysis;Data models;Computational modeling;Labeling,learning (artificial intelligence),ML model;NOVA tool;explanation interface;automated analysis;social interactions;manual labelling efforts;intuitive understanding;learning process;cooperative machine learning;explainable AI method,,,3.0,,,,,IEEE,IEEE Conferences
38,Unified framework of Explainable AI to enhance classifier performance,R. Manjunath; B. N. Chandrashekar; B. N. Vinutha; R. Arya; A. Chatterjee,"Wipro technologies,Bangalore,INDIA; Wipro technologies,Bangalore,INDIA; Wipro technologies,Bangalore,INDIA; Wipro technologies,Bangalore,INDIA; Wipro technologies,Bangalore,INDIA",2019 Grace Hopper Celebration India (GHCI),,2019,,,1,5,"Deep learning image classifiers are extensively used in document processing, activity monitoring, object recognition and separations etc. However, even the best classifiers are not free from errors. It would be very helpful if the errors that are pumped in to the system due to the classifier decisions are reduced. The framework comprises of heat map generation, attribute generation, text explanation generation and activation.",,978-1-7281-4264-7,10.1109/GHCI47972.2019.9071811,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071811,Explainable AI;misclassification;Human in loop;classifier;deep learning;Heat map,,image classification;learning (artificial intelligence),deep learning image classifiers;document processing;activity monitoring;heat map generation;text explanation generation;object recognition;attribute generation,,,10.0,,,,,IEEE,IEEE Conferences
40,Nonparametric Learning Via Successive Subspace Modeling (SSM),Y. Chen,"University of Southern California, Los Angeles, California, USA",2019 IEEE International Conference on Image Processing (ICIP),,2019,,,3031,3032,"A novel nonparametric machine learning methodology, called successive subspace modeling (SSM), is proposed in this work. Without loss of generality, we use image classification as an illustrative example. The SSM procedure consists of two stages: 1) feature learning and 2) decision learning. For feature learning, we partition input images into overlapping patches of different sizes recursively. While the input images define a vector space, patches of smaller sizes form a sequence of growing subspaces. From the smallest to the largest subspaces, we build a model for each subspace in a successive manner through the Saab transform. At the end, we obtain a lower-dimensional feature vector space that contains significant spatial-spectral information of input images. For decision learning, we summarize the distribution of feature vectors with two techniques; namely, feature space partitioning and local manifold learning. Then, for every test sample, an ensemble decision is made based on decisions at each of its near clusters. The superior performance of SSM is demonstrated on the MNIST dataset.",2381-8549,978-1-5386-6249-6,10.1109/ICIP.2019.8803352,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803352,Machine Learning;Explainable Machine Learning;Nonparametric Learning;Subspace Modeling;Successive Subspace Modeling,Manifolds;Transforms;Training;Testing;Training data;Backpropagation;Machine learning,feature extraction;image classification;image representation;learning (artificial intelligence);nonparametric statistics;transforms,successive subspace modeling;partitioned input images;ensemble decision;local manifold learning;space partitioning;feature vectors;decision learning;spatial-spectral information;lower-dimensional feature vector space;overlapping patches;feature learning;SSM procedure;image classification;nonparametric machine learning methodology,,,2.0,,,,,IEEE,IEEE Conferences
41,Evaluating Cognitive and Affective Intelligent Agent Explanations in a Long-Term Health-Support Application for Children with Type 1 Diabetes,F. Kaptein; J. Broekens; K. Hindriks; M. Neerincx,"TU Delft,Van Mourik Broekmanweg 6, Delft,XE,The Netherlands,2628; Leiden University, LIACS,Niels Bohrweg 1, Leiden,CA,The Netherlands,2333; VU,De Boelelaan 1105, Amsterdam,HV,The Netherlands,1081; TNO,Postbus 23, Soesterberg,ZG,The Netherlands,3769",2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII),,2019,,,1,7,"Explanation of actions is important for transparency of-, and trust in the decisions of smart systems. Literature suggests that emotions and emotion words - in addition to beliefs and goals - are used in human explanations of behaviour. Furthermore, research in e-health support systems and human-robot interaction stresses the need for studying long-term interaction with users. However, state of the art explainable artificial intelligence for intelligent agents focuses mainly on explaining an agent's behaviour based on the underlying beliefs and goals in short-term experiments. In this paper, we report on a long-term experiment in which we tested the effect of cognitive, affective and lack of explanations on children's motivation to use an e-health support system. Children (aged 6-14) suffering from type 1 diabetes mellitus interacted with a virtual robot as part of the e-health system over a period of 2.5 - 3 months. Children alternated between the three conditions. Agent behaviours that were explained to the children included why 1) the agent asks a certain quiz question; 2) the agent provides a specific tip (a short instruction) about diabetes; or, 3) the agent provides a task suggestion, e.g., play a quiz, or, watch a video about diabetes. Their motivation was measured by counting how often children would follow the agent's suggestion, how often they would continue to play the quiz or ask for an additional tip, and how often they would request an explanation from the system. Surprisingly, children proved to follow task suggestions more often when no explanation was given, while other explanation effects did not appear. This is to our knowledge the first longterm study to report empirical evidence for an agent explanation effect, challenging the next studies to uncover the underlying mechanism.",2156-8111,978-1-7281-3888-6,10.1109/ACII.2019.8925526,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8925526,Explainable AI;Long-term human-agent interaction;Goal-based XAI;Emotions in explanations,Task analysis;Pediatrics;Diabetes;Robots;Artificial intelligence;Intelligent agents;Aging,cognition;cognitive systems;diseases;medical computing;software agents,cognitive intelligent agent explanations;affective intelligent agent explanations;long-term health-support application;children;smart systems;emotion words;e-health support system;human-robot interaction;explainable artificial intelligence;intelligent agents;type 1 diabetes mellitus;e-health system;agent behaviours;task suggestion;agent explanation effect;time 2.5 month to 3.0 month,,,29.0,,,,,IEEE,IEEE Conferences
42,Improved Deep Fuzzy Clustering for Accurate and Interpretable Classifiers,M. Yeganejou; S. Dick,"Dept. of Electrical & Computer Engineering, University of Alberta, Edmonton, Canada; Dept. of Electrical & Computer Engineering, University of Alberta, Edmonton, Canada",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2019,,,1,7,"While deep learning has demonstrated excellent performance in many challenging learning tasks, it has not yet found broad acceptance amongst users. The key deficiency seems to be the uninterpretable nature of a deep neural network; no general, practical method for explaining the predictions or decisions of such a network has been devised. Lacking such, users seem unwilling to entrust deep learning with critical decisions. One approach to generating explanations is to design algorithms that are inherently more interpretable. Neuro-fuzzy systems are an example, which we are extending to deep networks; in particular by designing deep fuzzy clustering algorithms. Deep fuzzy clustering employs a deep learner as an automated feature extractor. A fuzzy clustering is performed in the extracted feature space, and a classifier built from it. The resulting model appears more interpretable, but at the cost of lower accuracy. This paper explores improvements to deep fuzzy clustering leading to a more accurate deep fuzzy classifier that still seems highly interpretable. We evaluate the accuracy and interpretability of the model on the MNIST dataset.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8858809,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858809,Deep learning;Machine learning;Neuro-fuzzy systems;Fuzzy clustering;eXplainable artificial intelligence,Clustering algorithms;Classification algorithms;Feature extraction;Deep learning;Neurons;Neural networks,feature extraction;fuzzy neural nets;fuzzy set theory;fuzzy systems;image classification;learning (artificial intelligence);pattern clustering,deep learning;deep neural network;neuro-fuzzy systems;deep fuzzy clustering algorithms;deep fuzzy classifier;automated feature extractor;MNIST dataset,,,62.0,,,,,IEEE,IEEE Conferences
43,Hierarchical visual case-based reasoning for supporting breast cancer therapy,J. Lamy; B. Sekar; G. Guezennec; J. Bouaud; B. Séroussi,"Université Paris 13, Sorbonne, Université, INSERM,LIMICS,93017 Bobigny,France; Ulster University,School of Computing and Mathematics,United Kingdom; Université Paris 13, Sorbonne, Université, INSERM,LIMICS,93017 Bobigny,France; LIMICS,France; LIMICS,France",2019 Fifth International Conference on Advances in Biomedical Engineering (ICABME),,2019,,,1,4,"Breast cancer therapy is particularly complex. Case-based reasoning (CBR) is an approach that can support clinicians when prescribing a therapy, and that is able to explain its recommendation to the clinicians. In a previous work, we proposed a visual CBR approach for helping clinicians to choose a treatment between four main categories (e.g. surgery, chemotherapy). However, these are broad categories and clinicians need more details about the treatment, e.g. several surgeries exist such as lumpectomy. Here, we extend our visual CBR approach for fully supporting the therapy for breast cancer, using a hierarchical approach: first, decide the category, then decide the exact treatment, etc.",2377-5696,978-1-7281-2314-1,10.1109/ICABME47164.2019.8940223,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940223,breast cancer;case-based reasoning;explainable artificial intelligence;XAI,Surgery;Visualization;Chemotherapy;Breast cancer;Color;Cognition,cancer;case-based reasoning;medical computing;patient treatment;surgery;tumours,hierarchical visual case-based reasoning;breast cancer therapy;visual CBR approach;surgery;lumpectomy,,,14.0,,,,,IEEE,IEEE Conferences
44,Explaining What a Neural Network has Learned: Toward Transparent Classification,K. Amarasinghe; M. Manic,"Department of Computer Science, Virginia Commonwealth University, Richmond, Virginia, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, Virginia, USA",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2019,,,1,6,"Deep Neural Networks (DNNs) have limited ability to explain their acquired knowledge or decision rationale. As a result, end-users perceive DNNs as black-boxes and are hesitant to fully adopt them in safety-critical applications. Therefore, developing explainable DNNs has become a prime interest in neural network research. This paper presents a methodology for linguistically explaining the knowledge a DNN classifier has acquired in training. The main objective is to help users understand what the DNN has learned about each class. The presented methodology is fuzzy logic based and involves end-users of the system in the explanation process, enabling users to customize the explanations to match their requirements. This paper presents the explanation methodology, metrics of explanation quality, validation steps, and a discussion of advantages and limitations. The explanation methodology was implemented on a benchmark classification problem. Experimental results demonstrated the method's capability to explain the DNN-knowledge and validated the explanations.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8858899,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858899,Explainable Artificial Intelligence;Explainable Neural Networks;Linguistic Summarization;Fuzzy Logic,Fuzzy sets;Linguistics;Neurons;Neural networks;Engines;Fuzzy logic;Feature extraction,fuzzy logic;neural nets;pattern classification,DNN classifier;fuzzy logic;explanation process;explanation methodology;explanation quality;benchmark classification problem;DNN-knowledge;transparent classification;acquired knowledge;decision rationale;black-boxes;safety-critical applications;deep neural networks,,,31.0,,,,,IEEE,IEEE Conferences
45,Natural Language Generation of Explanations of Fuzzy Inference Decisions,I. Baaj; J. Poli,"CEA, LIST, Gif-sur-Yvette cedex, 91191, France; CEA, LIST, Gif-sur-Yvette cedex, 91191, France",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2019,,,1,6,"As Artificial Intelligence and fuzzy systems are at the center of the emergence of advanced technologies such as autonomous vehicles or medical decision support systems, a problem of trust from a human point of view is strongly appearing. In this article, we tackle the problem of explanation of a fuzzy inference system decision in its entirety: from the conception of an algorithm that produces a textual explanation to its evaluation.We define a function which is able to associate to any activated fuzzy rule, the structure responsible of its activation degree. To assess our method, we defined a protocol to evaluate AIgenerated explanation, and made an experiment: explanations obtained from the classification of pastas. Despite limitations, the results show a good transparency of the reasoning, consistency and good global effectiveness in generated explanations.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8858994,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858994,Explainable Artificial Intelligence;Fuzzy Inference System,Linguistics;Fuzzy logic;Expert systems;Fuzzy systems;Cognition;Intelligent systems,fuzzy reasoning;fuzzy set theory;inference mechanisms;natural language processing,natural language generation;fuzzy inference decisions;fuzzy systems;autonomous vehicles;medical decision support systems;fuzzy inference system decision;textual explanation;activated fuzzy rule;activation degree;generated explanations,,,32.0,,,,,IEEE,IEEE Conferences
46,ExpliClas: Automatic Generation of Explanations in Natural Language for Weka Classifiers,J. M. Alonso; A. Bugarín,"Centro Singular de Investigacion en Tecnoloxías da Información (CiTIUS), Universidade de Santiago de Compostela, Santiago de Compostela, E-15782, Spain; Centro Singular de Investigacion en Tecnoloxías da Información (CiTIUS), Universidade de Santiago de Compostela, Santiago de Compostela, E-15782, Spain",2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2019,,,1,6,"ExpliClas is a web service aimed at providing users with multimodal (textual + graphical) explanations related to Weka classifiers. In ExpliClas, two types of explanations are automatically generated. On the one hand, global explanations pay attention to the behavior of the classifier as a whole, i.e., they refer to a list of structural properties (number of classes, features, etc.) along with quality indicators such as accuracy or confusion matrix. On the other hand, local explanations go in depth with how the classifier deals with single instances. Current version of ExpliClas already explains classifications made by three different decision tree Weka implementations (J48, RepTree, and RandomTree) and one fuzzy algorithm (FURIA). In this paper, we describe ExpliClas in detail and illustrate its use with the Bank telemarketing dataset.",1558-4739,978-1-5386-1728-1,10.1109/FUZZ-IEEE.2019.8859018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859018,Explainable Artificial Intelligence;Natural Language Generation;Decision Trees;Fuzzy Unordered Rule Induction Algorithm;Open Source Software;Weka,Artificial intelligence;Data mining;Linguistics;Web services;Decision trees;Europe;Java,decision trees;fuzzy set theory;natural languages;pattern classification;random forests;Web services,ExpliClas;global explanations;local explanations;natural language;Weka classifiers;web service;decision tree Weka implementations;automatic explanations generation;multimodal explanations;fuzzy algorithm;Bank telemarketing dataset,,,22.0,,,,,IEEE,IEEE Conferences
47,Fault Tolerance Tool for Human and Machine Interaction & Application to Civilian Aircraft,J. Viaña; K. Cohen,"University of Cincinnati,Department of the Aerospace Engineering & Engineering Mechanics,Cincinnati,USA; University of Cincinnati,Department of the Aerospace Engineering & Engineering Mechanics,Cincinnati,USA",2019 IEEE Latin American Conference on Computational Intelligence (LA-CCI),,2019,,,1,2,"Enhancing human-machine interaction is critical to aerospace applications. An essential requirement in safety critical systems is the clear need to guarantee trustworthiness of a system as well as V&V (Verification and Validation). However, the current state of the art concerning decision support systems lacks effective tools in this area. The Coherence Function Package, introduced in this research, is a tool towards providing assurance that the action needed has the approval of both the human and the machine in terms of SAFETY. These algorithms shed light on the future of an Explainable Artificial Intelligence (XAI, [1]), that fosters a synergy between these two factors. This vital requirement that has been further underscored after the tragic events of the Boeing 737 Max 8 crashes [2]. Preliminary results show that the proposed approach is not only able to detect any errors in the system, it also assists in circumventing conflicts leading to incoherence and suggests a preferred solution in real-time.",,978-1-7281-5666-8,10.1109/LA-CCI47412.2019.9037045,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9037045,Human-Machine Interaction;Flight Critical Systems;Aircraft Safety;Tautology;Implications;Equivalences;Recursive Functions,,aerospace computing;aircraft;artificial intelligence;decision making;decision support systems;failure analysis;fault diagnosis;fault tolerant computing;human computer interaction;safety-critical software;security of data,machine interaction & application;civilian aircraft;human-machine interaction;aerospace applications;safety critical systems;trustworthiness;decision support systems;Coherence Function Package;Explainable Artificial Intelligence;vital requirement;Boeing 737 Max 8 crashes;fault tolerance tool,,,7.0,,,,,IEEE,IEEE Conferences
48,Explaining Black-box Android Malware Detection,M. Melis; D. Maiorca; B. Biggio; G. Giacinto; F. Roli,"DIEE, University of Cagliari, Piazza d'Armi, 09123, Cagliari; DIEE, University of Cagliari, Piazza d'Armi, 09123, Cagliari; DIEE, University of Cagliari, Piazza d'Armi, 09123, Cagliari; DIEE, University of Cagliari, Piazza d'Armi, 09123, Cagliari; DIEE, University of Cagliari, Piazza d'Armi, 09123, Cagliari",2018 26th European Signal Processing Conference (EUSIPCO),,2018,,,524,528,"Machine-learning models have been recently used for detecting malicious Android applications, reporting impressive performances on benchmark datasets, even when trained only on features statically extracted from the application, such as system calls and permissions. However, recent findings have highlighted the fragility of such in-vitro evaluations with benchmark datasets, showing that very few changes to the content of Android malware may suffice to evade detection. How can we thus trust that a malware detector performing well on benchmark data will continue to do so when deployed in an operating environment? To mitigate this issue, the most popular Android malware detectors use linear, explainable machine-learning models to easily identify the most influential features contributing to each decision. In this work, we generalize this approach to any black-box machine-learning model, by leveraging a gradient-based approach to identify the most influential local features. This enables using nonlinear models to potentially increase accuracy without sacrificing interpretability of decisions. Our approach also highlights the global characteristics learned by the model to discriminate between benign and malware applications. Finally, as shown by our empirical analysis on a popular Android malware detection task, it also helps identifying potential vulnerabilities of linear and nonlinear models against adversarial manipulations.",2076-1465,978-9-0827-9701-5,10.23919/EUSIPCO.2018.8553598,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8553598,,Malware;Feature extraction;Detectors;Machine learning;Support vector machines;Signal processing algorithms;Approximation algorithms,Android (operating system);invasive software;learning (artificial intelligence),black-box Android malware detection;malicious Android applications;benchmark datasets;system calls;in-vitro evaluations;malware detector;benchmark data;popular Android malware detectors;linear machine-learning models;explainable machine-learning models;influential features;black-box machine-learning model;gradient-based approach;influential local features;nonlinear models;benign applications;malware applications;Android malware detection task,,,19.0,,,,,IEEE,IEEE Conferences
50,Human-Centric AI for Trustworthy IoT Systems With Explainable Multilayer Perceptrons,I. García-Magariño; R. Muttukrishnan; J. Lloret,"Department of Software Engineering and Artificial Intelligence, Complutense University of Madrid, Madrid, Spain; Department of Electrical and Electronic Engineering, City, University of London, London, U.K.; Integrated Management Coastal Research Institute, Universitat Politècnica de València, València, Spain",IEEE Access,,2019,7,,125562,125574,"Internet of Things (IoT) widely use analysis of data with artificial intelligence (AI) techniques in order to learn from user actions, support decisions, track relevant aspects of the user, and notify certain events when appropriate. However, most AI techniques are based on mathematical models that are difficult to understand by the general public, so most people use AI-based technology as a black box that they eventually start to trust based on their personal experience. This article proposes to go a step forward in the use of AI in IoT, and proposes a novel approach within the Human-centric AI field for generating explanations about the knowledge learned by a neural network (in particular a multilayer perceptron) from IoT environments. More concretely, this work proposes two techniques based on the analysis of artificial neuron weights, and another technique aimed at explaining each estimation based on the analysis of training cases. This approach has been illustrated in the context of a smart IoT kitchen that detects the user depression based on the food used for each meal, using a simulator for this purpose. The results revealed that most auto-generated explanations made sense in this context (i.e. 97.0%), and the execution times were low (i.e. 1.5 ms or lower) even considering the common configurations varying independently the number of neurons per hidden layer (up to 20), the number of hidden layers (up to 20) and the number of training cases (up to 4,000).",2169-3536,,10.1109/ACCESS.2019.2937521,Engineering and Physical Sciences Research Council; Ministerio de Ciencia y Tecnología; Ministerio de Ciencia e Innovación; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8813027,Explainable artificial intelligence;human-centric artificial intelligence;Internet of Things;multilayer perceptron;smart kitchen;emotion detection,Artificial intelligence;Neurons;Training;Sensors;Artificial neural networks;Estimation;Internet of Things,data analysis;Internet of Things;learning (artificial intelligence);multilayer perceptrons,trustworthy IoT systems;explainable multilayer perceptrons;artificial intelligence techniques;user actions;support decisions;track relevant aspects;AI techniques;mathematical models;general public;black box;personal experience;neural network;IoT environments;artificial neuron weights;training cases;smart IoT kitchen;user depression;auto-generated explanations;multilayer perceptron;human-centric AI field;Internet of Things,,,20.0,CCBY,,,,IEEE,IEEE Journals
51,RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records,B. C. Kwon; M. Choi; J. T. Kim; E. Choi; Y. B. Kim; S. Kwon; J. Sun; J. Choo,"IBM T.J. Watson Research CenterKorea University; Georgia Institute of Technology; Georgia Institute of Technology; Chung-Ang University; IBM T.J. Watson Research CenterKorea University; Catholic University, Daegu; IBM T.J. Watson Research CenterKorea University; Georgia Institute of Technology",IEEE Transactions on Visualization and Computer Graphics,,2019,25,1.0,299,309,"We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.",1941-0506,,10.1109/TVCG.2018.2865027,Korea Electric Power Corporation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440842,Interactive Artificial Intelligence;XAI (Explainable Artificial Intelligence);Interpretable Deep Learning;Healthcare,Machine learning;Medical diagnostic imaging;Task analysis;Predictive models;Computational modeling;Visual analytics;Data models,artificial intelligence;data analysis;data visualisation;interactive systems;medical information systems;recurrent neural nets,interactive RNN-based model;EMR data;prediction tasks;RetainVis;individual medical codes;risk predictions;temporal information;increase interactivity;interpretable analytics tool;interpretable networks;interactive recurrent neural networks;electronic medical records;black-box nature;interactively leverage users;design study;visual analytics solution;medical experts;artificial intelligence scientists;iterative design process;newly improved RNN-based model;RNN-based model;visual analytic researchers;interactive visual analytic tool,,8.0,85.0,,,,,IEEE,IEEE Journals
52,Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding,R. Chhatwal; P. Gronvall; N. Huber-Fliflet; R. Keeling; J. Zhang; H. Zhao,"Legal AT&T Services, Inc., Washington DC, USA; Data & Technology Ankura Consulting Group, LLC, Washington DC, USA; Data & Technology Ankura Consulting Group, LLC, Washington DC, USA; Complex Commercial Litigation, Sidley Austin LLP, Washington DC, USA; Data & Technology Ankura Consulting Group, LLC, Washington DC, USA; Data & Technology Ankura Consulting Group, LLC, Washington DC, USA",2018 IEEE International Conference on Big Data (Big Data),,2018,,,1905,1911,"In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.",,978-1-5386-5035-6,10.1109/BigData.2018.8622073,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622073,machine learning;text categorization;explainable AI;predictive coding;explainable predictive coding;legal document review,Predictive coding;Law;Predictive models;Text categorization;Machine learning,law administration;pattern classification;supervised learning;text analysis,responsive documents;explainable AI system;typical legal document review scenarios;responsive snippets;text classification;explainable predictive coding methods;data-focused engagements;technology-based techniques;machine learning researchers;document classification;supervised learning task;electronically-stored documents,,2.0,10.0,,,,,IEEE,IEEE Conferences
53,A Framework for Explainable Text Classification in Legal Document Review,C. J. Mahoney; J. Zhang; N. Huber-Fliflet; P. Gronvall; H. Zhao,"e-Discovery Cleary Gottlieb Steen & Hamilton LLP,Washington,DC,USA; LLC,Data & Technology, Ankura Consulting Group,DC,Washington; LLC,Data & Technology, Ankura Consulting Group,DC,Washington; LLC,Data & Technology, Ankura Consulting Group,DC,Washington; LLC,Data & Technology, Ankura Consulting Group,DC,Washington",2019 IEEE International Conference on Big Data (Big Data),,2019,,,1858,1867,"Companies regularly spend millions of dollars producing electronically-stored documents in legal matters. Over the past two decades, attorneys have been using a variety of technologies to conduct this exercise, and most recently, parties on both sides of the `legal aisle' are accepting the use of machine learning techniques like text classification to cull massive volumes of data and to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs in legal matters, text classification also faces a peculiar perception challenge: amongst lawyers, this technology is sometimes looked upon as a black box Put simply, very little information is provided for attorneys to understand why documents are classified as responsive. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if text classification can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, text classification can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. This paper describes a framework for explainable text classification as a valuable tool in legal services: for enhancing the quality and efficiency of legal document review and for assisting in locating responsive snippets within responsive documents. This framework has been implemented in our legal analytics product, which has been used in hundreds of legal matters. We also report our experimental results using the data from an actual legal matter that used this type of document review.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9005659,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005659,machine learning;text categorization;text classification;explainable AI;predictive coding;legal document review;XAI;false negatives,Text categorization;Law;Machine learning;Predictive coding;Predictive models,document handling;law administration;learning (artificial intelligence);pattern classification;text analysis,legal document review scenarios;text snippets;responsive snippets;explainable text classification;responsive documents;explainable AI system;machine learning,,,17.0,,,,,IEEE,IEEE Conferences
54,Explaining Machine Learning-Based Classifications of In-Vivo Gastral Images,A. Malhi; T. Kampik; H. Pannu; M. Madhikermi; K. Främling,"Department of Computer Science, Aalto University Finland; CSED, Thapar Institute of Engineering and Technology Patiala India; Department of Computing Science, Umeå University, Sweden; Department of Computer Science, Aalto University Finland; Department of Computer Science, Aalto University Finland",2019 Digital Image Computing: Techniques and Applications (DICTA),,2019,,,1,7,"This paper proposes an explainable machine learning tool that can potentially be used for decision support in medical image analysis scenarios. For a decision-support system it is important to be able to reverse-engineer the impact of features on the final decision outcome. In the medical domain, such functionality is typically required to allow applying machine learning to clinical decision making. In this paper, we present initial experiments that have been performed on in-vivo gastral images obtained from capsule endoscopy. Quantitative analysis has been performed to evaluate the utility of the proposed method. Convolutional neural networks have been used for training the validating of the image data set to provide the bleeding classifications. The visual explanations have been provided in the images to help health professionals trust the black box predictions. While the paper focuses on the in-vivo gastral image use case, most findings are generalizable.",,978-1-7281-3857-2,10.1109/DICTA47822.2019.8945986,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945986,Explainable artificial intelligence;Convolutional neural networks;Black box explanations;LIME,,decision making;decision support systems;endoscopes;learning (artificial intelligence);medical image processing;neural nets,in-vivo gastral image use case;bleeding classifications;image data;quantitative analysis;clinical decision making;machine learning;medical domain;decision-support system;medical image analysis scenarios;in-vivo gastral images;machine learning-based classifications,,,26.0,,,,,IEEE,IEEE Conferences
55,How to produce complementary explanations using an Ensemble Model,W. Silva; K. Fernandes; J. S. Cardoso,"INESC TEC and Faculdade de Engenharia, Universidade do Porto, Porto, Portugal; NILG.AI and INESC TEC, Porto, Portugal; INESC TEC and Faculdade de Engenharia, Universidade do Porto, Porto, Portugal",2019 International Joint Conference on Neural Networks (IJCNN),,2019,,,1,8,"In order to increase the adoption of machine learning models in areas like medicine and finance, it is necessary to have correct and diverse explanations for the decisions that the models provide, to satisfy the curiosity of decision-makers and the needs of the regulators. In this paper, we introduced a method, based in a previously presented framework, to explain the decisions of an Ensemble Model. Moreover, we instantiate the proposed approach to an ensemble composed of a Scorecard, a Random Forest, and a Deep Neural Network, to produce accurate decisions along with correct and diverse explanations. Our methods are tested on two biomedical datasets and one financial dataset. The proposed ensemble leads to an improvement in the quality of the decisions, and in the correctness of the explanations, when compared to its constituents alone. Qualitatively, it produces diverse explanations that make sense and convince the experts.",2161-4407,978-1-7281-1985-4,10.1109/IJCNN.2019.8852409,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852409,Interpretable Machine Learning;Explainable Machine Learning;Ensemble Model;Scorecards;Random Forests;Deep Neural Networks;Dermoscopics;Aesthetic Evaluation;Credit Scoring,Machine learning;Neural networks;Prototypes;Task analysis;Data visualization;Semantics;Mathematical model,data analysis;learning (artificial intelligence);neural nets;random forests,decision-makers;diverse explanations;complementary explanations;machine learning models;ensemble model;deep neural network;random forest;biomedical datasets;financial dataset,,,41.0,,,,,IEEE,IEEE Conferences
56,From Machine Learning to Explainable AI,A. Holzinger,"Holzinger Group, HCI-KDD, Medical University, Institute for Medical Informatics, Statistics & Documentation, Graz, Austria",2018 World Symposium on Digital Intelligence for Systems and Machines (DISA),,2018,,,55,66,"The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible “glass-box” approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.",,978-1-5386-5102-5,10.1109/DISA.2018.8490530,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8490530,,Machine learning;Data mining;Data visualization;Uncertainty;Games;Cognitive science,learning (artificial intelligence);ontologies (artificial intelligence);probability,statistical machine learning methods;AI winter;deep learning approaches;big quality data;computational power;engineering effort;ontologies;knowledge representations;glass-box approaches;mathematical principles;artificial intelligence;logical approaches;probabilistic learning methods;rule-based approaches;contextual explanatory models;context-adaptive procedures;high-dimensional vectors,,4.0,86.0,,,,,IEEE,IEEE Conferences
57,Designing interpretable Hierarchical Fuzzy Systems,L. Magdalena,"Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Campus de Montegancedo, Madrid, Spain",2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2018,,,1,8,"Complexity is a typical criteria to analyze interpretability of fuzzy systems: number of variables, terms, rules, etc. Hierarchical fuzzy systems have shown a great potential to reduce fuzzy systems complexity. On the other hand, the counterpart to complexity reduction is the presence of synthetic variables generated at intermediate levels of the hierarchy. Those synthetic variables are generally affected by an absolute absence of semantics, minimizing their interpretability. As a consequence, when analyzing interpretability in hierarchical fuzzy systems, complexity should only be a part of the equation, since semantics should also be considered. And particularly, semantics of the intermediate variables added to reduce complexity. The use of hierarchical fuzzy systems will only produce an effective interpretability improvement when the design of the hierarchical structure was driven by the semantics of the intermediate variables. In other words, intermediate variables should be interpretable in terms of the problem under analysis. This consideration, made in the framework of hierarchical fuzzy systems, could be extended to any kind of hierarchical system defined under the umbrella of explainable artificial intelligence. The present paper will consider different aspects of interpretability concerning hierarchical fuzzy systems, and will explore the role of intermediate variables in accordance with the previous comments. The appropriate selection of those intermediate variables will drive to a process where subsystems will be decoupled for design and interpretation. Under this assumption we will finally consider the measurement of interpretability in hierarchical fuzzy systems as a process where the interpretability of each component (fuzzy system) of the hierarchy is first evaluated and then aggregated to achieve an overall interpretability evaluation.",,978-1-5090-6020-7,10.1109/FUZZ-IEEE.2018.8491452,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8491452,Fuzzy systems;modeling;hierarchy;complexity;semantics;interpretability;explainable artificial intelligence,Fuzzy systems;Complexity theory;Input variables;Semantics;Mathematical model;Fuzzy logic;Hafnium,artificial intelligence;fuzzy set theory;fuzzy systems;hierarchical systems,interpretability;intermediate variables;fuzzy systems complexity;complexity semantics;interpretable hierarchical fuzzy systems;artificial intelligence,,1.0,25.0,,,,,IEEE,IEEE Conferences
58,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,E. Oyekanlu,"Electr. & Comput. Eng. Dept., Drexel Univ., Philadelphia, PA, USA",2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),,2018,,,179,188,"Challenges associated with developing analytics solutions at the edge of large scale Industrial Internet of Things (IIoT) networks close to where data is being generated in most cases involves developing analytics solutions from ground up. However, this approach increases IoT development costs and system complexities, delay time to market, and ultimately lowers competitive advantages associated with delivering next-generation IoT designs. To overcome these challenges, existing, widely available, hardware can be utilized to successfully participate in distributed edge computing for IIoT systems. In this paper, an osmotic computing approach is used to illustrate how distributed osmotic computing and existing low-cost hardware may be utilized to solve complex, compute-intensive Explainable Artificial Intelligence (XAI) deep learning problem from the edge, through the fog, to the network cloud layer of IIoT systems. At the edge layer, the C28x digital signal processor (DSP), an existing low-cost, embedded, real-time DSP that has very wide deployment and integration in several IoT industries is used as a case study for constructing real-time graph-based Coiflet wavelets that could be used for several analytic applications including deep learning pre-processing applications at the edge and fog layers of IIoT networks. Our implementation is the first known application of the fixed-point C28x DSP to construct Coiflet wavelets. Coiflet Wavelets are constructed in the form of an osmotic microservice, using embedded low-level machine language to program the C28x at the network edge. With the graph-based approach, it is shown that an entire Coiflet wavelet distribution could be generated from only one wavelet stored in the C28x based edge device, and this could lead to significant savings in memory at the edge of IoT networks. Pearson correlation coefficient is used to select an edge generated Coiflet wavelet and the selected wavelet is used at the fog layer for pre-processing and denoising IIoT data to improve data quality for fog layer based deep learning application. Parameters for implementing deep learning at the fog layer using LSTM networks have been determined in the cloud. For XAI, communication network noise is shown to have significant impact on results of predictive deep learning at IIoT network fog layer.",,978-1-5386-9555-5,10.1109/AIKE.2018.00042,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527474,Explainable Artificial Intelligence;Deep Learning;Real-Time;Industrial IoT;Wavelet Graph;Data Quality;Distributed Computing,Hardware;Wavelet analysis;Cloud computing;Logic gates;Real-time systems,cloud computing;delays;digital signal processing chips;distributed processing;graph theory;Internet of Things;learning (artificial intelligence);wavelet transforms,distributed osmotic computing approach;industrial IoT network edges;real-time adaptive wavelet graphs;IoT development costs;system complexities;delay time;IIoT systems;network cloud layer;edge layer;real-time DSP;real-time graph-based Coiflet wavelets;osmotic microservice;low-level machine language;graph-based approach;LSTM networks;communication network noise;IIoT network fog layer;distributed edge computing;digital signal processor;artificial intelligence deep learning problem;XAI deep learning pre-processing applications;large scale industrial Internet of Things,,,56.0,,,,,IEEE,IEEE Conferences
59,Explainable Prediction of Chronic Renal Disease in the Colombian Population Using Neural Networks and Case-Based Reasoning,G. R. Vásquez-Morales; S. M. Martínez-Monterrubio; P. Moreno-Ger; J. A. Recio-García,"Office of Information and Communications Technology, Ministry of Health and Social Protection, Bogotá, Colombia; Department of Software Engineering and Artificial Intelligence, Faculty of Computer Science, Group of Artificial Intelligence Applications, Universidad Complutense de Madrid, Ciudad Universitaria, Madrid, Spain; School of Engineering, Universidad Internacional de La Rioja (UNIR), Logroño, Spain; Office of Information and Communications Technology, Ministry of Health and Social Protection, Bogotá, Colombia",IEEE Access,,2019,7,,152900,152910,"This paper presents a neural network-based classifier to predict whether a person is at risk of developing chronic kidney disease (CKD). The model is trained with the demographic data and medical care information of two population groups: on the one hand, people diagnosed with CKD in Colombia during 2018, and on the other, a sample of people without a diagnosis of this disease. Once the model is trained and evaluation metrics for classification algorithms are applied, the model achieves 95% accuracy in the test data set, making its application for disease prognosis feasible. However, despite the demonstrated efficiency of the neural networks to predict CKD, this machine-learning paradigm is opaque to the expert regarding the explanation of the outcome. Current research on eXplainable AI proposes the use of twin systems, where a black-box machine-learning method is complemented by another white-box method that provides explanations about the predicted values. Case-Based Reasoning (CBR) has proved to be an ideal complement as this paradigm is able to find explanatory cases for an explanation-by-example justification of a neural network's prediction. In this paper, we apply and validate a NN-CBR twin system for the explanation of CKD predictions. As a result of this research, 3,494,516 people were identified as being at risk of developing CKD in Colombia, or 7% of the total population.",2169-3536,,10.1109/ACCESS.2019.2948430,"Secretaría de Estado de Investigación, Desarrollo e Innovación; Postdoctoral Schollarship of the second author is founded by the Secretariat of Education, Technology and Innovation of Mexico City (SECTEI); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877828,Chronic kidney disease prediction;neural networks;case-based reasoning;twin systems;explainable AI;support vector machines;random forest,Biological neural networks;Diseases;Training;Data models;Artificial intelligence;Sociology,case-based reasoning;diseases;health care;kidney;learning (artificial intelligence);medical computing;neural nets;pattern classification,white-box method;black-box machine-learning method;twin systems;explainable AI;machine-learning paradigm;test data;classification algorithms;evaluation metrics;population groups;medical care information;demographic data;chronic kidney disease;neural network-based classifier;neural networks;colombian population;chronic renal disease;explainable prediction;Colombia;CKD predictions;NN-CBR twin system;explanation-by-example justification;explanatory cases;case-based reasoning,,1.0,59.0,CCBY,,,,IEEE,IEEE Journals
60,A Social Science-based Approach to Explanations for (Game) AI,V. Volz; K. Majchrzak; M. Preuss,"Department of Computer Science, TU Dortmund University; Department of Computer Science, TU Dortmund University; WWU Münster, ERCIS",2018 IEEE Conference on Computational Intelligence and Games (CIG),,2018,,,1,2,"The current AI revolution provides us with many new, but often very complex algorithmic systems. This complexity does not only limit understanding, but also acceptance of e.g. deep learning methods. In recent years, explainable AI (XAI) has been proposed as a remedy. However, this research is rarely supported by publications on explanations from social sciences. We suggest a bottom-up approach to explanations for (game) AI, by starting from a baseline definition of understandability informed by the concept of limited human working memory. We detail our approach and demonstrate its application to two games from the GVGAI framework. Finally, we discuss our vision of how additional concepts from social sciences can be integrated into our proposed approach and how the results can be generalised.",2325-4289,978-1-5386-4359-4,10.1109/CIG.2018.8490361,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8490361,explainable AI;working memory;super-sensors,Artificial intelligence;Games;Object oriented modeling;Brain modeling;Complexity theory;Image recognition;Collaboration,computer games;learning (artificial intelligence);social sciences,social science-based approach;complex algorithmic systems;explainable AI;XAI;social sciences;games;AI revolution;deep learning methods;bottom-up approach;baseline definition;GVGAI framework;human working memory limitation,,,53.0,,,,,IEEE,IEEE Conferences
61,Explanation-Based Reward Coaching to Improve Human Performance via Reinforcement Learning,A. Tabrez; S. Agrawal; B. Hayes,"University of Colorado Boulder, Boulder, CO, 80309; University of Colorado Boulder, Boulder, CO, 80309; University of Colorado Boulder, Boulder, CO, 80309",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,2019,,,249,257,"For robots to effectively collaborate with humans, it is critical to establish a shared mental model amongst teammates. In the case of incongruous models, catastrophic failures may occur unless mitigating steps are taken. To identify and remedy these potential issues, we propose a novel mechanism for enabling an autonomous system to detect model disparity between itself and a human collaborator, infer the source of the disagreement within the model, evaluate potential consequences of this error, and finally, provide human-interpretable feedback to encourage model correction. This process effectively enables a robot to provide a human with a policy update based on perceived model disparity, reducing the likelihood of costly or dangerous failures during joint task execution. This paper makes two contributions at the intersection of explainable AI (xAI) and human-robot collaboration: 1) The Reward Augmentation and Repair through Explanation (RARE) framework for estimating task understanding and 2) A human subjects study illustrating the effectiveness of reward augmentation-based policy repair in a complex collaborative task.",2167-2148,978-1-5386-8555-6,10.1109/HRI.2019.8673104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673104,Explainable AI;Policy Explanation;Human-Robot Collaboration;Reward Estimation;Joint Task Execution,Task analysis;Robots;Collaboration;Maintenance engineering;Hidden Markov models;Cognitive science;Team working,human-robot interaction;learning (artificial intelligence);team working,reinforcement learning;shared mental model;teammates;incongruous models;catastrophic failures;mitigating steps;autonomous system;human collaborator;potential consequences;human-interpretable feedback;model correction;policy update;perceived model disparity;costly failures;dangerous failures;joint task execution;explainable AI;human-robot collaboration;human subjects;reward augmentation-based policy repair;complex collaborative task;explanation-based reward coaching;human performance improvement;model disparity;reward augmentation and repair through explanation framework;RARE framework;task understanding,,1.0,34.0,,,,,IEEE,IEEE Conferences
63,Visual Explanation of Simple Neural Networks using Interactive Rainbow Boxes,J. Lamy; R. Tsopra,"LIMICS, Université Paris 13, Sorbonne Université, France; Université Paris 13, SMBH, Bobigny, AP-HP, Paris, France",2019 23rd International Conference Information Visualisation (IV),,2019,,,50,55,"Artificial neural networks are machine-learning algorithms inspired by biological neural networks. Their main inconvenient is their “black-box” nature: while they are very efficient for making predictions, it is difficult to explain these predictions. In this paper, we propose a visual translation of the reasoning performed by simple neural networks, i.e. without hidden layers. This visualization relies on rainbow boxes, a recently-introduced technique for set visualization, and on three improvements we propose for rainbow boxes, including interactivity. We also present a small application of the proposed approach to decision support in antibiotherapy, for helping a physician to choose an antibiotic in urinary infections.",2375-0138,978-1-7281-2838-2,10.1109/IV.2019.00018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811976,Explainable artificial intelligence;XAI;artificial neural networks;medical decision support;set visualization,Visualization;Biological neural networks;Cognition;Antibiotics;Machine learning,belief networks;decision support systems;diseases;learning (artificial intelligence);medical expert systems;neural nets,black-box nature;visual translation;set visualization;visual explanation;interactive rainbow boxes;artificial neural networks;machine-learning algorithms;biological neural networks;neural networks,,,20.0,,,,,IEEE,IEEE Conferences
64,Black-Box vs. White-Box: Understanding Their Advantages and Weaknesses From a Practical Point of View,O. Loyola-González,"Tecnologico de Monterrey, Puebla, México",IEEE Access,,2019,7,,154096,154113,"Nowadays, in the international scientific community of machine learning, there exists an enormous discussion about the use of black-box models or explainable models; especially in practical problems. On the one hand, a part of the community defends that black-box models are more accurate than explainable models in some contexts, like image preprocessing. On the other hand, there exist another part of the community alleging that explainable models are better than black-box models because they can obtain comparable results and also they can explain these results in a language close to a human expert by using patterns. In this paper, advantages and weaknesses for each approach are shown; taking into account a state-of-the-art review for both approaches, their practical applications, trends, and future challenges. This paper shows that both approaches are suitable for solving practical problems, but experts in machine learning need to understand the input data, the problem to solve, and the best way for showing the output data before applying a machine learning model. Also, we propose some ideas for fusing both, explainable and black-box, approaches to provide better solutions to experts in real-world domains. Additionally, we show one way to measure the effectiveness of the applied machine learning model by using expert opinions jointly with statistical methods. Throughout this paper, we show the impact of using explainable and black-box models on the security and medical applications.",2169-3536,,10.1109/ACCESS.2019.2949286,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8882211,Black-box;white-box;explainable artificial intelligence;deep learning,Machine learning;Mathematical model;Biological system modeling;Gallium nitride;Biological neural networks;Statistical analysis;Computational modeling,learning (artificial intelligence);statistical analysis,black-box models;explainable models;applied machine learning model;white-box,,1.0,169.0,CCBY,,,,IEEE,IEEE Journals
65,Leveraging Cognitive Context Knowledge for Argumentation-Based Object Classification in Multi-Sensor Networks,Z. Hao; J. Wu; T. Liu; X. Chen,"College of Management, Shenzhen University, Shenzhen, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, China; College of Management, Shenzhen University, Shenzhen, China; College of Management, Shenzhen University, Shenzhen, China",IEEE Access,,2019,7,,71361,71373,"It is a great challenge to achieve interpretable collaborative object classification in multi-sensor networks. In this situation, argumentation-based object classification has been considered a promising paradigm, due to its natural means of justifying and explaining complicated decision making within multiple agents. However, disagreements between sensor agents are often encountered because of various object category levels. To address this category of granularity inconsistent problem in multi-sensor collaborative object classification tasks, we propose a cognitive context knowledge-enriched method for classification conflict resolution. The cognitive context is concerned, in this paper, to investigate how rich contextual knowledge-equipped cognitive agents can facilitate semantic consensus in argumentation-based object classification. The empirical evaluation demonstrates the effectiveness of our method with improvement over state-of-the-art, especially in the presence of noisy sensor data, while giving argumentative explanations. Therefore, it is suggested that people who can benefit from the proposed method in this paper are the human user of multi-sensor object classification systems, in which explaining decision support is one of the important factors concerned.",2169-3536,,10.1109/ACCESS.2019.2919073,China Postdoctoral Science Foundation; Natural Science Foundation of Guangdong Province; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723041,Multi-sensor networks;argumentation;cognitive context;explainable artificial intelligence;object classification,Semantics;Collaboration;Task analysis;Ontologies;Cognition;Machine learning;Training,decision making;image classification;learning (artificial intelligence);multi-agent systems;object recognition,sensor agents;object category levels;multisensor collaborative object classification tasks;cognitive context knowledge-enriched method;classification conflict resolution;argumentation-based object classification;multisensor object classification systems;multisensor networks;interpretable collaborative object classification;contextual knowledge;empirical evaluation,,,30.0,,,,,IEEE,IEEE Journals
66,Identifying Simple Shapes to Classify the Big Picture,M. Liang; G. Palado; W. N. Browne,"Victoria University of Wellington,Wellington,New Zealand; Victoria University of Wellington,Wellington,New Zealand; Victoria University of Wellington,Wellington,New Zealand",2019 International Conference on Image and Vision Computing New Zealand (IVCNZ),,2019,,,1,6,"In recent years, Deep Artificial Neural Networks (DNNs) have demonstrated their ability in solving visual classification problems. However, an impediment is transparency where it is difficult to interpret why an object is classified in a particular way. Furthermore, it is also difficult to validate whether a learned model truly represents a problem space. Learning Classifier Systems (LCSs) are an Evolutionary Computation technique capable of producing human-readable rules that explain why an instance has been classified, i.e. the system is fully transparent. However, because they can encode complex relationships between features, they are not best suited to domains with a large number of input features, e.g. classification in pixel images. Thus, the aim of this work is to develop a novel DNN-LCS system where the former extracts features from pixels and the latter classifies objects from these features with clear decision boundaries. Results show that the system can explain its classification decisions on curated image data, e.g. plates have elliptical or rectangular shapes. This work represents a promising step towards explainable artificial intelligence in computer vision.",2151-2205,978-1-7281-4187-9,10.1109/IVCNZ48456.2019.8960989,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8960989,Object Recognition;Learning Classifier Systems,,computer vision;evolutionary computation;feature extraction;image classification;learning (artificial intelligence);neural nets;shape recognition,Deep Artificial Neural Networks;visual classification problems;impediment;transparency;LCSs;Evolutionary Computation technique;human-readable rules;pixel images;feature extraction;object classification;decision boundaries;classification decisions;explainable artificial intelligence;computer vision;DNN-LCS system;shape identification,,,42.0,,,,,IEEE,IEEE Conferences
67,Automatic Diagnosis of Parkinson Disease through Handwriting Analysis: A Cartesian Genetic Programming Approach,R. Senatore; A. Della Cioppa; A. Marcelli,"Natural Computation Lab, DIEM, Università degli Studi di Salerno; Natural Computation Lab, DIEM, Università degli Studi di Salerno; Natural Computation Lab, DIEM, Università degli Studi di Salerno",2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),,2019,,,312,317,"Early disease identification through non-invasive and automatic techniques has gathered increasing interest by the scientific community in the last decades. In this context, Parkinsons Disease (PD) has received particular attention in that it is a severe and progressive neurodegenerative disease and, therefore, early diagnosis would provide more prompt and effective intervention strategies. This, in turn, would successfully influence the life expectancy of the patients. However, the acceptance of computer-based diagnosis by doctors is hampered by the black-box approach implemented by the most performing systems, such as Artificial Neural Networks and Support Vector Machines, which do not explicit the rules adopted by the system. In this context, we propose a Cartesian Genetic Programming, aimed at automatically identify PD through the analysis of handwriting performed by PD patients and healthy controls. The use of such approach is particularly interesting in that it allows to infer explicit models of classification and, at same time, to automatically identify a suitable subset of features relevant for a correct diagnosis. The approach has been evaluated on the features extracted from the handwriting samples contained in the publicly available PaHaW dataset. Experimental results show that our approach compares favorably with state-of-the-art methods and, more importantly, provides an explicit model of the classification criteria.",2372-9198,978-1-7281-2286-1,10.1109/CBMS.2019.00071,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787399,Explainable Artificial Intelligence;Parkinson disease;E-Health;Evolutionary Computation,Task analysis;Feature extraction;Diseases;Genetic programming;Support vector machines;Artificial intelligence,diseases;feature extraction;genetic algorithms;handwriting recognition;image classification;medical diagnostic computing;neurophysiology;patient diagnosis,handwriting analysis;disease identification;life expectancy;computer-based diagnosis;black-box approach;PD patients;Parkinsons disease diagnosis;neurodegenerative disease;intervention strategies;Cartesian genetic programming approach;features extraction,,,30.0,,,,,IEEE,IEEE Conferences
68,Autonomous Causally-Driven Explanation of Actions,G. E. Katz; D. Dullnig; G. P. Davis; R. J. Gentili; J. A. Reggia,"Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA; Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA; Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA; Dept. of Kinesiology, Univ. of Maryland, College Park, MD, USA; Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA",2017 International Conference on Computational Science and Computational Intelligence (CSCI),,2017,,,772,778,"We propose a cause-effect reasoning mechanism with which an autonomous system can justify planned actions to a human end user. The mechanism is based on a structure we call a ""causal plan graph,"" which encodes the causal relationships between the actions, intentions, and goals of the autonomous system. Causal chains within this graph can potentially serve as intuitive, human-friendly justifications for the autonomous system's planned actions. A prototype of this mechanism is tested in simulation on a set of planning problems from an autonomous maintenance scenario. We demonstrate empirically that shortest path algorithms can effectively reduce a very large number of possible causal chains to a small, intelligible subset that might reasonably be inspected and ranked by a human. Consequently this work can serve as the basis for an experimental platform for future end user studies with human participants.",,978-1-5386-2652-8,10.1109/CSCI.2017.133,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560892,cause-effect reasoning;explainable artificial intelligence (XAI);imitation learning;robotics,,cause-effect analysis;control engineering computing;graph theory;inference mechanisms;maintenance engineering;planning (artificial intelligence),autonomous system;planning problems;autonomous maintenance scenario;possible causal chains;future end user studies;human participants;autonomous causally-driven explanation;cause-effect reasoning mechanism;planned actions;human end user;causal plan graph;causal relationships;human-friendly justifications,,,14.0,,,,,IEEE,IEEE Conferences
69,Visual Approach to Support Analysis of Optimum-Path Forest Classifier,D. Medeiros Eler; M. Prachedes Batista; R. E. Garcia; D. R. Pereira; W. E. Marcilio,"Sao Paulo State Univ., Presidente Prudente, Brazil; Sao Paulo State Univ., Presidente Prudente, Brazil; Sao Paulo State Univ., Presidente Prudente, Brazil; Univ. do Oeste Paulista, Presidente Prudente, Brazil; Sao Paulo State Univ., Presidente Prudente, Brazil",2019 8th Brazilian Conference on Intelligent Systems (BRACIS),,2019,,,777,782,"Optimum-path forest (OPF) is a graph based classifier in which the training process computes optimum-path trees rooted by prototype instances. Thus, one or more optimum-path trees represent each class and the testing process is based on identifying which optimum-path tree would contain a test sample. Usually, OPF performance is analyzed based on measures computed from training and testing process, such as f-score and correct classification rate (accuracy). This paper proposes an approach based on visualization to support understanding of OPF training and testing processes. The visual approach uses multidimensional projection techniques to reduce the feature space dimensionality and to generate graphical representation from instances similarities. As a result, one can visualize, analyze and understand each step of OPF classifier: generation of the minimum-spanning tree, prototypes choosing, computation of optimum-path trees, and test samples classification. The experiments show that our approach is useful to understand how the prototypes are chosen, to identify what are the best prototypes, to visualize how the training dataset size influences the OPF performance, to analyze how a weak feature space can impact the OPF performance, and to identify some insights about OPF classifier as a whole.",2643-6264,978-1-7281-4253-1,10.1109/BRACIS.2019.00139,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8923927,"optimum-path forest, multidimensional projection, visualization assisted machine learning, explainable artificial intelligence",Training;Prototypes;Testing;Vegetation;Visualization;Forestry;Data visualization,data visualisation;graph theory;pattern classification;trees (mathematics),visual approach;optimum-path forest classifier;graph based classifier;optimum-path tree;OPF classifier,,,18.0,,,,,IEEE,IEEE Conferences
70,Regional Multi-Scale Approach for Visually Pleasing Explanations of Deep Neural Networks,D. Seo; K. Oh; I. Oh,"National Institute of Agricultural Sciences, Jeonju, South Korea; Division of Computer Science and Engineering, Jeonbuk National University, Jeonju, South Korea; Division of Computer Science and Engineering, Jeonbuk National University, Jeonju, South Korea",IEEE Access,,2020,8,,8572,8582,"Recently, many methods to interpret and visualize deep neural network predictions have been proposed, and significant progress has been made. However, a more class-discriminative and visually pleasing explanation is required. Thus, this paper proposes a region-based approach that estimates feature importance in terms of appropriately segmented regions. By fusing the saliency maps generated from multi-scale segmentations, a more class-discriminative and visually pleasing map is obtained. This paper incorporates this regional multi-scale concept into a prediction difference method that is model-agnostic. An input image is segmented in several scales using the superpixel method, and exclusion of a region is simulated by sampling a normal distribution constructed via the boundary prior. The experimental results demonstrate that the regional multi-scale method produces much more class-discriminative and visually pleasing saliency maps.",2169-3536,,10.1109/ACCESS.2019.2963055,National Research Foundation of Korea; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945372,Computer vision;neural networks;explainable artificial intelligence;machine learning,Predictive models;Image segmentation;Prediction algorithms;Neural networks;Neurons;Visualization,image sampling;image segmentation;learning (artificial intelligence);neural nets;normal distribution,normal distribution;input image segmentation;visually pleasing saliency maps;superpixel method;multiscale segmentations;class-discriminative;deep neural networks;visually pleasing explanation;regional multiscale approach,,1.0,33.0,CCBY,,,,IEEE,IEEE Journals
71,Towards Interpretable Deep Extreme Multi-Label Learning,Y. Kang; I. Cheng; W. Mao; B. Kuo; P. Lee,"National Sun Yat-sen University, Taiwan; National Chung Hsing University, Taiwan; National Sun Yat-sen University, Taiwan; National Sun Yat-sen University, Taiwan; National Chung Cheng University",2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI),,2019,,,69,74,"Many Machine Learning algorithms, such as deep neural networks, have long been criticized for being ""black-boxes""-a kind of models unable to provide how it arrive at a decision without further efforts to interpret. This problem has raised concerns on model applications' trust, safety, nondiscrimination, and other ethical issues. In this paper, we discuss the machine learning interpretability of a real-world application, eXtreme Multi-label Learning (XML), which involves learning models from annotated data with many pre-defined labels. We propose a two-step XML approach that combines deep non-negative autoencoder with other multi-label classifiers to tackle different data applications with a large number of labels. Our experimental result shows that the proposed approach is able to cope with many-label problems as well as to provide interpretable label hierarchies and dependencies that helps us understand how the model recognizes the existences of objects in an image.",,978-1-7281-1337-1,10.1109/IRI.2019.00024,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843489,Representation Learning;Artificial Neural Networks;Explainable Artificial Intelligence;Machine Learning Interpretability;Multi-label Learning,,learning (artificial intelligence);neural nets;pattern classification;XML,deep neural networks;black-boxes;ethical issues;annotated data;two-step XML approach;nonnegative autoencoder;multilabel classifiers;machine learning algorithms;interpretable deep extreme multilabel learning,,,32.0,,,,,IEEE,IEEE Conferences
72,Distributional memory explainable word embeddings in continuous space,L. Snidaro; G. Ferrin; G. L. Foresti,"Computer Science and Physics University of Udine,Department of Mathematics,Via delle Scienze, 206, Udine,UD,Italy,33170; Computer Science and Physics University of Udine,Department of Mathematics,Via delle Scienze, 206, Udine,UD,Italy,33170; Computer Science and Physics University of Udine,Department of Mathematics,Via delle Scienze, 206, Udine,UD,Italy,33170",2019 22th International Conference on Information Fusion (FUSION),,2019,,,1,7,"Natural Language Processing (NLP) is a key processing step in fusion systems that need to process unstructured -and possibly human generated- text in natural language. Recent developments in Deep Learning have greatly increased the performance of NLP tasks. In particular, learned word representations have the form of high dimensional real valued vectors, called word embeddings, that have a number of amenable properties such as representing similar words with similar values of their vectorial representation, and capturing semantic regularities that correspond to geometric properties in the continuous high dimensional space. However, word embeddings have the drawback of being non interpretable. That is, their dimensions cannot be clearly associated to linguistic features. In this work, we propose real valued explicit linguistic word vectors that enjoy the properties of learned word embeddings while being human understandable.",,978-0-9964527-8-6,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9011324,Word Embeddings;NLP;Explainable Artificial Intelligence;Interpretability;Situation assessment;Semantics;Lexical semantics;Distributional semantics,Semantics;Linguistics;Tensile stress;Context modeling;Predictive models;Machine learning,computational linguistics;geometry;learning (artificial intelligence);natural language processing;sensor fusion;text analysis;vectors;word processing,word embeddings;continuous space;Natural Language Processing;fusion systems;deep learning;NLP tasks;vectorial representation;semantic regularities;geometric properties;word representations;linguistic word vectors,,,29.0,,,,,IEEE,IEEE Conferences
73,Assessment of Machine Learning Performance for Decision Support in Venture Capital Investments,J. Arroyo; F. Corea; G. Jimenez-Diaz; J. A. Recio-Garcia,"Department of Software Engineering and Artificial Intelligence, Universidad Complutense of Madrid, Madrid, Spain; Department of Management, Ca’ Foscari University, Venice, Italy; Department of Software Engineering and Artificial Intelligence, Universidad Complutense of Madrid, Madrid, Spain; Department of Software Engineering and Artificial Intelligence, Universidad Complutense of Madrid, Madrid, Spain",IEEE Access,,2019,7,,124233,124243,"The venture capital (VC) industry offers opportunities for investment in early-stage companies where uncertainty is very high. Unfortunately, the tools investors currently have available are not robust enough to reduce risk and help them managing uncertainty better. Machine learning data-driven approaches can bridge this gap, as they already do in the hedge fund industry. These approaches are now possible because data from thousands of companies over the world is available through platforms such as Crunchbase. Previous academic efforts have focused only on predicting two classes of exits, i.e., being acquired by other company or offering shares to the public, using only one or a few subsets of explanatory variables. These events are typically related to high returns, but also higher risk, making hard for a venture fund to get repeatable and sustainable returns. On the contrary, we will try to predict more possible outcomes including a subsequent funding round or the closure of the company using a large set of signals. In this way, our approach would provide VC investors with more information to set up a portfolio with lower risk that may eventually achieve higher returns than those based on finding unicorns (i.e., companies with a valuation higher than one billion dollars). We will analyze the performance of several machine learning methods in a dataset of over 120,000 early-stage companies in a realistic setting that tries to predict their progress in a 3-year time window. Results show that machine learning can support venture investors in their decision-making processes to find opportunities and better assessing the risk of potential investments.",2169-3536,,10.1109/ACCESS.2019.2938659,"Four Trees Merchant Partners Inc; Secretaría de Estado de Investigación, Desarrollo e Innovación; H2020 LEIT Information and Communication Technologies; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821312,Crunchbase;decision support systems;investment;machine learning;risk assessment;venture capital;explainable artificial intelligence,Companies;Machine learning;Investment;Industries;Logistics;Venture capital,decision making;decision support systems;financial data processing;investment;learning (artificial intelligence);risk analysis;venture capital,decision support;venture capital investments;venture capital industry;machine learning data-driven approaches;venture fund;decision-making processes;uncertainty management;machine learning performance assessment;risk reduction;explanatory variables;portfolio;3-year time window;time 3 year,,,16.0,CCBY,,,,IEEE,IEEE Journals
74,Salient Explanation for Fine-Grained Classification,K. Oh; S. Kim; I. Oh,"Division of Computer Science and Engineering, Jeonbuk National University, Jeonju, South Korea; Division of Computer Science and Engineering, Jeonbuk National University, Jeonju, South Korea; Division of Computer Science and Engineering, Jeonbuk National University, Jeonju, South Korea",IEEE Access,,2020,8,,61433,61441,"Explaining the prediction of deep models has gained increasing attention to increase its applicability, even spreading it to life-affecting decisions. However there has been no attempt to pinpoint only the most discriminative features contributing specifically to separating different classes in a fine-grained classification task. This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light and shadow in nature. GLAS provides a useful coarse-to-fine control benefiting from scalability of Gaussian mask. We also devised the ability to identify multiple instances through recursive GLAS. We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset. To show the general applicability, we also illustrate that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224 × 224 image) via the ImageNet Large Scale Visual Recognition Challenge.",2169-3536,,10.1109/ACCESS.2020.2980742,"National Research Foundation of Korea (NRF) through the Korean Government, Ministry of Science and ICT (MSIT); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035446,Computer vision;neural networks;explainable artificial intelligence;machine learning,,Gaussian processes;image classification,deep models;feature perturbation;coarse-to-fine control;recursive GLAS;fine-grained classification dataset;life-affecting decisions;discriminative features;salient explanation method;Gaussian light and shadow;Gaussian mask;ImageNet Large Scale Visual Recognition Challenge,,,38.0,CCBY,,,,IEEE,IEEE Journals
75,Argumentation-Based Agents that Explain Their Decisions,M. Morveli Espinoza; A. T. Possebom; C. A. Tacla,"Federal University of Technology of Parana; IFPR, Paranavai, Brazil; CPGEI, UTFPR, Curitiba, Brazil",2019 8th Brazilian Conference on Intelligent Systems (BRACIS),,2019,,,467,472,"Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions, behaviours and reasoning that produce their choices to the humans (or other systems) with which they interact. In this paper, we focus on how an extended model of BDI (Beliefs-Desires-Intentions) agents can be able to generate explanations about their reasoning, specifically, about the goals he decides to commit to. Our proposal is based on argumentation theory, we use arguments to represent the reasons that lead an agent to make a decision and use argumentation semantics to determine acceptable arguments (reasons). We propose two types of explanations: the partial one and the complete one. We apply our proposal to a scenario of rescue robots.",2643-6264,978-1-7281-4253-1,10.1109/BRACIS.2019.00088,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8923631,intelligent agents;explainable agency;argumentation,Multi-agent systems;Inference mechanisms;Intelligent agents;Rescue robots;Artificial intelligence;Decision making,inference mechanisms;multi-agent systems,argumentation-based agents;explainable artificial intelligence systems;XAI;intelligent agents;BDI agents;argumentation theory;argumentation semantics;beliefs-desires-intentions agents;rescue robots,,,11.0,,,,,IEEE,IEEE Conferences
76,Topological Learning for Semi-Supervised Anomaly Detection in Hyperspectral Imagery,J. Ramirez; T. Armitage; T. Bihl; R. Kramer,"Wright-Patterson Air Force Base,U.S. Air Force Laboratory,OH,USA; Wright-Patterson Air Force Base,U.S. Air Force Laboratory,OH,USA; Wright-Patterson Air Force Base,U.S. Air Force Laboratory,OH,USA; Wright-Patterson Air Force Base,U.S. Air Force Laboratory,OH,USA",2019 IEEE National Aerospace and Electronics Conference (NAECON),,2019,,,560,564,"Herein, we develop a probabilistic methodology that enables the application of semi-supervised learning over a data architecture for knowledge representation. The data architecture, proposed here, is known as the Topological Hierarchal Decomposition (THD) and is derived from the use of topological compression to decompose data into subsets of increasing attribute similarity. We demonstrate the use of the THD and a probabilistic model for interrogating the THD for object detection in hyperspectral imagery. In particular, we develop a classifier to identify objects that share similar topological attributes with a training reference object.",2379-2027,978-1-7281-1416-3,10.1109/NAECON46414.2019.9058127,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9058127,Mapper Algorithm;Knowledge Representation;Non-linear Dimensionality Reduction;Topological Data Analysis;Explainable Artificial Intelligence;Machine Intelligence,Data models;Hyperspectral imaging;Image segmentation;Image coding;Forestry;Probabilistic logic;Dimensionality reduction,geophysical image processing;graph theory;knowledge representation;learning (artificial intelligence);object detection,topological learning;semisupervised anomaly detection;hyperspectral imagery;probabilistic methodology;semisupervised learning;data architecture;knowledge representation;THD;topological compression;attribute similarity;probabilistic model;object detection;share similar topological attributes;topological hierarchal decomposition,,,19.0,,,,,IEEE,IEEE Conferences
78,ProtoSteer: Steering Deep Sequence Model with Prototypes,Y. Ming; P. Xu; F. Cheng; H. Qu; L. Ren,Hong Kong University of Science and Technology; Bosch Research North America; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Bosch Research North America,IEEE Transactions on Visualization and Computer Graphics,,2020,26,1.0,238,248,"Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.",1941-0506,,10.1109/TVCG.2019.2934267,Hong Kong TRS; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827944,Sequence Data;Explainable Artificial Intelligence (XAI);Recurrent Neural Networks (RNNs);Prototype Learning,Prototypes;Data visualization;Data models;Machine learning;Computational modeling;Predictive models;Task analysis,case-based reasoning;data analysis;data visualisation,domain-specific knowledge;ProtoSteer;deep sequence model;human problem-solving process;model decisions;user-specified prototypes;application domains;domain users;interpretable models;natural language processing;prototype steering;case-based reasoning;ProSeNet;prototype sequence network;visual summary,,,51.0,,,,,IEEE,IEEE Journals
80,A Model-Agnostic Approach for Explaining the Predictions on Clustered Data,Z. Zhou; M. Sun; J. Chen,Louisiana State University; Louisiana State University; Louisiana State University,2019 IEEE International Conference on Data Mining (ICDM),,2019,,,1528,1533,"Machine learning models especially deep neural network models have shown great potential in making decisions when analyzing clustered or longitudinal data. However, lack of model transparency is a major concern in risk sensitive domains such as social science and medical diagnosis. Despite the early success of explaining machine learning models, there is a lack of explanation methods that can be applied to any predictors on clustered data since most of the existing models assume that all observations are independent of each other. In this paper, we address this deficiency and propose to use a linear mixed model to mimic the local behavior of any complex model on clustered data, which can also improve the fidelity of the explanation method to the complex models. We apply our method to explain several models including a deep neural network model on two tasks including movie recommendation and medical record diagnosis. Experiment results show that our model outperforms the baseline models on several metrics such as fidelity and exactness.",2374-8486,978-1-7281-4604-1,10.1109/ICDM.2019.00202,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970981,"Explainable machine learning, Deep neural network, Clustered data",,decision making;learning (artificial intelligence);medical information systems;neural nets;patient diagnosis;pattern clustering;recommender systems,clustered data;longitudinal data;risk sensitive domains;social science;medical diagnosis;linear mixed model;movie recommendation;medical record diagnosis;model-agnostic approach;deep neural network model;machine learning models,,,8.0,,,,,IEEE,IEEE Conferences
81,The Best of Both Worlds: Challenges in Linking Provenance and Explainability in Distributed Machine Learning,S. Scherzinger; C. Seifert; L. Wiese,"OTH Regensburg, Germany; University of Twente, Netherlands; Leibniz University Hannover",2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS),,2019,,,1620,1629,"Machine learning experts prefer to think of their input as a single, homogeneous, and consistent data set. However, when analyzing large volumes of data, the entire data set may not be manageable on a single server, but must be stored on a distributed file system instead. Moreover, with the pressing demand to deliver explainable models, the experts may no longer focus on the machine learning algorithms in isolation, but must take into account the distributed nature of the data stored, as well as the impact of any data pre-processing steps upstream in their data analysis pipeline. In this paper, we make the point that even basic transformations during data preparation can impact the model learned, and that this is exacerbated in a distributed setting. We then sketch our vision of end-to-end explainability of the model learned, taking the pre-processing into account. In particular, we point out the potentials of linking the contributions of research on data provenance with the efforts on explainability in machine learning. In doing so, we highlight pitfalls we may experience in a distributed system on the way to generating more holistic explanations for our machine learning models.",2575-8411,978-1-7281-2519-0,10.1109/ICDCS.2019.00161,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8885193,explainable machine learning;distributed computing;provenance,Machine learning;Distributed databases;Data models;Decision trees;Computational modeling;Entropy,data analysis;learning (artificial intelligence),linking provenance;distributed machine learning;machine learning experts;single data;homogeneous data;consistent data;entire data set;distributed file system;explainable models;data pre-processing steps;data analysis pipeline;basic transformations;data preparation;distributed setting;end-to-end explainability;data provenance;distributed system;machine learning models,,,60.0,,,,,IEEE,IEEE Conferences
82,An Interpretable Generative Model for Handwritten Digits Synthesis,Y. Zhu; S. Suri; P. Kulkarni; Y. Chen; J. Duan; C. -. J. Kuo,"University of Southern California, Los Angeles, CA, USA; IIIT, Delhi, India; IIT, Mumbai, India; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA",2019 IEEE International Conference on Image Processing (ICIP),,2019,,,1910,1914,"An interpretable generative model for handwritten digits synthesis is proposed in this work. Modern image generative models such as the variational autoencoder (VAE) are trained by backpropagation (BP). The training process is complex, and its underlying mechanism is not transparent. Here, we present an explainable generative model using a feedforward design methodology without BP. Being similar to VAEs, it has an encoder and a decoder. For the encoder design, we derive principal-component-analysis-based (PCA-based) transform kernels using the covariance of its inputs. This process converts input images of correlated pixels to uncorrelated spectral components, which play the same role as latent variables in a VAE system. For the decoder design, we convert randomly generated spectral components to synthesized images through the inverse PCA transform. A subject test is conducted to compare the quality of digits generated using the proposed method and the VAE method. They offer comparable perceptual quality yet our model can be obtained at much lower complexity.",2381-8549,978-1-5386-6249-6,10.1109/ICIP.2019.8803129,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803129,Generative model;feedforward Design;variational autoencoder (VAE);explainable machine learning;principal component analysis (PCA),Principal component analysis;Transforms;Image synthesis;Decoding;Kernel;Training;Image reconstruction,backpropagation;feedforward;handwritten character recognition;neural nets;principal component analysis;spectral analysis,handwritten digits synthesis;modern image generative models;BP;training process;feedforward design methodology;encoder design;uncorrelated spectral components;VAE system;decoder design;VAE method;PCA-based transform kernels;principal component analysis-based transform kernels,,,31.0,,,,,IEEE,IEEE Conferences
83,MTEX-CNN: Multivariate Time Series EXplanations for Predictions with Convolutional Neural Networks,R. Assaf; I. Giurgiu; F. Bagehorn; A. Schumann,"IBM Research, Zurich; IBM Research, Zurich; IBM Research, Zurich; IBM Research, Zurich",2019 IEEE International Conference on Data Mining (ICDM),,2019,,,952,957,"In this work we present MTEX-CNN, a novel explainable convolutional neural network architecture which can not only be used for making predictions based on multivariate time series data, but also for explaining these predictions. The network architecture consists of two stages and utilizes particular kernel sizes. This allows us to apply gradient based methods for generating saliency maps for both the time dimension and the features. The first stage of the architecture explains which features are most significant to the predictions, while the second stage explains which time segments are the most significant. We validate our approach on two use cases, namely to predict rare server outages in the wild, as well as the average energy production of photovoltaic power plants based on a benchmark data set. We show that our explanations shed light over what the model has learned. We validate this by retraining the network using the most significant features extracted from the explanations and retaining similar performance to training with the full set of features.",2374-8486,978-1-7281-4604-1,10.1109/ICDM.2019.00106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970899,Explainable Machine Learning;Multivariate Time Series;Convolutional Neural Network;Deep Learning,,convolutional neural nets;feature extraction;gradient methods;image representation;learning (artificial intelligence);time series,MTEX-CNN;multivariate time series data;gradient based methods;saliency maps;convolutional neural network architecture;photovoltaic power plants;multivariate time series explanations,,,15.0,,,,,IEEE,IEEE Conferences
85,RuleMatrix: Visualizing and Understanding Classifiers with Rules,Y. Ming; H. Qu; E. Bertini,Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; New York University,IEEE Transactions on Visualization and Computer Graphics,,2019,25,1.0,342,352,"With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.",1941-0506,,10.1109/TVCG.2018.2864812,973 National Basic Research Program of China; Defense Advanced Research Projects Agency; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440085,explainable machine learning;rule visualization;visual analytics,Machine learning;Data visualization;Visualization;Neural networks;Decision trees;Data models;Support vector machines,data visualisation;interactive systems;knowledge representation;learning (artificial intelligence);matrix algebra;pattern classification,rule matrix;black-box model;matrix-based visualization;standardized rule-based knowledge representation;predictive models;interactive visualization technique;machine learning systems,,9.0,48.0,,,,,IEEE,IEEE Journals
87,Explainable data-driven modeling of patient satisfaction survey data,N. Liu; S. Kumara; E. Reich,"Department of Industrial and Manufacturing Engineering, The Pennsylvania State University, University Park, PA 16802, USA; Department of Industrial and Manufacturing Engineering, The Pennsylvania State University, University Park, PA 16802, USA; Healthcare Re-Engineering, Biomedical & Translational Informatics, Geisinger Health System Danville, PA 17822, USA",2017 IEEE International Conference on Big Data (Big Data),,2017,,,3869,3876,"In the personalized patient-centered healthcare, self-reported patient satisfaction survey data plays an important role. Given the patient survey data, it is necessary to identify the drivers of patient satisfaction and explain them so that such patterns can be used in future as well as necessary corrective actions can be taken. In healthcare, both accuracy and interpretability are important criteria for choosing a reliable predictive model for analyzing patient data. Usually, complex models such as Random Forest, neural networks can achieve high prediction accuracy but lack necessary interpretation to their prediction results. In this paper, we address this problem by proposing a local explanation method to interpret complex model prediction results. First, we build a predictive model using Random Forest to fit the patient satisfaction data. Second, we utilize local explanation method to provide insights into the Random Forest prediction results so as to discover true reasons behind patient experiences and overall ratings. Specifically, our approach allows us to interpret patient's overall rating of a hospital at the individual level, and find out the set of the most influential factors for each patient. We focus on all unhappy patients to investigate the top reasons for patient dissatisfaction. Our approach and findings will help to establish guidelines for a quality healthcare.",,978-1-5386-2715-0,10.1109/BigData.2017.8258391,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258391,explainable machine learning;data mining;lazy lasso;patient satisfaction;healthcare management,Hospitals;Predictive models;Data models;Training data;Pain;Vegetation,data analysis;health care;hospitals;learning (artificial intelligence);medical computing;neural nets;patient care,self-reported patient satisfaction survey data;necessary corrective actions;interpretability;reliable predictive model;complex models;local explanation method;complex model prediction results;Random Forest prediction results;patient experiences;unhappy patients;patient dissatisfaction;neural networks;patient data analysis;personalized patient-centered healthcare;explainable data-driven modeling,,,26.0,,,,,IEEE,IEEE Conferences
88,Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution,M. El-Assady; F. Sperrle; O. Deussen; D. Keim; C. Collins,"University of Konstanz, Germany; University of Konstanz, Germany; University of Konstanz, Germany; University of Konstanz, Germany; University of Ontario Institute of Technology, Canada",IEEE Transactions on Visualization and Computer Graphics,,2019,25,1.0,374,384,"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.",1941-0506,,10.1109/TVCG.2018.2864769,DFG; NSERC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8467535,User-Steerable Topic Modeling;Speculative Execution;Mixed-Initiative Visual Analytics;Explainable Machine Learning,Analytical models;Visual analytics;Optimization;Clustering algorithms;Computational modeling;Task analysis,data analysis;data visualisation;decision making;optimisation,topic model optimization;user-steerable speculative execution;potential consequences;human interventions;model-driven analytics systems;visual analytics paradigm;user-steerable preview mechanisms;mixed-initiative topic modeling framework;algorithmic decision-making process;model-space;explicit model manipulation interactions;expected topic seeds;optimization strategies;current model state;model iterations;human-in-the-Ioop process;topic model quality improvements;incremental hierarchical topic modeling algorithm;users domain knowledge;requests feedback;minimum feedback,,6.0,69.0,,,,,IEEE,IEEE Journals
89,EXPLAINER: Entity Resolution Explanations,A. Ebaid; S. Thirumuruganathan; W. G. Aref; A. Elmagarmid; M. Ouzzani,"Purdue University; Qatar Computing Research Institute, HBKU; Purdue University; Qatar Computing Research Institute, HBKU; Qatar Computing Research Institute, HBKU",2019 IEEE 35th International Conference on Data Engineering (ICDE),,2019,,,2000,2003,"Entity Resolution is a fundamental data cleaning and integration problem that has received considerable attention in the past few decades. While rule-based methods have been used in many practical scenarios and are often easy to understand, machine-learning-based methods provide the best accuracy. However, the state-of-the-art classifiers are very opaque. There has been some work towards understanding and debugging the early stages of the entity resolution pipeline, e.g. blocking and generating features (similarity scores). However, there are no such efforts for explaining the model or its predictions. In this demo, we propose ExplainER, a tool to understand and explain entity resolution classifiers with different granularity levels of explanations. Using several benchmark datasets, we will demonstrate how ExplainER can handle different scenarios for a variety of classifiers.",2375-026X,978-1-5386-7474-1,10.1109/ICDE.2019.00224,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8731597,"Entity Resolution, Data Cleaning, Data Integration, Data Cleaning Explanations, Explainable AI",Predictive models;Erbium;Analytical models;Tools;Computational modeling;Pipelines;Visualization,learning (artificial intelligence);pattern classification,machine-learning-based methods;entity resolution pipeline;similarity scores;ExplainER;entity resolution classifiers;EXPLAINER;rule-based methods;data integration problem;data cleaning,,,15.0,,,,,IEEE,IEEE Conferences
90,Interpreting Undesirable Pixels for Image Classification on Black-Box Models,S. Kang; H. Jung; S. Lee,"Korea University, Korea; Korea University, Korea; Korea University, Korea",2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,2019,,,4250,4254,"In an effort to interpret black-box models, researches for developing explanation methods have proceeded in recent years. Most studies have tried to identify input pixels that are crucial to the prediction of a classifier. While this approach is meaningful to analyse the characteristic of black-box models, it is also important to investigate pixels that interfere with the prediction. To tackle this issue, in this paper, we propose an explanation method that visualizes undesirable regions to classify an image as a target class. To be specific, we divide the concept of undesirable regions into two terms: (1) factors for a target class, which hinder that black-box models identify intrinsic characteristics of a target class and (2) factors for non-target classes that are important regions for an image to be classified as other classes. We visualize such undesirable regions on heatmaps to qualitatively validate the proposed method. Furthermore, we present an evaluation metric to provide quantitative results on ImageNet.",2473-9944,978-1-7281-5023-9,10.1109/ICCVW.2019.00523,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022619,Explainable-AI;Interpretability,Visualization;Computer vision;Computational modeling;Predictive models;Perturbation methods;Linear programming;Neural networks,data visualisation;explanation;image classification;image segmentation;neural nets,neural networks;undesirable region visualization;image classification;explanation method;undesirable pixels;black-box models,,,19.0,,,,,IEEE,IEEE Conferences
91,Effective Distributed Representation of Code-Mixed Text,A. Malte; D. S. Sonawane,"Pune Institute of Computer Technology,Department of Computer Engineering,Pune,Maharashtra,India; Pune Institute of Computer Technology,Department of Computer Engineering,Pune,Maharashtra,India",2019 IEEE 16th India Council International Conference (INDICON),,2019,,,1,4,"As an increasing number of people embrace social media, mining data generated from the same has become an important task. Possible applications range from opinion mining, sentiment analysis to hate speech detection. More importantly, analyzing code-mixed multilingual text has gained popularity due to the reason that it holds important socio-cultural clues that may be lost in translation. Methods to effectively analyse code-mixed Hindi/English(Hinglish) text have been explored in this paper. Firstly, we generate a large scale code-mixed corpus that would aid in further research of code mixed text on social media. High-quality word embeddings are trained on this code-mixed text. Finally, we demonstrate the efficacy of our proposed method by training machine learning models that improve upon the previous state-of-the-art using a much lighter and explainable architecture. Our main intention behind training the classifier model was not only high performance but also good model explainability and speed.",2325-9418,978-1-7281-2327-1,10.1109/INDICON47234.2019.9028960,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9028960,Machine Learning;NLP;code-mixing;word embeddings;explainable AI,Social network services;Task analysis;Training;Computer architecture;Machine learning;Computational modeling;Data models,data mining;learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis,code-mixed multilingual text;social media;effective distributed representation;socio-cultural clues;data mining;code-mixed Hindi-English text;large scale code-mixed corpus;high-quality word embeddings;machine learning models;classifier model,,,14.0,,,,,IEEE,IEEE Conferences
92,XINA: Explainable Instance Alignment using Dominance Relationship (Extended Abstract),J. Yeo; H. Park; S. Lee; E. W. Lee; S. Hwang,SK T-Brain; Yonsei University; NAVER Corporation; Emory University; Yonsei University,2019 IEEE 35th International Conference on Data Engineering (ICDE),,2019,,,2135,2136,"In this extended abstract, we present an instance alignment framework, namely XINA, for KB integration. We then show its effectiveness and efficiency on real-world KBs.",2375-026X,978-1-5386-7474-1,10.1109/ICDE.2019.00262,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8731478,Instance alignment;Explainable AI,Knowledge based systems;Feature extraction;Logic gates;Knowledge discovery;Task analysis;Optimization,knowledge based systems,XINA;explainable instance alignment;dominance relationship;KB integration;knowledge base system,,,9.0,,,,,IEEE,IEEE Conferences
93,Automatic Mouse Embryo Brain Ventricle & Body Segmentation and Mutant Classification From Ultrasound Data Using Deep Learning,Z. Qiu; N. Nair; J. Langerman; O. Aristizábal; J. Mamou; D. H. Turnbull; J. A. Ketterling; Y. Wang,"New York University,Electrical and Computer Engineering, Tandon School of Engineering,Brooklyn,USA; New York University,Electrical and Computer Engineering, Tandon School of Engineering,Brooklyn,USA; New York University,Computer Science, Tandon School of Engineering,Brooklyn,USA; Riverside Research,F. L. Lizzi Center for Biomedical Engineering,New York,USA; Riverside Research,F. L. Lizzi Center for Biomedical Engineering,New York,USA; New York University School of Medicine,Skirball Institute of Biomolecular Medicine,New York,USA; Riverside Research,F. L. Lizzi Center for Biomedical Engineering,New York,USA; New York University,Electrical and Computer Engineering, Tandon School of Engineering,Brooklyn,USA",2019 IEEE International Ultrasonics Symposium (IUS),,2019,,,12,15,"High-frequency ultrasound (HFU) is well suited for imaging embryonic mice in vivo because it is non-invasive and real-time. Manual segmentation of the brain ventricles (BVs) and whole body from 3D HFU images is time-consuming and requires specialized training. This paper presents a deep-learning-based segmentation pipeline which automates several time-consuming, repetitive tasks currently performed to study genetic mutations in developing mouse embryos. Namely, the pipeline accurately segments the BV and body regions in 3D HFU images of mouse embryos, despite significant challenges due to position and shape variation of the embryos, as well as imaging artifacts. Based on the BV segmentation, a 3D convolutional neural network (CNN) is further trained to detect embryos with the Engrailed-1 (En1) mutation. The algorithms achieve 0.896 and 0.925 Dice Similarity Coefficient (DSC) for BV and body segmentation, respectively, and 95.8% accuracy on mutant classification. Through gradient based interrogation and visualization of the trained classifier, it is demonstrated that the model focuses on the morphological structures known to be affected by the En1 mutation.",1948-5727,978-1-7281-4596-9,10.1109/ULTSYM.2019.8925720,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8925720,ultrasound;segmentation;mutant classification;visualization;deep learning;explainable ai,Image segmentation;Three-dimensional displays;Mice;Embryo;Training;Ultrasonic imaging;Manuals,biomedical ultrasonics;brain;convolutional neural nets;genetics;image classification;image segmentation;learning (artificial intelligence);medical image processing;neurophysiology;ultrasonic imaging,trained classifier;mutant classification;high-frequency ultrasound;genetic mutations;3D convolutional neural network;Engrailed-1 mutation;Dice similarity coefficient;gradient based interrogation;automatic mouse embryo brain ventricle segmentation;automatic mouse body segmentation;deep learning-based segmentation pipeline,,,12.0,,,,,IEEE,IEEE Conferences
94,Deep Learning in Practice: Guidelines for Model Selection,G. Ramachandran; V. Bhatia,Philips Research; Indraprastha Institute of Information Technology,2019 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM),,2019,,,64,68,"Deep Learning models are becoming the de facto standard in most image, text and speech understanding tasks. Model selection based only on numerical metrics such as accuracy and inference time is not sufficient especially in highly regulated industries like healthcare or autonomous driving where lives are at stake. In order to trust the model, interpreting the reasons behind the model's decisions is essential. We propose a three pronged approach in selecting models based on accuracy, inference time and on whether they learn the right features.",,978-1-7281-6334-5,10.1109/CCEM48484.2019.00014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9051941,deep learning;artificial intelligence;neural networks;explainable AI,,feature selection;learning (artificial intelligence),model selection;Deep Learning models;speech understanding task;image understanding task;text understanding task,,,17.0,,,,,IEEE,IEEE Conferences
95,"Informational Privacy, A Right to Explanation, and Interpretable AI",T. W. Kim; B. R. Routledge,"Tepper Sch. of Bus., Carnegie Mellon Univ., Pittsburgh, PA, USA; Tepper Sch. of Bus., Carnegie Mellon Univ., Pittsburgh, PA, USA",2018 IEEE Symposium on Privacy-Aware Computing (PAC),,2018,,,64,74,"Businesses increasingly utilize secret algorithms and infringe users' informational privacy. We argue that to best protect users' online privacy, the use of an algorithm that assists with decisions or autonomously makes decisions that impact people requires a right to explanation.",,978-1-5386-8442-9,10.1109/PAC.2018.00013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8511831,GDPR;A right to explanation;Trust;Privacy;Explainable AI,Companies;Mathematical model;Privacy;Decision making;Loans and mortgages;Artificial intelligence;Law,artificial intelligence;business data processing;data protection,informational privacy;interpretable AI;online privacy protection,,1.0,5.0,,,,,IEEE,IEEE Conferences
96,Feedback-Based Keyphrase Extraction from Unstructured Text Documents,N. Madaan; M. Saxena; H. Patel; S. Mehta,"IBM Research AI,New Delhi,India; Shiv Nadar University,Gurgaon,India; IBM Research AI,Bangalore,India; IBM Research AI,Bangalore,India",2020 International Conference on COMmunication Systems & NETworkS (COMSNETS),,2020,,,674,676,"Machine Learning experts use classification and tagging algorithms considering the black box nature of these algorithms. These algorithms, primarily key-tags extraction from unstructured text documents are meant to capture key concepts in a document. With increasing amount of data, size and complexity of the data, this problem is key in industrial setup. Different possible use cases being in IT support, conversational systems/ chatbots and financial domains, this problem is important as shown in [1], [2]. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a key-tags extraction framework in the form of natural language. We focus on the problem of key-tags extraction in which the quality of the output can easily be judged by non-experts. Our system automatically reads natural language documents, extracts key concepts and presents an interactive information exploration user interface for analysing these documents.",2155-2509,978-1-7281-3187-0,10.1109/COMSNETS48256.2020.9027399,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9027399,Human-in-the-Loop ML;Keyword Extraction;Explainable AI,Tools;Artificial intelligence;Tagging;Data models;Communication systems;Analytical models;Conferences,interactive systems;learning (artificial intelligence);natural language processing;pattern classification;text analysis;user interfaces,feedback-based keyphrase extraction;unstructured text documents;machine learning experts;classification;tagging algorithms;black box nature;key-tags extraction framework;natural language documents;IT support;conversational systems;chatbots;financial domains;interactive information exploration user interface,,,3.0,,,,,IEEE,IEEE Conferences
97,Explainable Deep Learning Applied to Understanding Opioid Use Disorder and Its Risk Factors,T. E. Workman; Y. Shao; J. Kupersmith; F. Sandbrink; J. L. Goulet; N. M. Shaar; C. Spevak; C. Brandt; M. R. Blackman; Q. Zeng-Treitler,"The George Washington University,Biomedical Informatics Center,Washington,D.C.,U.S.A.; The George Washington University,Biomedical Informatics Center,Washington,D.C.,U.S.A.; Georgetown University,Veterans Initiatives,Washington,D.C.,U.S.A.; Pain Management VA Medical Center,Washington,D.C.,U.S.A.; VA Connecticut Healthcare System,West Haven,CT,U.S.A.; MedStar Health Research,Biostatistics & Biomedical Informatics,Hyattsville,MD,U.S.A.; Georgetown University,Prescription Medication Misuse Program,Washington,D.C.,U.S.A.; Biostatistics Yale University,New Haven,CT,U.S.A.; Research Service VA Medical Center,Washington,D.C.,U.S.A.; The George Washington University,Biomedical Informatics Center,Washington,D.C.,U.S.A.",2019 IEEE International Conference on Big Data (Big Data),,2019,,,4883,4888,"Opioid Use Disorder is an international crisis, affecting many populations. Deep learning models can potentially predict opioid use disorder, but provide little insight to how predictions are derived. Impact scores, a new development in explainable artificial intelligence, measure how individual features affect deep learning outcomes. We modeled clinical visits to predict opioid use disorder, computed impact scores, and compared them to odds log ratios from logistic regression. Impact scores were generally comparable to odds log ratios, in providing insight to opioid abuse risk, but from a better-performing method than logistic regression.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9006297,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006297,Opioid Use Disorder;Explainable AI;Impact Scores;Deep Learning,Machine learning;Logistics;Predictive models;Pain;Training;Correlation,learning (artificial intelligence);medical computing;neural nets;regression analysis,international crisis;logistic regression;opioid abuse risk;computed impact scores;modeled clinical visits;deep learning outcomes;artificial intelligence;deep learning models;opioid use disorder;risk factors,,,14.0,,,,,IEEE,IEEE Conferences
99,"Explaining Explanation, Part 4: A Deep Dive on Deep Nets",R. Hoffman; T. Miller; S. T. Mueller; G. Klein; W. J. Clancey,Institute for Human and Machine Cognition; University of Melbourne; Michigan Technological University; MacroCognition LLC; Institute for Human and Machine Cognition,IEEE Intelligent Systems,,2018,33,3.0,87,95,"This is the fourth in a series of essays about ""explainable AI."" Previous essays laid out the theoretical and empirical foundations. This essay focuses on Deep Nets, and considers methods for allowing system users to generate self-explanations. This is accomplished by exploring how the Deep Net systems perform when they are operating at their ""boundary conditions."" Inspired by recent research into adversarial examples that demonstrate the weaknesses of Deep Nets, we invert the purpose of these adversarial examples and argue that spoofing can be used as a tool to answer contrastive explanation questions via user-driven exploration.",1941-1294,,10.1109/MIS.2018.033001421,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8423529,deep nets;explanation;contrastive explanation;adversarial examples,Cognition;Videos;Cognitive science;Games;Human computer interaction;Artificial intelligence;Machine learning,artificial intelligence;neural nets,system users;Deep Net systems;contrastive explanation questions;explainable AI;boundary conditions;user-driven exploration,,,27.0,,,,,IEEE,IEEE Magazines
100,Lightweight and Effective Facial Landmark Detection using Adversarial Learning with Face Geometric Map Generative Network,H. J. Lee; S. T. Kim; H. Lee; Y. M. Ro,"Image and Video Systems Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea",IEEE Transactions on Circuits and Systems for Video Technology,,2020,30,3.0,771,780,"Facial landmark detection plays an important role in face analysis tasks. Moreover, it is used as a prerequisite in many facial related applications, the simplicity, as well as effectiveness, is essential in the facial landmark detection. In this paper, we propose an effective facial landmark detection network and an associated learning framework with the geometric prior-generative adversarial network. The geometric prior-generative adversarial network consists of one generator and two discriminators. The generator consists of an encoder and two decoders. The encoder predicts facial landmark points. The decoders generate a facial inner and contour geometric map from predicted landmark points. Generating face geometric maps from predicted landmark points helps the predicted landmark points to represent the face geometric information, including shape and configuration. The discriminators determine that the given geometric maps are generated from actual landmark points or estimated landmark points. Our proposed network is end-to-end trainable, and only the encoder part is used simply as the facial landmark detector in the testing stage. To verify the effectiveness of the proposed method, we have conducted comprehensive experiments with benchmark data sets. The results have shown that the proposed method achieves comparable performances over recently proposed facial landmark detection methods with a simple and effective facial landmark detection network.",1558-2205,,10.1109/TCSVT.2019.2897243,"Institute for Information and communications Technology Promotion; Ministry of Science, ICT and Future Planning; ICT R&D Program of MSIT/IITP (2017-0-01779, A Machine Learning and Statistical Inference Framework for Explainable Artificial Intelligence); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633862,Facial landmark detection;geometric prior-generative adversarial network;face geometric map,Face;Generators;Shape;Geometry;Task analysis;Decoding;Gallium nitride,face recognition;feature extraction;geometry;learning (artificial intelligence);neural nets,face geometric map generative network;facial related applications;effective facial landmark detection network;prior-generative adversarial network;facial landmark points;predicted landmark points;face geometric information;actual landmark points;estimated landmark points;facial landmark detector;associated learning framework;face analysis tasks,,1.0,37.0,IEEE,,,,IEEE,IEEE Journals
101,MCSIP Net: Multichannel Satellite Image Prediction via Deep Neural Network,J. Lee; S. S. Lee; H. G. Kim; S. Song; S. Kim; Y. M. Ro,"Image and Video Systems Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Ybrain, Seongnam-si, South Korea; Signal Processing Laboratory, Institute of Electrical Engineering, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; Research Data Sharing Center, Korea Institute of Science and Technology (KISTI), Daejeon, South Korea; Research Data Sharing Center, Korea Institute of Science and Technology (KISTI), Daejeon, South Korea; Image and Video Systems Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea",IEEE Transactions on Geoscience and Remote Sensing,,2020,58,3.0,2212,2224,"Satellite image prediction is important in weather nowcasting. In this article, we propose a novel multichannel satellite image prediction network (MCSIP Net) for predicting satellite images. The proposed MCSIP Net consists of three parts such as the satellite image predictor, the spatio-temporal 3-D discriminators, and the domain knowledge critic networks. The satellite image predictor takes a multichannel satellite image as an input and predicts a multichannel satellite image by learning spatio-temporal characteristics of each input channel. The spatio-temporal 3-D discriminators are trained to distinguish whether the input satellite image consists of a real satellite image or predicted image. By learning the spatio-temporal 3-D discriminator to distinguish and the satellite image predictor to deceive, the satellite image predictor can generate satellite image more similar to real satellite image distribution. The domain knowledge critic networks take the satellite image and the corresponding analysis data (which is obtained from a meteorological model) as an input and learn to distinguish whether the input satellite image is real or predicted on the basis of the analysis data. By utilizing the analysis data, the proposed MCSIP Net could take the meteorological knowledge into account efficiently. For the purpose of verification of the proposed method, ablation study and qualitative evaluation were conducted. Experimental results demonstrated that the proposed MCSIP Net could be learned efficiently and predict a multichannel satellite image with remarkable quality.",1558-0644,,10.1109/TGRS.2019.2955538,Institute for Information and Communications Technology Planning and Evaluation (IITP); Korean Government (MSIT) (machine learning and statistical inference framework for explainable artificial intelligence); Korea Institute of Science and Technology Information; Brain Korea 21 Plus Project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933126,Image prediction;multichannel satellite image prediction;satellite image prediction,,artificial satellites;geophysical image processing;learning (artificial intelligence);neural nets;spatiotemporal phenomena;weather forecasting,MCSIP Net;satellite image predictor;domain knowledge critic networks;input satellite image;satellite image distribution;spatiotemporal 3D discriminators;multichannel satellite image prediction network;deep neural network;weather nowcasting;meteorological knowledge,,,38.0,IEEE,,,,IEEE,IEEE Journals
102,BBC Net: Bounding-Box Critic Network for Occlusion-Robust Object Detection,J. U. Kim; J. Kwon; H. G. Kim; Y. M. Ro,"Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea",IEEE Transactions on Circuits and Systems for Video Technology,,2020,30,4.0,1037,1050,"Object detection has received significant interest in the research field of computer vision and is widely used in human-centric applications. The occlusion problem is a frequent obstacle that degrades detection quality. In this paper, we propose a novel object detection framework targeting robust object detection in occlusion. The proposed deep learning-based network consists mainly of two parts: 1) object detection framework, which classifies the object categories and localizes the object location and 2) plug-in bounding-box (BB) estimator, which estimates the object and occlusion region from the feature map of the backbone network and the corresponding critic network for evaluating the predicted BB map. The BB estimator and the critic network are the plug-in modules added to the object detection framework and learned competitively with adversarial manner. As the plug-in BB estimator is learned to estimate the BB map containing the object and occlusion pattern information, the backbone network can embed this information to enable robust detection under occlusion in the test phase. The comprehensive experimental results on the PASCAL VOC, MS COCO, and KITTI dataset showed that the performance is improved with the plug-in BB-Critic network by predicting and criticizing object and occlusion in general generic object detection framework.",1558-2205,,10.1109/TCSVT.2019.2900709,Institute for Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT); A machine learning and statistical inference framework for explainable artificial intelligence); KEPRI; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8649739,Object detection;occlusion;deep learning;actor-critic network,Object detection;Feature extraction;Deep learning;Encoding;Object recognition;Detectors;Proposals,computer vision;learning (artificial intelligence);neural nets;object detection,occlusion-robust object detection;occlusion problem;deep learning-based network;object categories;object location;bounding-box estimator;backbone network;predicted BB map;BB estimator;robust detection;BB-Critic;general generic object detection framework;bounding-box critic network;BBC Net,,2.0,46.0,IEEE,,,,IEEE,IEEE Journals
103,Factual and Counterfactual Explanations for Black Box Decision Making,R. Guidotti; A. Monreale; F. Giannotti; D. Pedreschi; S. Ruggieri; F. Turini,ISTI CNR; University of Pisa; ISTI CNR; University of Pisa; University of Pisa; University of Pisa,IEEE Intelligent Systems,,2019,34,6.0,14,23,"The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.",1941-1294,,10.1109/MIS.2019.2957223,European Commission; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920138,Explainable AI;Interpretable Machine Learning;Open the Black Box;Explanation Rules;Counterfactuals,Genetic algorithms;Intelligent systems;Decision making;Decision trees;Machine learning algorithms;Prediction algorithms;Data models,decision making;genetic algorithms;learning (artificial intelligence);pattern classification,local classifier;synthetic neighborhood;genetic algorithm;interpretable classifier;decision rule;factual reasons;instance features;black box decision making;sophisticated machine learning models;accurate but obscure decision systems;transparency;artificial intelligence;safety-critical contexts;local rule-based explanation method;faithful explanations;black box classifier;specific instance,,,10.0,IEEE,,,,IEEE,IEEE Magazines
104,Improving User Trust on Deep Neural Networks Based Intrusion Detection Systems,K. Amarasinghe; M. Manic,"Virginia Commonwealth University, Richmond, Virginia, USA; Virginia Commonwealth University, Richmond, Virginia, USA",IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,,2018,,,3262,3268,"Deep Neural Networks based intrusion detection systems (DNN-IDS) have proven to be effective. However, in domains like critical infrastructure security, user trust on the DNN-IDS is imperative and high accuracy isn't sufficient. The black-box nature of DNNs hinders transparency of the DNN-IDS, which is necessary for building trust. The main objective of this work is to improve user trust by improving transparency of the DNN-IDS by making it more communicative. This paper presents a methodology to generate offline and online feedback to the user on the decision making process of the DNN-IDS. Offline, the user is reported the input features that are most relevant in detecting each type of intrusion by the trained DNN-IDS. Online, for each detection, the user is reported the inputs features that contributed most to the detection. The presented method was implemented on the KDD-NSL dataset with a multi-layer perceptron (MLP) based DNN-IDS. Binary and multi-class classification was carried out on the dataset. Further, several DNN-IDS architectures with different depth were tested to study the factors that drive classification. It was observed that despite showing very similar accuracy results, the factors that drove the decisions were different across architectures. This evidences that the qualitative analysis that is enabled through reporting relevant input features is important for the user to make a more informed decision in choosing a DNN-IDS. This online and offline feedback leads to improving the transparency of the DNN-IDS and helps build trust prior to and during deployment.",2577-1647,978-1-5090-6684-1,10.1109/IECON.2018.8591322,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8591322,Intrusion Detection;Deep Learning;Deep Neural Networks;Explainable AI;Layer wise Relevance Propagation;Anomaly Detection,Neurons;Intrusion detection;Neural networks;Feature extraction;Critical infrastructure;Computer architecture;Training,decision making;multilayer perceptrons;pattern classification;security of data;trusted computing,deep neural networks based intrusion detection systems;multilayer perceptron based DNN-IDS;user trust;decision making process;black-box nature;KDD-NSL dataset;binary classification;multiclass classification,,3.0,23.0,,,,,IEEE,IEEE Conferences
105,OncoNetExplainer: Explainable Predictions of Cancer Types Based on Gene Expression Data,M. R. Karim; M. Cochez; O. Beyan; S. Decker; C. Lange,Fraunhofer FIT; Vrije Universiteit Amsterdam; RWTH Aachen University; Fraunhofer FIT; Fraunhofer FIT,2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE),,2019,,,415,422,"The discovery of important biomarkers is a significant step towards understanding the molecular mechanisms of carcinogenesis; enabling accurate diagnosis for, and prognosis of, a certain cancer type. Before recommending any diagnosis, genomics data such as gene expressions (GE) and clinical outcomes need to be analyzed. However, complex nature, high dimensionality, and heterogeneity in genomics data make the overall analysis challenging. Convolutional neural networks (CNN) have shown tremendous success in solving such problems. However, neural network models are perceived mostly as 'black box' methods because of their not well-understood internal functioning. However, interpretability is important to provide insights on why a given cancer case has a certain type. Besides, finding the most important biomarkers can help in recommending more accurate treatments and drug repositioning. Moreover, the 'right to explanation' of the EU GDPR gives patients the right to know why and how an algorithm made a diagnosis decision. Hence, in this paper, we propose a new approach called OncoNetExplainer to make explainable predictions of cancer types based on GE data. We used genomics data about 9,074 cancer patients covering 33 different cancer types from the Pan-Cancer Atlas on which we trained CNN and VGG16 networks using guided-gradient class activation maps++ (GradCAM++). Further, we generate class-specific heat maps to identify significant biomarkers and computed feature importance in terms of mean absolute impact to rank top genes across all the cancer types. Quantitative and qualitative analyses show that both models exhibit high confidence at predicting the cancer types correctly giving an average precision of 96.25%. To provide comparisons with the baselines, we identified top genes, and cancer-specific driver genes using gradient boosted trees and SHapley Additive exPlanations (SHAP). Finally, our findings were validated with the annotations provided by the TumorPortal.",2471-7819,978-1-7281-4617-1,10.1109/BIBE.2019.00081,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941872,"Cancer genomics, Gene expression, Neural networks, GradCAM++, Feature importance, Interpretability, Explainable AI.",,cancer;drugs;genetics;genomics;medical diagnostic computing;neural nets;patient diagnosis;patient treatment,explainable predictions;gene expression data;biomarkers;genomics data;clinical outcomes;neural network models;GE data;cancer types;Pan-Cancer Atlas;cancer-specific driver genes,,,28.0,,,,,IEEE,IEEE Conferences
106,Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models,H. Strobelt; S. Gehrmann; M. Behrisch; A. Perer; H. Pfister; A. M. Rush,IBM ReseatchMIT-IBM Watson AI Lab.; Harvard NLP group; Hatvatd Visual Computing group; IBM ReseatchMIT-IBM Watson AI Lab.; Hatvatd Visual Computing group; Harvard NLP group,IEEE Transactions on Visualization and Computer Graphics,,2019,25,1.0,353,363,"Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and “what if”-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.",1941-0506,,10.1109/TVCG.2018.2865044,Google research; MIT-IBM Watson AI Lab; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8494828,Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP,Analytical models;Visualization;Tools;Predictive models;Machine learning;Data models;Atmosphere,data visualisation;learning (artificial intelligence);neural nets;program debugging;sequences,seq2seq-Vis;source sequence;target sequence;visual debugging tool;neural sequence-to-sequence models;blackbox pipeline;vector space;deep learning methods;visual analysis tool,,16.0,55.0,,,,,IEEE,IEEE Journals
107,Towards Balancing the Complexity of Convolutional Neural Network with the Role of Optical Coherence Tomography in Retinal Conditions,A. Marginean; A. Groza; S. D. Nicoara; G. Muntean; R. R. Slavescu; I. A. Letia,"University of Cluj-Napoca,Department of Computer Science Technical,Cluj-Napoca,Romania; University of Cluj-Napoca,Department of Computer Science Technical,Cluj-Napoca,Romania; University of Medicine and Pharmacy,Department of Ophthalmology “Iuliu Hatieganu”,Cluj-Napoca,Romania; University of Medicine and Pharmacy,Department of Ophthalmology “Iuliu Hatieganu”,Cluj-Napoca,Romania; University of Cluj-Napoca,Department of Computer Science Technical,Cluj-Napoca,Romania; University of Cluj-Napoca,Department of Computer Science Technical,Cluj-Napoca,Romania",2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing (ICCP),,2019,,,475,482,"Convolutional neural networks have shown impressive performance in the medical image domain, but medical experts are somewhat skeptical in their predictions since the features are not directly graspable. We are looking into one of the technical challenges, namely the explainability of the results, to try to find out some demonstration of the regions deemed abnormal by deep learning. Following the trend of heatmaps indicating which local morphology changes would modify the predictions, we are trying to verify the facilitation of the clinical understanding in the eyes of the ophthalmologist.",,978-1-7281-4914-1,10.1109/ICCP48234.2019.8959714,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8959714,Convolutional Neural Networks;Optical Coherence Tomography;diagnosis of retinal conditions;occlusion test;explainable AI,,biomedical optical imaging;eye;filtering theory;learning (artificial intelligence);medical image processing;neural nets;optical tomography,convolutional neural network;medical image domain;medical experts;optical coherence tomography;retinal conditions;ophthalmologist,,,22.0,,,,,IEEE,IEEE Conferences
108,Editable AI: Mixed Human-AI Authoring of Code Patterns,K. Chugh; A. Y. Solis; T. D. LaToza,"Department of Computer Science, University of Virginia Charlottesville, VA, USA; Department of Computer Science, George Mason University Fairfax, VA, USA; Department of Computer Science, George Mason University Fairfax, VA, USA",2019 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC),,2019,,,35,43,"Developers authoring HTML documents define elements following patterns which establish and reflect the visual structure of a document, such as making all images in a footer the same height by applying a class to each. To surface these patterns to developers and support developers in authoring consistent with these patterns, we propose a mixed human-AI technique for creating code patterns. Patterns are first learned from individual HTML documents through a decision tree, generating a representation which developers may view and edit. Code patterns are used to offer developers autocomplete suggestions, list examples, and flag violations. To evaluate our technique, we conducted a user study in which 24 participants wrote, edited, and corrected HTML documents. We found that our technique enabled developers to edit and correct documents more quickly and create, edit, and correct documents more successfully.",1943-6106,978-1-7281-0810-0,10.1109/VLHCC.2019.8818871,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818871,Explainable AI;Autocomplete;Example-Centric Programming;Decision Trees;Development Environments,Iris;Decision trees;Visualization;Feature extraction;Training data;Computational modeling;Computer science,artificial intelligence;authoring systems;decision trees;hypermedia markup languages;Internet;user interfaces,code patterns;developers autocomplete suggestions;editable AI;human-AI authoring;support developers;human-AI technique;HTML documents,,,24.0,,,,,IEEE,IEEE Conferences
109,Generalized PatternAttribution for Neural Networks with Sigmoid Activations,J. Sun; A. Binder,"Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore",2019 International Joint Conference on Neural Networks (IJCNN),,2019,,,1,9,"Explanation methods for deep neural networks (DNN) such as LRP [1], PatternAttribution [2], LIME [3], DeepLIFT [4] have promoted the explanation of convolutional neural networks (CNN). Results are obtained mostly using ReLU activation function. In this paper, we investigate the performance of explanation methods on neural networks with sigmoid activations like sigmoid and tanh. PatternAttribution is a recent approach which allows learning explanation patterns from data. We show that the saturated zones of sigmoids pose difficulties for PatternAttribution. In order to solve these issues, as a first novelty, we generalize global explanations to piece-wise dependent explanations. In a second contribution, we learn the parameters of PatternAttribution in near-linear activation zones of the sigmoids, while replacing it with LRP in the saturated zones. Finally, we introduce and evaluate direct layer-wise Taylor approximation to show that LRP as a deep-Taylor-motivated approach outperforms the ad-hoc application of Taylor approximation. We show results on MNIST and also on LSTM and GRU networks used for two sentiment classification tasks as important application cases of models using sigmoids. Our results demonstrate that the proposed method, Piece-wise PAtternLRP (PPAP), outperforms PatternAttribution as well as LRP for networks with sigmoids, thus combining their strengths effectively.",2161-4407,978-1-7281-1985-4,10.1109/IJCNN.2019.8851761,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851761,explainable AI;interpretability;deep neural networks;recurrent neural networks;LRP;PatternAttribution,Biological neural networks;Neurons;Task analysis;Recurrent neural networks;Correlation;Backpropagation,approximation theory;convolutional neural nets;learning (artificial intelligence);pattern classification;transfer functions,saturated zones;direct layer-wise Taylor approximation;deep-Taylor-motivated approach;sigmoid activations;deep neural networks;convolutional neural networks;ReLU activation function;explanation patterns;piece-wise dependent explanations;near-linear activation zones;LRP;DeepLIFT;generalized pattern attribution;LSTM networks;GRU networks;sentiment classification tasks;piece-wise PAtternLRP,,,23.0,,,,,IEEE,IEEE Conferences
110,Generation of Movement Explanations for Testing Gesture Based Co-Operative Learning Applications,A. Banerjee; I. Lamrani; P. Paudyal; S. Gupta,Arizona State University; Arizona State University; Arizona State University; Arizona State University,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),,2019,,,9,16,"The paper proposes an explanation framework for machine learning based gesture recognition systems to increase trust, and also provide users an interface to ask questions about the recognition result. Gestures have three components: a) handshape, b) location, and c) movement. Several techniques exist for handshape and location recognition explanation, but very limited analysis exists on explaining movement. The challenge is that modeling the movement between handshapes in a gesture require dynamic modeling of arm kinematics using differential equations. The arm models can be of various complexity, but many of them may not be explainable. Our approach in this paper, is to mine hybrid system models of gestures using a coalition of hand-shape recognition technology and explainable kinematic models. The hybrid dynamical systems are mined using video data collected from users. Change in dynamics of a test user is expressed using the parameters of the kinematic equations. The parameters are converted into human understandable explanations by experts in movement analysis. The novel outcome is the combination of fault detection in hybrid dynamical systems and machine learning to provide explanation for recognition of continuous events. We have applied our technique on 60 users for 20 ASL gestures. Results show that the mined parameters of the kinematic equations can represent each gesture with precision of 83 %, recall of 80 % and accuracy of 82 %.",,978-1-7281-0492-8,10.1109/AITest.2019.00-15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718241,"artificial intelligence, testing, explainable AI, gesture recognition",Shape;Mathematical model;Kinematics;Data models;Computational modeling;Data mining;Testing,gesture recognition;learning (artificial intelligence);shape recognition,human understandable explanations;movement analysis;hybrid dynamical systems;machine learning;mined parameters;kinematic equations;movement explanations;gesture recognition systems;location recognition explanation;arm kinematics;arm models;hybrid system models;hand-shape recognition technology;ASL gestures;kinematic models;video data;data mining;gesture based co-operative learning applications,,,47.0,,,,,IEEE,IEEE Conferences
111,Plan Explanations as Model Reconciliation -- An Empirical Study,T. Chakraborti; S. Sreedharan; S. Grover; S. Kambhampati,"IBM Research AI, AI Composition Lab, Cambridge, MA, USA; Computer Science, Arizona State University, Tempe, AZ, USA; Computer Science, Arizona State University, Tempe, AZ, USA; Computer Science, Arizona State University, Tempe, AZ, USA",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,2019,,,258,266,"Recent work in explanation generation for decision making agents has looked at how unexplained behavior of autonomous systems can be understood in terms of differences in the model of the system and the human's understanding of the same, and how the explanation process as a result of this mismatch can be then seen as a process of reconciliation of these models. Existing algorithms in such settings, while having been built on contrastive, selective and social properties of explanations as studied extensively in the psychology literature, have not, to the best of our knowledge, been evaluated in settings with actual humans in the loop. As such, the applicability of such explanations to human-AI and human-robot interactions remains suspect. In this paper, we set out to evaluate these explanation generation algorithms in a series of studies in a mock search and rescue scenario with an internal semi-autonomous robot and an external human commander. During that process, we hope to demonstrate to what extent the properties of these algorithms hold as they are evaluated by humans.",2167-2148,978-1-5386-8555-6,10.1109/HRI.2019.8673193,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673193,Explainable AI;planning and decision-making;human-robot interaction;explanations as model reconciliation,Robots;Planning;Cognitive science;Artificial intelligence;Decision making;Computer science;Task analysis,decision making;human-robot interaction;mobile robots;psychology,plan explanations;model reconciliation;decision making agents;autonomous systems;explanation process;contrastive properties;social properties;psychology literature;human-robot interactions;explanation generation algorithms;semiautonomous robot;external human commander;human-AI interactions;mock search and rescue scenario,,,43.0,,,,,IEEE,IEEE Conferences
112,Health Intervention Evaluation Using Semantic Explainability and Causal Reasoning,J. H. Brenas; A. Shaban-Nejad,"Department of Pediatrics, College of Medicine, The University of Tennessee Health Science Center – Oak-Ridge National Laboratory (UTHSC-ORNL) Center for Biomedical Informatics, Memphis, TN, USA; Department of Pediatrics, College of Medicine, The University of Tennessee Health Science Center – Oak-Ridge National Laboratory (UTHSC-ORNL) Center for Biomedical Informatics, Memphis, TN, USA",IEEE Access,,2020,8,,9942,9952,"As serious public health problems require complex responses, health interventions often involve multiple components implemented by groups including policy experts, social workers, and health practitioners. The success or failure of an intervention depends on many different factors, ranging from available resources to characteristics of the targeted public health issue and community to the complex mechanics relating cause and effects of the actions performed. In this paper, we present a novel formal methodology to evaluate public health interventions, policies, and programs. Our method uses the theory of change (TOC) approach along with logic models that define the intervention under consideration to generate a causal diagram and an ontology-based inference model for causal description. The resulting causal diagram will then be compared to existing knowledge and data to determine whether the intervention is coherent, internally consistent and its goals are achievable in the allotted time with the resources provided. The contextual knowledge and semantics provided by the ontology will generate a more explainable, understandable, and trustworthy approach to compare and assess different interventions based on their shared goals. Depending upon the quality and quantity of data available we perform a mix of qualitative and quantitative evaluation of the interventions. This study uses smoking cessation interventions to showcase the proposed methodology in action.",2169-3536,,10.1109/ACCESS.2020.2964802,"Health Science Center, University of Tennessee; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952710,Causal graphs;intervention evaluation;logic models;ontologies;explainable AI;public health program evaluation,Ontologies;Public healthcare;Semantics;Cognition;Adaptation models;Biological system modeling;Sociology,causality;health care;inference mechanisms;medical computing;medical information systems;ontologies (artificial intelligence),health intervention evaluation;semantic explainability;causal reasoning;public health problems;complex responses;policy experts;health practitioners;public health issue;complex mechanics;public health interventions;ontology-based inference model;causal description;causal diagram;qualitative evaluation;quantitative evaluation;cessation interventions,,,43.0,CCBY,,,,IEEE,IEEE Journals
113,Explaining Visual Classification using Attributes,M. ul Hassan; P. Mulhem; D. Pellerin; G. Quénot,"Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, Grenoble, F-38000, France; Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, Grenoble, F-38000, France; Univ. Grenoble Alpes, CNRS, GIPSA-Lab, Grenoble, F-38000, France; Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, Grenoble, F-38000, France",2019 International Conference on Content-Based Multimedia Indexing (CBMI),,2019,,,1,6,"The performance of deep Convolutional Neural Networks (CNN) has been reaching or even exceeding the human level on large number of tasks. Some examples are image classification, Mastering Go game, speech understanding etc. However, their lack of decomposability into intuitive and understandable components make them hard to interpret, i.e. no information is provided about what makes them arrive at their prediction. We propose a technique to interpret CNN classification task and justify the classification result with visual explanation and visual search. The model consists of two sub networks: a deep recurrent neural network for generating textual justification and a deep convolutional network for image analysis. This multimodal approach generates the textual justification about the classification decision. To verify the textual justification, we use the visual search to extract the similar content from the training set. We evaluate our strategy on a novel CUB dataset with the ground-truth attributes. We make use of these attributes to further strengthen the justification by providing the attributes of images.",1949-3991,978-1-7281-4673-7,10.1109/CBMI.2019.8877393,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877393,Explainable AI;Deep Neural Networks;Interpretability,Visualization;Feature extraction;Training;Birds;Artificial intelligence;Task analysis;Neural networks,convolutional neural nets;image classification;recurrent neural nets,visual search;deep recurrent neural network;textual justification;image analysis;multimodal approach;ground-truth attributes;visual classification;deep convolutional neural networks;image classification;Mastering Go game;speech understanding;CNN classification task;visual explanation,,,36.0,,,,,IEEE,IEEE Conferences
114,Why the Failure? How Adversarial Examples Can Provide Insights for Interpretable Machine Learning,R. Tomsett; A. Widdicombe; T. Xing; S. Chakraborty; S. Julier; P. Gurram; R. Rao; M. Srivastava,"IBM Research, IBM Emerging Technology, Hursley, UK; UCL, Dept of Computer Science, London, UK; UCLA, Electrical & Computer Engineering Dept, Los Angeles, USA; IBM Research, Yorktown, USA; UCL, Dept of Computer Science, London, UK; Reseach Laboratory, Booz Allen Hamilton Army, Adelphi, USA; Army Research Laboratory, Adelphi, USA; Electrical & Computer Engineering Dept, UCLA, Computer Science Dept, Los Angeles, USA",2018 21st International Conference on Information Fusion (FUSION),,2018,,,838,845,"Recent advances in Machine Learning (ML) have profoundly changed many detection, classification, recognition and inference tasks. Given the complexity of the battlespace, ML has the potential to revolutionise how Coalition Situation Understanding is synthesised and revised. However, many issues must be overcome before its widespread adoption. In this paper we consider two - interpretability and adversarial attacks. Interpretability is needed because military decision-makers must be able to justify their decisions. Adversarial attacks arise because many ML algorithms are very sensitive to certain kinds of input perturbations. In this paper, we argue that these two issues are conceptually linked, and insights in one can provide insights in the other. We illustrate these ideas with relevant examples from the literature and our own experiments.",,978-0-9964527-6-2,10.23919/ICIF.2018.8455710,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455710,interpretability;interpretable machine learning;deep learning;adversarial machine learning;adversarial examples;explainable AI;AI alignment;internet of battlefield things,Task analysis;Machine learning;Measurement;Data models;Taxonomy;Internet;Sensors,decision making;inference mechanisms;learning (artificial intelligence);military computing,interpretable machine learning;ML algorithms;military decision-makers;adversarial attacks;Coalition Situation Understanding;inference tasks,,2.0,54.0,,,,,IEEE,IEEE Conferences
116,CNN Fixations: An Unraveling Approach to Visualize the Discriminative Image Regions,K. R. Mopuri; U. Garg; R. Venkatesh Babu,"Department of Computational and Data Sciences, Video Analytics Laboratory, Indian Institute of Science, Bengaluru, India; Department of Computational and Data Sciences, Video Analytics Laboratory, Indian Institute of Science, Bengaluru, India; Department of Computational and Data Sciences, Video Analytics Laboratory, Indian Institute of Science, Bengaluru, India",IEEE Transactions on Image Processing,,2019,28,5.0,2116,2125,"Deep convolutional neural networks (CNNs) have revolutionized the computer vision research and have seen unprecedented adoption for multiple tasks, such as classification, detection, and caption generation. However, they offer little transparency into their inner workings and are often treated as black boxes that deliver excellent performance. In this paper, we aim at alleviating this opaqueness of CNNs by providing visual explanations for the network's predictions. Our approach can analyze a variety of CNN-based models trained for computer vision applications, such as object recognition and caption generation. Unlike the existing methods, we achieve this via unraveling the forward pass operation. The proposed method exploits feature dependencies across the layer hierarchy and uncovers the discriminative image locations that guide the network's predictions. We name these locations CNN fixations, loosely analogous to human eye fixations. Our approach is a generic method that requires no architectural changes, additional training, or gradient computation, and computes the important image locations (CNN fixations). We demonstrate through a variety of applications that our approach is able to localize the discriminative image locations across different network architectures, diverse vision tasks, and data modalities.",1941-0042,,10.1109/TIP.2018.2881920,Qualcomm; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8537979,Explainable AI;CNN visualization;visual explanations;label localization;weakly supervised localization,Neurons;Visualization;Task analysis;Computer architecture;Training;Network architecture;Convolution,computer vision;convolutional neural nets;feature extraction;object recognition,CNN fixations;discriminative image regions;caption generation;visual explanations;CNN-based models;computer vision applications;object recognition;forward pass operation;discriminative image locations;human eye fixations;diverse vision tasks;deep convolutional neural networks;image locations;network architectures;feature dependencies,,,39.0,,,,,IEEE,IEEE Journals
117,Toward Explainable Deep Neural Network Based Anomaly Detection,K. Amarasinghe; K. Kenney; M. Manic,"Virginia Commonwealth University, Richmond, Virginia, USA; Idaho National Laboratory, Idaho Falls, Idaho, USA; Virginia Commonwealth University, Richmond, Virginia, USA",2018 11th International Conference on Human System Interaction (HSI),,2018,,,311,317,"Anomaly detection in industrial processes is crucial for general process monitoring and process health assessment. Deep Neural Networks (DNNs) based anomaly detection has received increased attention in recent work. Albeit their high accuracy, the black-box nature of DNNs is a drawback in practical deployment. Especially in industrial anomaly detection systems, explanations of DNN detected anomalies are crucial. This paper presents a framework for DNN based anomaly detection which provides explanations of detected anomalies. The framework answers the following questions during online processing: 1) “why is it an anomaly?” and 2) “what is the confidence?” Further, the framework can be used offline to evaluate the “knowledge” of the trained DNN. The framework reduces the opaqueness of the DNN based anomaly detector and thus improves human operators' trust in the algorithm. This paper implements the first steps of the presented framework on the benchmark KDD-NSL dataset for Denial of Service (DoS) attack detection. Offline DNN explanations showed that the DNN was detecting DoS attacks based on features indicating destination of connection, frequency and amount of data transferred while showing an accuracy around 97%.",,978-1-5386-5024-0,10.1109/HSI.2018.8430788,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8430788,Deep Learning;Deep Neural Networks;Explainable AI;Layer wise Relevance Propagation;Anomaly Detection,,neural nets;security of data;trusted computing,industrial processes;process health assessment;industrial anomaly detection systems;DNN;deep neural network;process monitoring;Denial of Service attack detection;human operators trust,,4.0,38.0,,,,,IEEE,IEEE Conferences
119,Interpretable Approximation of a Deep Reinforcement Learning Agent as a Set of If-Then Rules,S. Nageshrao; B. Costa; D. Filev,Ford Motor Co; Ford Motor Co; Ford Motor Co,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),,2019,,,216,221,"In many industrial applications, one of the major bottlenecks in using advanced learning-based methods (such as reinforcement learning) for controls is the lack of interpretability of the trained agent. In this paper, we present a methodology for translating a trained reinforcement learning agent into a set of simple and easy to interpret if-then rules by using the proven universal approximation property of the rules with fuzzy predicates. Proposed methodology combines the optimality of reinforcement learning with interpretability of the theory of approximate reasoning, thus making reinforcement learning-based solutions more accessible to industrial practitioners. The framework presented in this paper has the potential to help address the fundamental problem in widespread adoption of reinforcement learning in industrial applications.",,978-1-7281-4550-1,10.1109/ICMLA.2019.00041,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999226,Reinforcement learning;car following;explainable AI;Fuzzy rules,Acceleration;Lead;Automobiles;Training;Learning (artificial intelligence);Machine learning;Task analysis,approximation theory;inference mechanisms;learning (artificial intelligence),interpretable approximation;advanced learning-based methods;deep reinforcement learning agent;universal approximation property;approximate reasoning;reinforcement learning-based solutions;fuzzy predicates;if-then rules,,,19.0,,,,,IEEE,IEEE Conferences
120,Interpreting Social Media-Based Substance Use Prediction Models with Knowledge Distillation,T. Ding; F. Hasan; W. K. Bickel; S. Pan,"Inf. Syst. Dept., Univ. of Maryland, Baltimore, MD, USA; Inf. Syst. Dept., Univ. of Maryland, Baltimore, MD, USA; Addiction Recovery Res. Center, Virginia Tech, Blacksburg, VA, USA; Inf. Syst. Dept., Univ. of Maryland, Baltimore, MD, USA",2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI),,2018,,,623,630,"People nowadays spend a significant amount of time on social media such as Twitter, Facebook, and Instagram. As a result, social media data capture rich human behavioral evidence that can be used to help us understand their thoughts, behavior and decision making process. Social media data, however, are mostly unstructured (e.g., text and images) and may involve a large number of raw features (e.g., millions of raw text and image features). Moreover, the ground truth data about human behavior and decision making could be difficult to obtain at a large scale. As a result, most state-of-the-art social media-based human behavior models employ sophisticated unsupervised feature learning to leverage a large amount of unsupervised data. Unfortunately, these advanced models often rely on latent features that are hard to explain. Since understanding the knowledge captured in these models is important for behavior scientists, public health providers as well as policymakers, in this research, we focus on employing a knowledge distillation framework to build machine learning models with not only state-of-the-art predictive performance but also interpretable results. We evaluate the effectiveness of the proposed framework in explaining Substance Use Disorder (SUD) prediction models. Our best models achieved 87% ROC AUC for predicting tobacco use, 84% for alcohol use and 93% for drug use, which are comparable to existing state-of-the-art SUD prediction models. Since these models are also interpretable (e.g., a logistics regression model and a gradient boosting tree model), we combine the results from these models to gain insight into the relationship between a user's social media behavior (e.g., social media likes and word usage) and substance use.",2375-0197,978-1-5386-7449-9,10.1109/ICTAI.2018.00100,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576098,"human behavior, social media, substance use disorders, explainable AI, interpretable AI",Data models;Predictive models;Neural networks;Facebook;Analytical models;Drugs,decision making;learning (artificial intelligence);regression analysis;social networking (online);social sciences computing,tree model;social media data capture;decision making;raw features;raw text;image features;ground truth data;sophisticated unsupervised feature;unsupervised data;advanced models;latent features;behavior scientists;knowledge distillation framework;machine learning models;logistics regression model;social media-based substance use prediction models;human behavioral evidence;social media-based human behavior models;substance use disorder prediction models;SUD prediction models,,1.0,33.0,,,,,IEEE,IEEE Conferences
121,Inducing Readable Oblique Decision Trees,A. Leroux; M. Boussard; R. Dés,"Craft ai, Paris, France; Craft ai, Paris, France; Craft ai, Paris, France",2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI),,2018,,,401,408,"Although machine learning models are found in more and more practical applications, stakeholders can be suspicious about the fact that they are not hard-coded and fully specified. To foster trust, it is crucial to provide models whose predictions are explainable. Decision Trees can be understood by humans if they are simple enough, but they suffer in accuracy when compared to other common machine learning methods. Oblique Decision Trees can provide better accuracy and smaller trees, but their decision rules are more complex. This article presents MUST (Multivariate Understandable Statistical Tree), an Oblique Decision Tree split algorithm based on Linear Discriminant Analysis that aims to preserve explainability by limiting the number of variables that appear in decision rules.",2375-0197,978-1-5386-7449-9,10.1109/ICTAI.2018.00069,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576067,Oblique Decision Tree;Decision trees;Explainable AI;Linear discriminant analysis;Machine Learning,Decision trees;Heuristic algorithms;Covariance matrices;Machine learning;Complexity theory;Optimization,decision trees;learning (artificial intelligence),decision rules;inducing readable Oblique Decision;machine learning models;practical applications;stakeholders;common machine learning methods;smaller trees;oblique decision trees;oblique decision tree split algorithm;understandable statistical tree,,,24.0,,,,,IEEE,IEEE Conferences
123,Explainable Classifier Supporting Decision-making for Breast Cancer Diagnosis from Histopathological Images,P. Sabol; P. Sinčák; K. Ogawa; P. Hartono,"Department of Cybernetics and Artificial Intelligence, Technical University of Košice, Košice, Slovakia; Department of Cybernetics and Artificial Intelligence, Technical University of Košice, Košice, Slovakia; School of Engineering, Chukyo University, Nagoya, Japan; School of Engineering, Chukyo University, Nagoya, Japan",2019 International Joint Conference on Neural Networks (IJCNN),,2019,,,1,8,"This paper presents an application of semantically explainable classifier, Cumulative Fuzzy Class Membership Criterion (CFCMC), in medical domain, specifically for breast cancer detection from histopathological images. This classifier, in contrast with commonly used classifiers for image classification, is able to provide additional information about its classification results in human-friendly form. In this paper, we proposed a means for presenting the additional semantical informations that is potentionally useful for decision-making in medical domain. First, the classifier provides semantic explanation, regarding the possibility of misclassification of the test sample. Alongside with semantics, it provides visualization of similar and non-similar samples of different class. The classification performance of CFCMC is compared against three commonly used classifiers for image classification, Convolutional Neural Network (CNN), Stacked Auto-encoder (SAE) and Deep Multi-layered Perceptron. The experimental result shows that the CFCMC is not necessarily the best classifier. However, the ability to provide semantic and visual explanation of classification result allows the classifier to be applied as a supporting tool for pathologists in diagnostic of breast cancer.",2161-4407,978-1-7281-1985-4,10.1109/IJCNN.2019.8852070,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852070,cancer detection;explainable classifier;explainable AI;image classification,Semantics;Breast cancer;Training;Medical diagnostic imaging;Mathematical model;Clustering algorithms,cancer;convolutional neural nets;fuzzy set theory;image classification;learning (artificial intelligence);medical image processing;multilayer perceptrons;patient diagnosis,medical domain;breast cancer detection;histopathological images;image classification;semantic explanation;CFCMC;breast cancer diagnosis;Cumulative Fuzzy Class Membership Criterion;decision-making;explainable classifier;convolutional neural network;CNN;stacked autoencoder;deep multilayered perceptron,,,19.0,,,,,IEEE,IEEE Conferences
124,Requirements Classification with Interpretable Machine Learning and Dependency Parsing,F. Dalpiaz; D. Dell'Anna; F. B. Aydemir; S. Çevikol,Utrecht University; Utrecht University; Bogaziçi University; Bogaziçi University,2019 IEEE 27th International Requirements Engineering Conference (RE),,2019,,,142,152,"Requirements classification is a traditional application of machine learning (ML) to RE that helps handle large requirements datasets. A prime example of an RE classification problem is the distinction between functional and non-functional (quality) requirements. State-of-the-art classifiers build their effectiveness on a large set of word features like text n-grams or POS n-grams, which do not fully capture the essence of a requirement. As a result, it is arduous for human analysts to interpret the classification results by exploring the classifier's inner workings. We propose the use of more general linguistic features, such as dependency types, for the construction of interpretable ML classifiers for RE. Through a feature engineering effort, in which we are assisted by modern introspection tools that reveal the hidden inner workings of ML classifiers, we derive a set of 17 linguistic features. While classifiers that use our proposed features fit the training set slightly worse than those that use high-dimensional feature sets, our approach performs generally better on validation datasets and it is more interpretable.",2332-6441,978-1-7281-3912-8,10.1109/RE.2019.00025,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920401,requirements engineering;interpretable machine learning;requirements classification;explainable AI,Training;Measurement;Tagging;Linguistics;Libraries;Machine learning;Tools,computational linguistics;feature extraction;grammars;learning (artificial intelligence);pattern classification;text analysis,requirements classification;interpretable machine learning;dependency parsing;requirements datasets;prime example;classification problem;word features;text n-grams;POS n-grams;classifier;general linguistic features;dependency types;interpretable ML classifiers;feature engineering effort;hidden inner workings;linguistic features;high-dimensional feature sets,,,44.0,,,,,IEEE,IEEE Conferences
125,Curating Explanations of Machine Learning Models for Business Stakeholders,I. Golbin; K. K. Lim; D. Galla,PwC; PwC; PwC,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),,2019,,,44,49,"Consumer-oriented products and services are increasingly incorporating artificial intelligence (AI) functionality or being influenced by machine learning in development; for example, AI is replacing rule-based systems in identifying anomalous behavior or selecting services for customers. With AI's increasing uses and impact on consumers, explainability becomes required by consumers, business stakeholders, and regulator. Many of relevant researchers have been geared toward End Users and Technical Audiences, especially in the space of feature importance and local explainability. However, Business Sponsors and Regulators have different requirements which can be further explored. We present a framework of potential requirements and concerns encompassing these stakeholders, including End Users, Technical Audiences, Business Sponsors and Regulators. Finally, we apply the framework to an example in the financial services space.",,978-1-7281-4087-2,10.1109/AI4I46381.2019.00019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9027775,Explainable AI;Local Explainability;Global Explainability;Stakeholder oriented;Financial Services;Model-agnostic Explanation,Predictive models;Stakeholders;Machine learning;Regulators;Decision making,business data processing;knowledge based systems;learning (artificial intelligence),end users;business sponsors and regulators;technical audiences;consumer-oriented services;consumer-oriented products;rule-based systems;machine learning;artificial intelligence functionality;business stakeholders;financial services space,,,20.0,,,,,IEEE,IEEE Conferences
126,Cell Fault Management Using Machine Learning Techniques,D. Mulvey; C. H. Foh; M. A. Imran; R. Tafazolli,"5G Innovation Center, University of Surrey, Guildford, U.K.; 5G Innovation Center, University of Surrey, Guildford, U.K.; School of Engineering, University of Glasgow, Glasgow, U.K.; 5G Innovation Center, University of Surrey, Guildford, U.K.",IEEE Access,,2019,7,,124514,124539,"This paper surveys the literature relating to the application of machine learning to fault management in cellular networks from an operational perspective. We summarise the main issues as 5G networks evolve, and their implications for fault management. We describe the relevant machine learning techniques through to deep learning, and survey the progress which has been made in their application, based on the building blocks of a typical fault management system. We review recent work to develop the abilities of deep learning systems to explain and justify their recommendations to network operators. We discuss forthcoming changes in network architecture which are likely to impact fault management and offer a vision of how fault management systems can exploit deep learning in the future. We identify a series of research topics for further study in order to achieve this.",2169-3536,,10.1109/ACCESS.2019.2938410,Engineering and Physical Sciences Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8819935,Cellular networks;self healing;cell outage;cell degradation;fault diagnosis;deep learning;explainable AI,Computer architecture;Microprocessors;Deep learning;Neural networks;Fault diagnosis;Cellular networks,cellular radio;fault diagnosis;learning (artificial intelligence);neural nets,cell fault management;machine learning techniques;cellular networks;deep learning systems;network operators;network architecture;5G networks,,,136.0,CCBY,,,,IEEE,IEEE Journals
127,Framework for Data Driven Health Monitoring of Cyber-Physical Systems,K. Amarasinghe; C. Wickramasinghe; D. Marino; C. Rieger; M. Manicl,"Virginia Commonwealth University, Richmond, Virginia, USA; Virginia Commonwealth University, Richmond, Virginia, USA; Virginia Commonwealth University, Richmond, Virginia, USA; Idaho National Laboratory, Idaho Falls, Idaho, USA; Virginia Commonwealth University, Richmond, Virginia, USA",2018 Resilience Week (RWS),,2018,,,25,30,"Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features.",,978-1-5386-6913-6,10.1109/RWEEK.2018.8473535,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8473535,Cyber-Physical Systems;Resilience;Unsupervised learning;Health Monitoring;Explainable AI;Anomaly Detection,Feature extraction;Monitoring;Real-time systems;Self-organizing feature maps;Neurons;Data mining;Degradation,building management systems;condition monitoring;cyber-physical systems;data acquisition;distributed power generation;feature extraction;power engineering computing;state estimation;unsupervised learning,data driven health monitoring;interconnected computational resources;physical resources;continuous monitoring;CPS operational health;data driven techniques;data driven methodology;heterogeneous data streams;time state estimation;CPS testbed;cyber-network;feature extraction method;data features;cyber-physical systems;data acquisition;state identification;visualization technique,,5.0,17.0,,,,,IEEE,IEEE Conferences
129,What Lies Beneath: A Note on the Explainability of Black-box Machine Learning Models for Road Traffic Forecasting,A. Barredo-Arrieta; I. Laña; J. Del Ser,"TECNALIA,Derio,Bizkaia,Spain,48160; TECNALIA,Derio,Bizkaia,Spain,48160; TECNALIA,Derio,Bizkaia,Spain,48160",2019 IEEE Intelligent Transportation Systems Conference (ITSC),,2019,,,2232,2237,"Traffic flow forecasting is widely regarded as an essential gear in the complex machinery underneath Intelligent Transport Systems, being a critical component of avant-garde Automated Traffic Management Systems. Research in this area has stimulated a vibrant activity, yielding a plethora of new forecasting methods contributed to the community on a yearly basis. Efforts in this domain are mainly oriented to the development of prediction models featuring with ever-growing levels of performances and/or computational efficiency. After the swerve towards Artificial Intelligence that gradually took place in the modeling sphere of traffic forecasting, predictive schemes have ever since adopted all the benefits of applied machine learning, but have also incurred some caveats. The adoption of highly complex, black-box models has subtracted comprehensibility to forecasts: even though they perform better, they are more obscure to ITS practitioners, which hinders their practicality. In this paper we propose the adoption of explainable Artificial Intelligence (xAI) tools that are currently being used in other domains, in order to extract further knowledge from black-box traffic forecasting models. In particular we showcase the utility of xAI to unveil the knowledge extracted by Random Forests and Recurrent Neural Networks when predicting real traffic. The obtained results are insightful and suggest that the traffic forecasting model should be analyzed from more points of view beyond that of prediction accuracy or any other regression score alike, due to the treatment each algorithm gives to input variables: even with the same nominal score value, some methods can take advantage of inner knowledge that others instead disregard.",,978-1-5386-7024-8,10.1109/ITSC.2019.8916985,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8916985,,Predictive models;Forecasting;Biological system modeling;Autoregressive processes;Computational modeling;Tools;Data models,learning (artificial intelligence);recurrent neural nets;regression analysis;road traffic;traffic engineering computing,intelligent transport systems;critical component;traffic management systems;vibrant activity;prediction models;modeling sphere;predictive schemes;machine learning;artificial intelligence tools;black-box traffic forecasting models;prediction accuracy;traffic flow forecasting,,,34.0,,,,,IEEE,IEEE Conferences
130,Explainable Software Analytics,H. K. Dam; T. Tran; A. Ghose,"Univ. of Wollongong, Wollongong, VIC, Australia; Deakin Univ., Deakin, VIC, Australia; Univ. of Wollongong, Wollongong, VIC, Australia",2018 IEEE/ACM 40th International Conference on Software Engineering: New Ideas and Emerging Technologies Results (ICSE-NIER),,2018,,,53,56,"Software analytics has been the subject of considerable recent attention but is yet to receive significant industry traction. One of the key reasons is that software practitioners are reluctant to trust predictions produced by the analytics machinery without understanding the rationale for those predictions. While complex models such as deep learning and ensemble methods improve predictive performance, they have limited explainability. In this paper, we argue that making software analytics models explainable to software practitioners is as important as achieving accurate predictions. Explainability should therefore be a key measure for evaluating software analytics models. We envision that explainability will be a key driver for developing software analytics models that are useful in practice. We outline a research roadmap for this space, building on social science, explainable artificial intelligence and software engineering.",,978-1-4503-5662-6,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444837,Software engineering;software analytics;Mining software repositories,Software;Analytical models;Predictive models;Software engineering;Machine learning;Neural networks,artificial intelligence;data analysis;software engineering,software analytics models;explainability;software engineering;explainable software analytics;software practitioners;research roadmap;social science;artificial intelligence,,,28.0,,,,,IEEE,IEEE Conferences
131,Adversarial ML Attack on Self Organizing Cellular Networks,S. Farooq; M. Usama; J. Qadir; M. A. Imran,"Information Technology University, Lahore, Punjab, Pakistan; Information Technology University, Lahore, Punjab, Pakistan; Information Technology University, Lahore, Punjab, Pakistan; University of Glasgow, Scotland, UK",2019 UK/ China Emerging Technologies (UCET),,2019,,,1,5,"Deep Neural Networks (DNN) have been widely adopted in self-organizing networks (SON) for automating different networking tasks. Recently, it has been shown that DNN lack robustness against adversarial examples where an adversary can fool the DNN model into incorrect classification by introducing a small imperceptible perturbation to the original example. SON is expected to use DNN for multiple fundamental cellular tasks and many DNN-based solutions for performing SON tasks have been proposed in the literature have not been tested against adversarial examples. In this paper, we have tested and explained the robustness of SON against adversarial example and investigated the performance of an important SON use case in the face of adversarial attacks. We have also generated explanations of incorrect classifications by utilizing an explainable artificial intelligence (AI) technique.",,978-1-7281-2797-2,10.1109/UCET.2019.8881842,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8881842,Adversarial machine learning;Self Organizing Cellular Networks,Optimization;Task analysis;Artificial intelligence;Key performance indicator;Cellular networks;Perturbation methods;Long Term Evolution,artificial intelligence;cellular radio;mobile computing;neural nets;pattern classification;security of data,adversarial attacks;adversarial ML attack;deep neural networks;self-organizing networks;DNN model;SON tasks;self organizing cellular networks;artificial intelligence technique,,,21.0,,,,,IEEE,IEEE Conferences
132,Artificial Intelligence for Public Health,K. P. Bennett,"Dept. of Math. Sci., Rensselaer Polytech. Inst., Troy, NY, USA",2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),,2019,,,1,2,"In this talk, we examine artificial intelligence approaches for extracting actionable insights from health care data in order to improve public health. Our goal is to simultaneously identify subpopulations with distinct health risks and health trajectories and find the distinct risk factors or determinants associated each subpopulation. These determinants can then be used treatments, programs, and policies in order to reduce mortality and comorbidity and provide more efficient healthcare. We examine novel cadre machine learning approaches that combine predictive neural network modeling with more traditional statistical epidemiology methods for risk and survival analyses. We embed the cadre methods into a Semantically Targeted Analytics (Semantalytics) System that combines semantics, inference, automatic machine learning, and explainable AI. The AI system translate the public health questions to an analysis plan, prepares data, conducts analysis and reports results with visualization and text. We demonstrated these approach on public health care surveillance datasets and electronic medical records. The award winning “MortalityMinder” app examines the social determinants of “Deaths Despair” (deaths from suicide and substance abuse) and other causes of mortality that are unexpectedly rising in the United States. Other applications include association of environment toxins associated with diseases, high needs patient management for a health management organization, and emergency department readmissions. We conclude with the discussion of the open challenges to create population health AI systems that can transform health care questions into data-driven actionable-insights on the fly.",,978-1-7281-1867-3,10.1109/BIBM47256.2019.8983112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8983112,,,diseases;health care;learning (artificial intelligence);medical information systems;neural nets,artificial intelligence approaches;health care data;subpopulation;health risks;health trajectories;risk factors;mortality;comorbidity;cadre machine;predictive neural network;survival analyses;cadre methods;automatic machine learning;public health questions;public health care surveillance datasets;social determinants;health management organization;health care questions;data-driven actionable-insights;health AI systems;semantically targeted analytics system,,,0.0,,,,,IEEE,IEEE Conferences
134,AI and IoT for Social Value Creation,Y. Mochizuki,"NEC Corporation,Tokyo,Japan",2019 IEEE Asian Solid-State Circuits Conference (A-SSCC),,2019,,,99,102,"The features of AI and IoT technologies leading to the realization of next smart cities/smart communities are discussed. The AI topics included are person identification AI for biometrics, white-box/explainable AI for value chain innovation, whose use cases reveal how these technologies are contributing to the broader range of new social values. IoT system also has significant implications in realizing the next smart society. Here, interoperability and openness are becoming increasingly important for scalability and cross-domain data utilization of solutions, in quest to the realization of citizen-centric and economically sustainable, thereby future-proof smart cities.",,978-1-7281-5106-9,10.1109/A-SSCC47793.2019.9056955,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9056955,AI;IoT;smart city;data exchange platform,Smart cities;Artificial intelligence;Business;Fingerprint recognition;Technological innovation,artificial intelligence;Internet of Things;smart cities,cross-domain data utilization;smart communities;future-proof smart cities;smart society;IoT system;value chain innovation;person identification AI;social value creation,,,11.0,,,,,IEEE,IEEE Conferences
136,Propagated Perturbation of Adversarial Attack for well-known CNNs: Empirical Study and its Explanation,J. Yoon; K. Kim; J. Jang,LG CNS; LG CNS; LG science park,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,2019,,,4226,4234,"Deep Neural Network based classifiers are known to be vulnerable to perturbations of inputs constructed by an adversarial attack to force misclassification. Most studies have focused on how to make vulnerable noise by gradient based attack methods or to defense model from adversarial attack. The use of the denoiser model is one of a well-known solution to reduce the adversarial noise although classification performance had not significantly improved. In this study, we aim to analyze the propagation of adversarial attack as an explainable AI(XAI) point of view. Specifically, we examine the trend of adversarial perturbations through the CNN architectures. To analyze the propagated perturbation, we measured normalized Euclidean Distance and cosine distance in each CNN layer between the feature map of the perturbed image passed through denoiser and the non-perturbed original image. We used five well-known CNN based classifiers and three gradient-based adversarial attacks. From the experimental results, we observed that in most cases, Euclidean Distance explosively increases in the final fully connected layer while cosine distance fluctuated and disappeared at the last layer. This means that the use of denoiser can decrease the amount of noise. However, it failed to defense accuracy degradation.",2473-9944,978-1-7281-5023-9,10.1109/ICCVW.2019.00520,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022421,adversarial-attack,Training;Perturbation methods;Feature extraction;Noise reduction;Iterative methods;Computer architecture;Measurement uncertainty,convolutional neural nets;image classification;image denoising;learning (artificial intelligence),cosine distance;Euclidean distance;adversarial noise;gradient based attack methods;deep neural network based classifiers;adversarial attack;CNN based classifiers;adversarial perturbations,,,29.0,,,,,IEEE,IEEE Conferences
139,On the Convergence of Artificial Intelligence and Distributed Ledger Technology: A Scoping Review and Future Research Agenda,K. D. Pandl; S. Thiebes; M. Schmidt-Kraepelin; A. Sunyaev,"Institute of Applied Informatics and Formal Description Methods, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Applied Informatics and Formal Description Methods, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Applied Informatics and Formal Description Methods, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Applied Informatics and Formal Description Methods, Karlsruhe Institute of Technology, Karlsruhe, Germany",IEEE Access,,2020,8,,57075,57095,"Developments in artificial intelligence (AI) and distributed ledger technology (DLT) currently lead to lively debates in academia and practice. AI processes data to perform tasks that were previously thought possible only for humans. DLT has the potential to create consensus over data among a group of participants in untrustworthy environments. In recent research, both technologies are used in similar and even the same systems. This can lead to a convergence of AI and DLT, which in the past, has paved the way for major innovations of other information technologies. Previous work highlights several potential benefits of a convergence of AI and DLT but only provides a limited theoretical framework to describe upcoming real-world integration cases of both technologies. In this research, we review and synthesize extant research on integrating AI with DLT and vice versa to rigorously develop a future research agenda on the convergence of both technologies. In terms of integrating AI with DLT, we identified research opportunities in the areas of secure DLT, automated referee and governance, and privacy-preserving personalization. With regard to integrating DLT with AI, we identified future research opportunities in the areas of decentralized computing for AI, secure data sharing and marketplaces, explainable AI, and coordinating devices. In doing so, this research provides a four-fold contribution. First, it is not constrained to blockchain but instead investigates the broader phenomenon of DLT. Second, it considers the reciprocal nature of a convergence of AI and DLT. Third, it bridges the gap between theory and practice by helping researchers active in AI or DLT to overcome current limitations in their field, and practitioners to develop systems along with the convergence of both technologies. Fourth, it demonstrates the feasibility of applying the convergence concept to research on AI and DLT.",2169-3536,,10.1109/ACCESS.2020.2981447,Karlsruhe Institute of Technology through the KIT-Publication Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039606,Artificial intelligence;blockchain;convergence;distributed ledger technology;machine learning,,artificial intelligence;convergence of numerical methods;cryptocurrencies;data privacy;distributed databases;organisational aspects,secure DLT;artificial intelligence;distributed ledger technology;AI processes data;decentralized computing;secure data sharing;marketplaces;convergence,,,111.0,CCBY,,,,IEEE,IEEE Journals
143,Customized Interpretable Conformal Regressors,U. Johansson; C. Sönströd; T. Löfström; H. Boström,Jönköping University; University of Borås; Jönköping University; KTH Royal Institute of Technology,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),,2019,,,221,230,"Interpretability is recognized as a key property of trustworthy predictive models. Only interpretable models make it straightforward to explain individual predictions, and allow inspection and analysis of the model itself. In real-world scenarios, these explanations and insights are often needed for a specific batch of predictions, i.e., a production set. If the input vectors for this production set are available when generating the predictive model, a methodology called oracle coaching can be used to produce highly accurate and interpretable models optimized for the specific production set. In this paper, oracle coaching is, for the first time, combined with the conformal prediction framework for predictive regression. A conformal regressor, which is built on top of a standard regression model, outputs valid prediction intervals, i.e., the error rate on novel data is bounded by a preset significance level, as long as the labeled data used for calibration is exchangeable with production data. Since validity is guaranteed for all conformal predictors, the key performance metric is the size of the prediction intervals, where tighter (more efficient) intervals are preferred. The efficiency of a conformal model depends on several factors, but more accurate underlying models will generally also lead to improved efficiency in the corresponding conformal predictor. A key contribution in this paper is the design of setups ensuring that when oracle coached regression trees, that per definition utilize knowledge about production data, are used as underlying models for conformal regressors, these remain valid. The experiments, using 20 publicly available regression data sets, demonstrate the validity of the suggested setups. Results also show that utilizing oracle-coached underlying models will generally lead to significantly more efficient conformal regressors, compared to when these are built on top of models induced using only training data.",,978-1-7281-4493-1,10.1109/DSAA.2019.00037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964179,"Conformal prediction, Oracle coaching, Regression trees, Predictive modeling, Interpretability",,learning (artificial intelligence);regression analysis;trees (mathematics),oracle coached regression trees;conformal regressors;customized interpretable conformal regressors;trustworthy predictive models;interpretable models;predictive model;oracle coaching;conformal prediction framework;predictive regression;key performance metric,,,35.0,,,,,IEEE,IEEE Conferences
144,"Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances",S. Poria; N. Majumder; R. Mihalcea; E. Hovy,"ISTD, Singapore University of Technology and Design, Singapore; CIC, Instituto Politécnico Nacional, Mexico City, Mexico; Computer Science and Engineering, University of Michigan, Ann Arbor, MI, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA",IEEE Access,,2019,7,,100943,100953,"Emotion is intrinsic to humans and consequently, emotion understanding is a key part of human-like artificial intelligence (AI). Emotion recognition in conversation (ERC) is becoming increasingly popular as a new research frontier in natural language processing (NLP) due to its ability to mine opinions from the plethora of publicly available conversational data on platforms such as Facebook, Youtube, Reddit, Twitter, and others. Moreover, it has potential applications in health-care systems (as a tool for psychological analysis), education (understanding student frustration), and more. In Addition, ERC is also extremely important for generating emotion-aware dialogues that require an understanding of the user's emotions. Catering to these needs calls for effective and scalable conversational emotion-recognition algorithms. However, it is a difficult problem to solve because of several research challenges. In this paper, we discuss these challenges and shed light on recent research in this field. We also describe the drawbacks of these approaches and discuss the reasons why they fail to successfully overcome the research challenges in ERC.",2169-3536,,10.1109/ACCESS.2019.2929050,Michigan Institute for Data Science; National Science Foundation; John Templeton Foundation; Defense Advanced Research Projects Agency; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8764449,Emotion recognition;sentiment analysis;dialogue systems;natural language processing,Emotion recognition;Task analysis;Context modeling;Taxonomy;Natural language processing;Licenses;Pragmatics,data mining;emotion recognition;sentiment analysis,ERC;emotion-aware dialogues;conversational data;opinions mining;emotion-recognition algorithms;emotion recognition in conversation;natural language processing;artificial intelligence;emotion understanding,,3.0,46.0,CCBY,,,,IEEE,IEEE Journals
145,SizeNet: Weakly Supervised Learning of Visual Size and Fit in Fashion Images,N. Karessli; R. Guigourès; R. Shirvany,"Zalando SE, Germany; Zalando SE, Germany; Zalando SE, Germany",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,2019,,,335,343,"Finding clothes that fit is a hot topic in the e-commerce fashion industry. Most approaches addressing this problem are based on statistical methods relying on historical data of articles purchased and returned to the store. Such approaches suffer from the cold start problem for the thousands of articles appearing on the shopping platforms everyday, for which no prior purchase history is available. We propose to employ visual data to infer size and fit characteristics of fashion articles. We introduce SizeNet, a weakly supervised teacher-student training framework that leverages the power of statistical models combined with the rich visual information from article images to learn visual cues for size and fit characteristics, capable of tackling the challenging cold start problem. Detailed experiments are performed on thousands of textile garments, including dresses, trousers, knitwear, tops, etc. from hundreds of different brands.",2160-7516,978-1-7281-2506-0,10.1109/CVPRW.2019.00046,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025680,,Data models;Visualization;Clothing;Predictive models;Bayes methods;History;Training,clothing industry;electronic commerce;image processing;learning (artificial intelligence);purchasing;statistical analysis,textile garments;cold start problem;visual cues;article images;rich visual information;weakly supervised teacher-student training framework;fashion articles;visual data;prior purchase history;shopping platforms;statistical methods;e-commerce fashion industry;clothes;fashion images;visual size;weakly supervised learning;SizeNet,,,29.0,,,,,IEEE,IEEE Conferences
147,Dialogue-Based Supervision and Explanation of Robot Spatial Beliefs: a Software Architecture Perspective,L. Buoncompagni; F. Mastrogiovanni,"the Department of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via Opera Pia 13, Genoa, 16145, Italy; the Department of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Via Opera Pia 13, Genoa, 16145, Italy",2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),,2018,,,977,984,"The paper presents a software architecture allowing a robot to learn new compositions of objects in table-top scenarios by human demonstrations. The robot qualitatively represents those scenes, reason upon their similarity, and interact with humans through dialogues to talk about represented scenes. We formalise the robot behaviour based on a Description Logic representation of scenes through spatial beliefs, i.e., learned logic predicates, on which the robot applies symbolic reasoning to recognise and explain the scene. We exploit the logical structure of predicates in a software architecture that enables a robot exposing its beliefs, and if required, it allows a human supervisor to apply corrections in a form akin to robot active perception. The paper critically discusses the design of the software components and their interfaces, discriminating between knowledge representation and dialogue management. Those components are developed for human-robot knowledge sharing applications involving visual, verbal, and auditory modalities of interaction. Software components are treated as grey boxes managing an ontology-based formalisation of robot beliefs through four contextualised dialogues, for which we present a unique design pattern.",1944-9437,978-1-5386-7980-7,10.1109/ROMAN.2018.8525828,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8525828,,Cognition;Robot sensing systems;Ontologies;Tagging;Shape,description logic;inference mechanisms;interactive systems;learning (artificial intelligence);mobile robots;ontologies (artificial intelligence);software architecture,human supervisor;robot active perception;software components;knowledge representation;dialogue management;human-robot knowledge;contextualised dialogues;robot spatial beliefs;software architecture perspective;table-top scenarios;human demonstrations;robot qualitatively;robot behaviour;learned logic predicates;symbolic reasoning;dialogue-based supervision;dialogue-based explanation;description logic representation;ontology-based formalisation;scenes representation;visual modalities;verbal modalities;auditory modalities,,1.0,18.0,,,,,IEEE,IEEE Conferences
148,Personalizing the Prediction: Interactive and Interpretable machine learning,S. Koh; H. J. Wi; B. Hyung Kim; S. Jo,"School of Computing, KAIST, Republic of Korea; School of Computing, KAIST, Republic of Korea; School of Computing, KAIST, Republic of Korea; School of Computing, KAIST, Republic of Korea",2019 16th International Conference on Ubiquitous Robots (UR),,2019,,,354,359,"While many applications with machine learning provide enough utilities for users, they mostly target average of users. Although it might be acceptable in certain domains, there are domains such as health and medical-care where it is crucial to provide personalized service. In such cases, personalization of machine learning model usually does not depend on end users to make change to the system. As machine learning models are black-box, the only information that the users can acquire is input and output of certain decision made by the model. Thus, with no reason behind specific prediction provided by the system, users cannot understand how the system works and make amendments to the system. This shortcoming is directly related to users' credibility in the system. In this paper, we present an interface where the system provides users the reason behind the decision made by the machine learning model and users provide feedback to the model. Moreover, we present the principle behind the suggested interface and prototype that instantiates the suggested interface. Our interface's effectiveness is evaluated through users' surveys regarding two main attributes: (1) how well users understand the system and more importantly, (2) how it influences users to trust in the system.",2325-033X,978-1-7281-3232-7,10.1109/URAI.2019.8768705,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768705,,Motion pictures;Machine learning;Analytical models;Predictive models;Graphical user interfaces;Bars,interactive systems;learning (artificial intelligence),personalized service;interpretable machine learning;interactive machine learning;machine learning personalization;black-box,,,22.0,,,,,IEEE,IEEE Conferences
149,Improving Human-Robot Interaction Through Explainable Reinforcement Learning,A. Tabrez; B. Hayes,"University Of Colorado Boulder, Boulder, CO, 80309; University Of Colorado Boulder, Boulder, CO, 80309",2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,2019,,,751,753,"Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.",2167-2148,978-1-5386-8555-6,10.1109/HRI.2019.8673198,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673198,,Task analysis;Robots;Collaboration;Hidden Markov models;Artificial intelligence;Maintenance engineering;Planning,decision support systems;human-robot interaction;learning (artificial intelligence),human-robot interaction;explainable reinforcement learning;DSS;changing environments;modern systems;rationale versus;future behaviors;probabilistic assessments;failure recovery;danger avoidance mechanisms;learned failure modes;nonobvious pieces;course-of-action evaluation;collaborative scenarios;irrelevant information;AI models;specific decisions;communicating;explaining justifications;decision support systems;task;skill;annotated failure modes;COA evaluation;mental workload;AI driven behaviors,,,16.0,,,,,IEEE,IEEE Conferences
150,Exploring Transferability in Deep Neural Networks with Functional Data Analysis and Spatial Statistics,R. McAllister; J. Sheppard,"Gianforte School of Computing, Montana State University, Bozeman, MT, USA; Gianforte School of Computing, Montana State University, Bozeman, MT, USA",2019 International Joint Conference on Neural Networks (IJCNN),,2019,,,1,10,"Recent advances in machine learning have brought with them considerable attention in applying such methods to complex prediction problems. However, in extremely large dataspaces, a single neural network covering that space may not be effective, and generating large numbers of deep neural networks is not feasible. In this paper, we analyze deep networks trained from stacked autoencoders in a spatio-temporal application area to determine the extent to which knowledge can be transferred to similar regions. Our analysis applies methods from functional data analysis and spatial statistics to identify such correlation. We apply this work in the context of numerical weather prediction in analyzing large-scale data from Hurricane Sandy. Results of our analysis indicate high likelihood that spatial correlation can be exploited if it can be identified prior to training.",2161-4407,978-1-7281-1985-4,10.1109/IJCNN.2019.8851994,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851994,,Neural networks;Training;Data models;Data analysis;Computational modeling;Wind,data analysis;learning (artificial intelligence);neural nets;statistical analysis;weather forecasting,spatial statistics;spatial correlation;deep neural networks;functional data analysis;machine learning;complex prediction problems;single neural network;stacked autoencoders;numerical weather prediction;spatio-temporal application,,,21.0,,,,,IEEE,IEEE Conferences
152,Quality Evaluation Assurance Levels for Deep Neural Networks Software,S. Nakajima,"National Institute of Informatics,Tokyo,Japan",2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI),,2019,,,1,6,"Quality of machine learning software products or services is dependent on datasets used for training. However, defining quality of datasets is difficult, which might bring about risks in business situations. Independent, third-party testing laboratories would mitigate the risks. This paper proposes quality evaluation assurance levels, which is a basis of a third-party evaluation and certification framework. Moreover, quality of machine learning software is indeed viewed from three perspectives: prediction performance quality, training mechanism quality, and lifecycle support quality enabling continuous operations.",2376-6824,978-1-7281-4666-9,10.1109/TAAI48200.2019.8959916,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8959916,,,certification;convolutional neural nets;learning (artificial intelligence);security of data;software engineering;software quality,lifecycle support quality;quality evaluation assurance levels;deep neural networks software;machine learning software products;third-party testing laboratories;third-party evaluation;certification framework;prediction performance quality;training mechanism quality,,,25.0,,,,,IEEE,IEEE Conferences
154,Privacy-awareness of Users in our Cloudy Smart World,G. G. Varkonyi; A. Kertesz; S. Varadi,"International and Regional Studies Institute, University of Szeged, Hungary; Software Engineering Department, University of Szeged, Hungary; Department of International and European Law, University of Szeged, Hungary",2019 Fourth International Conference on Fog and Mobile Edge Computing (FMEC),,2019,,,189,196,"Recent advances in Information and Communications Technology (ICT) are expanding the world of Internet services with smart applications running on interconnected smart devices. Cloud computing already started to dominate the Internet, and with the appearance of Internet of Things (IoT), and Fog and Edge computing IoT-Fog-Cloud systems are formed. Beside such architectural developments, Artificial Intelligence (AI) methods receive new interests by managing Big Data coming from these complex, heterogeneous systems. In May 2018, the General Data Protection Regulation (GDPR) came into force in the European Union (EU) to strengthen users' influence on their personal data. Though the initiative of the regulation was known two years before, the 25th May, 2018 was a shock for many ICT actors. The privacy awareness of end-users of the Internet of Services is also wanting, and the emergence of new technologies in this field further complicates the identification of data protection responsibilities. In this paper summarize the GDPR novelties affecting the design and operation of IoT applications in fogs and clouds, we present the results of a recent survey on the GDPR awareness of international students on privacy issues related to the usage of IoT-Fog-Cloud systems and AI solutions. Based on the results of the survey, we also state our implications for safer utilization of these complex ICT systems in the future.",,978-1-7281-1796-6,10.1109/FMEC.2019.8795310,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795310,Data Protection;GDPR;Artificial Intelligence;Internet of Things;Fog computing;Cloud computing,,artificial intelligence;Big Data;cloud computing;data protection;Internet of Things;security of data,heterogeneous systems;personal data;ICT actors;data protection responsibilities;IoT applications;GDPR awareness;privacy issues;complex ICT systems;cloudy smart world;Internet services;interconnected smart devices;architectural developments;general data protection regulation;user privacy awareness;cloud computing;artificial intelligence methods;big data;information and communications technology;Internet of Things;edge computing;IoT-fog-cloud systems;European Union,,,22.0,,,,,IEEE,IEEE Conferences
156,Future Computing Hardware for AI,J. Welser; J. W. Pitera; C. Goldberg,"IBM Research, Albany, Almaden, Yorktown Heights, Zurich; IBM Research, Albany, Almaden, Yorktown Heights, Zurich; IBM Research, Albany, Almaden, Yorktown Heights, Zurich",2018 IEEE International Electron Devices Meeting (IEDM),,2018,,,1.3.1,1.3.6,"Hardware has taken on a supporting role in the maturation and proliferation of narrow AI, but will take a leading role to enable the innovation and adoption of broad AI. The concurrent evolution of broad AI with purpose-built hardware will shift traditional balances between cloud and edge, structured and unstructured data, and training and inference. Heterogeneous system architectures are already being delivered where varied compute resources, including high-bandwidth CPUs, specialized AI accelerators, and high-performance networking are infused in each node to yield significant performance improvements. Looking to the future, we envision a roadmap of specialized technologies to accelerate AI, starting with heterogeneous digital von Neumann machines, exploring reduced-precision accelerator approaches, finding the limits of conventional device power-performance with analog AI devices, and finishing with quantum computing for AI.",2156-017X,978-1-7281-1987-8,10.1109/IEDM.2018.8614482,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614482,,Graphics processing units;Training;Deep learning;Memory management;Hardware,inference mechanisms,narrow AI;innovation;broad AI;concurrent evolution;purpose-built hardware;traditional balances;unstructured data;heterogeneous system architectures;specialized AI accelerators;high-performance networking;heterogeneous digital von Neumann machines;reduced-precision accelerator approaches;computing hardware;high-bandwidth CPU;quantum computing;analog AI devices,,,27.0,,,,,IEEE,IEEE Conferences
157,IP Session on Machine Learning Applications in IC Test-Related Tasks,G. Sokar; Y. Zakaria; A. Rabie; K. Madkour; I. Leventhal; J. Rivoir; X. Gu; H. Stratigopoulos,"Cairo University, Egypt; Cairo University, Egypt; A Siemens Business, USA; A Siemens Business, USA; Advantest, USA; Advantest, Germany; Futurewei Technologies, Inc., Huawei, USA; CNRS, Sorbonne Université, LIP6, France (Organizer)",2019 IEEE 37th VLSI Test Symposium (VTS),,2019,,,1,1,"Over the last decade there has been a surge of activity in employing advanced statistical analysis and machine learning methods to various test-related tasks. The topic is no longer simply a matter of academic curiosity but, rather, a pressing need of the industry as it seeks to address various challenges. In this session, three industry experts have been invited to give their perspective, describe machine learning use cases, and discuss challenges and future work ideas. The three talks will cover the use of deep learning for hotspot detection, the challenge of rendering machine learning-based decisions in the semiconductor industry trustable and explainable, and data analytics across the the complete product cycle towards improved product reliability.",2375-1053,978-1-7281-1170-4,10.1109/VTS.2019.8758634,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758634,,Blockchain;Broadcasting;Real-time systems;Bitcoin;Peer-to-peer computing;Internet;TCPIP,electronic engineering computing;integrated circuit design;integrated circuit testing;learning (artificial intelligence);neural nets,machine learning applications;IC test-related tasks;deep learning;hotspot detection;semiconductor industry;data analytics,,,0.0,,,,,IEEE,IEEE Conferences
159,Encoding of a Chaotic Attractor in a Reservoir Computer: A Directional Fiber Investigation,S. Krishnagopal; G. Katz; M. Girvan; J. Reggia,"Dept. of Physics, University of Maryland, College Park, MD, USA; Dept. of Elec. Engr. and Comp. Sci, Syracuse University, Syracuse, NY, USA; Dept. of Physics, University of Maryland, College Park, MD, USA; Dept. of Computer Science, University of Maryland, College Park, MD, USA",2019 International Joint Conference on Neural Networks (IJCNN),,2019,,,1,8,"In this work, we study the dynamical properties of a machine learning technique called reservoir computing in order to gain insight into how representations of chaotic signals are encoded through learning. We train the reservoir on individual chaotic Lorenz signals. The Lorenz system is characterized by a set of equations and known to have three fixed points, all of which are unstable in the chaotic regime of the strange attractor. Exploration of the fixed points of the reservoir whose outputs are trained allows us to understand whether inherent Lorenz dynamics are transposed onto reservoir dynamics during learning. We do so by using a novel fixed point finding technique called directional fibers. Directional fibers are mathematical objects that systematically locate fixed points in a high dimensional space, and are found to be competitive and complementary with other traditional approaches. We find that the reservoir, after training of output weights, contains a higher dimensional projection of the Lorenz fixed points with matching stability, even though the training data did not include the fixed points. This tells us that the reservoir does indeed learn dynamical properties of the Lorenz attractor. We also find that the directional fiber also identifies additional fixed points in the reservoir space outside the projected Lorenz attractor region; these amplify perturbations during prediction and play a role in failure of long-term time series prediction.",2161-4407,978-1-7281-1985-4,10.1109/IJCNN.2019.8851853,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851853,reservoir computing;chaos prediction;echo state network;neural network;directional fiber;Lorenz attractor;learning,Reservoirs;Training;Mathematical model;Machine learning;Computer architecture;Neural networks,chaos;learning (artificial intelligence);nonlinear dynamical systems;time series,chaotic attractor;directional fiber investigation;dynamical properties;Lorenz system;chaotic regime;inherent Lorenz dynamics;reservoir dynamics;Lorenz attractor;reservoir space;machine learning technique;fixed points;fixed point finding technique;chaotic Lorenz signals;reservoir computing,,,26.0,,,,,IEEE,IEEE Conferences
160,Building Activity Profiling: Explainable and Predictive Modeling for Building Automation,T. Kasuya; T. Takeshi; H. Esaki,"The Graduate School of Information Science and Technology, The University of Tokyo / Takenaka Corporation,Tokyo,Japan; Takenaka Corporation,Advanced Technology Research Dept.,Tokyo,Japan; The Graduate School of Information Science and Technology, The University of Tokyo,Tokyo,Japan",2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),,2020,,,242,247,"We propose a technique to model the activities in a building, which we call Building Activity Profiling (BAP). BAP is modeled using a hybrid system composed of continuous time-series data and discrete series of states. The time-series data are provided by various sensors of a Building Automation System, and the states are derived from the characteristics of the time-series data. Unlike a simple list of data, BAP enables us to provide a better understanding of the activities in a building and a precise prediction of what will happen next. The experimental results with real office buildings show that BAP successfully perceives seasonal periodicity and state transitions of activities such as on weekdays and weekends, and predicts electrical loads more accurately than a naive method of the time-series analysis.",,978-1-7281-4985-1,10.1109/ICAIIC48513.2020.9065268,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065268,Building Automation System;Hybrid System;Artificial Intelligence,Sensors;Predictive models;Load modeling;Building automation;Cloud computing,building management systems;home automation;time series,BAP;hybrid system;continuous time-series data;discrete series;building automation system;office buildings;seasonal periodicity;state transitions;time-series analysis;explainable modeling;predictive modeling;building activity profiling,,,13.0,,,,,IEEE,IEEE Conferences
161,Explainability of a Machine Learning Granting Scoring Model in Peer-to-Peer Lending,M. J. Ariza-Garzón; J. Arroyo; A. Caparrini; M. Segovia-Vargas,"Departamento de Ingeniería del Software e Inteligencia Artificial, Universidad Complutense de Madrid, Madrid, Spain; Departamento de Ingeniería del Software e Inteligencia Artificial, Universidad Complutense de Madrid, Madrid, Spain; Management Solutions, Madrid, Spain; Departamento de Economía Financiera y Actuarial y Estadística, Universidad Complutense de Madrid, Madrid, Spain",IEEE Access,,2020,8,,64873,64890,"Peer-to-peer (P2P) lending demands effective and explainable credit risk models. Typical machine learning algorithms offer high prediction performance, but most of them lack explanatory power. However, this deficiency can be solved with the help of the explainability tools proposed in the last few years, such as the SHAP values. In this work, we assess the well-known logistic regression model and several machine learning algorithms for granting scoring in P2P lending. The comparison reveals that the machine learning alternative is superior in terms of not only classification performance but also explainability. More precisely, the SHAP values reveal that machine learning algorithms can reflect dispersion, nonlinearity and structural breaks in the relationships between each feature and the target variable. Our results demonstrate that is possible to have machine learning credit scoring models be both accurate and transparent. Such models provide the trust that the industry, regulators and end-users demand in P2P lending and may lead to a wider adoption of machine learning in this and other risk assessment applications where explainability is required.",2169-3536,,10.1109/ACCESS.2020.2984412,"European Union’s H2020 Coordination and Support Actions; Santander-UCM Research Project, Call 2019; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050779,Credit risk;P2P lending;explainability;Shapley values;boosting;logistic regression,Machine learning;Logistics;Decision trees;Peer-to-peer computing;Machine learning algorithms;Analytical models;Neural networks,financial data processing;financial management;learning (artificial intelligence);peer-to-peer computing;regression analysis;risk management,peer-to-peer lending;explainable credit risk models;typical machine learning algorithms;high prediction performance;explainability tools;SHAP values;logistic regression model;machine learning alternative;credit scoring models;machine learning granting scoring model,,,75.0,CCBY,,,,IEEE,IEEE Journals
162,Next Generation Trustworthy Fraud Detection,S. Xie; P. S. Yu,"Lehigh Univ., Bethlehem, PA, USA; Univ. of Illinois at Chicago, Chicago, IL, USA",2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC),,2018,,,279,282,"Popular web applications, such as e-commerce, social networks and online ad auction, are providing valuable services to web users but have also been plagued by prevalent and diverse frauds. Many detection methodologies have been devised but detection trustworthiness is still one important and yet missing desideratum: a user will not trust a detector that has uncertain accuracy, can malfunction under unexpected situations, or can't explain its behaviors and interal working. Previous efforts mostly focused on detection accuracy, and our goal is to chart a path towards a more comprehensive definition of trustworthy detection, that consists of accuracy, transparency, and proactivity. To achieve the goal, we identify key challenges rooting at the specific settings of the above applications: the evolving nature and unexpectedness of the fraudsters' strategies, the ever-growing large amount of data, and the increasing complexity of effective detectors. We hope spark a large volume of research questions and solutions with respect to the above challenges.",,978-1-5386-9502-9,10.1109/CIC.2018.00045,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8537843,fraud detection;adversarial machine learning,Detectors;Sensitivity analysis;Data models;Security;Social network services;Machine learning;Support vector machines,fraud;Internet;trusted computing,e-commerce;social networks;online ad auction;prevalent frauds;trustworthy detection;web applications;next generation trustworthy fraud detection,,,50.0,,,,,IEEE,IEEE Conferences
163,Towards Explainable Face Aging with Generative Adversarial Networks,A. Genovese; V. Piuri; F. Scotti,"Department of Computer Science, Università degli Studi di Milano, Italy; Department of Computer Science, Università degli Studi di Milano, Italy; Department of Computer Science, Università degli Studi di Milano, Italy",2019 IEEE International Conference on Image Processing (ICIP),,2019,,,3806,3810,"Generative Adversarial Networks (GAN) are being increasingly used to perform face aging due to their capabilities of automatically generating highly-realistic synthetic images by using an adversarial model often based on Convolutional Neural Networks (CNN). However, GANs currently represent black box models since it is not known how the CNNs store and process the information learned from data. In this paper, we propose the first method that deals with explaining GANs, by introducing a novel qualitative and quantitative analysis of the inner structure of the model. Similarly to analyzing the common genes in two DNA sequences, we analyze the common filters in two CNNs. We show that the GANs for face aging partially share their parameters with GANs trained for heterogeneous applications and that the aging transformation can be learned using general purpose image databases and a fine-tuning step. Results on public databases confirm the validity of our approach, also enabling future studies on similar models.",2381-8549,978-1-5386-6249-6,10.1109/ICIP.2019.8803616,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803616,GAN;Face aging;CNN;Deep Learning,Face;Aging;Gallium nitride;Generators;Generative adversarial networks;Databases;Training,convolutional neural nets;DNA;face recognition;image filtering;learning (artificial intelligence),black box models;GAN;aging transformation;generative adversarial networks;synthetic images;adversarial model;convolutional neural networks;face aging;DNA sequences;image database,,1.0,30.0,,,,,IEEE,IEEE Conferences
164,Engineering for Emergence in Information Fusion Systems: A Review of Some Challenges,A. K. Raz; J. Llinas; R. Mittu; W. Lawless,"School of Aeronautics and Astronautics, Purdue University,West Lafayette,IN,United States; Center for Multisource, Information Fusion University at Buffalo,Bufallo,NY,United States; United States Naval Research Laboratory,Information Technology Division,Washington, DC,United States; School of Arts and Sciences, Paine College,Agusta,GA,United States",2019 22th International Conference on Information Fusion (FUSION),,2019,,,1,8,"Modern Information Fusion (IF) systems are faced with evolving operational environments where human and intelligent systems will function as a team to achieve mission objectives. These evolving operational contexts demand a full spectrum dynamic response of `data-to-decision' from IF systems. Traditional information extraction and fusion levels typically address the “data” end of the spectrum, while recent advancement in Machine Learning (ML) and Artificial Intelligence (AI) approaches are being used for the “decisions” end of the spectrum. However, the IF system behavior emerges from the various complex interactions that take place between different fusion levels (including human interaction), the operational context, and the employed AI/ML techniques. In this paper, we explore this emergent behavior of the IF system and argue that holistic system design and evaluation techniques, as offered by System Engineering (SE), provide means to recognize and characterize this emergent behavior. Furthermore, we describe the research challenges for future IF systems that will enable managing emergence by leveraging SE, while exploiting the context-aware information fusion aided by the advancements in AI/ML.",,978-0-9964527-8-6,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9011211,Data to Decision;Systems Engineering;Machine Learning;Artificial Intelligence;Data Fusion;Context-Aware Fusion,Complex systems;Machine learning;Complexity theory;Training;System analysis and design,feature extraction;learning (artificial intelligence);sensor fusion;ubiquitous computing,system engineering;intelligent systems;context-aware information fusion systems;information extraction;machine learning;artificial intelligence,,,46.0,,,,,IEEE,IEEE Conferences
165,AI and Blockchain: A Disruptive Integration,T. N. Dinh; M. T. Thai,Virginia Commonwealth University; University of Florida,Computer,,2018,51,9.0,48,53,"AI and blockchain are among the most disruptive technologies and will fundamentally reshape how we live, work, and interact. The authors summarize existing efforts and discuss the promising future of their integration, seeking to answer the question: What can smart, decentralized, and secure systems do for our society?",1558-0814,,10.1109/MC.2018.3620971,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481263,Future of AI;artificial intelligence;blockchain;disruptive technology;human-computer interaction;HCI;intelligent systems;security,Artificial intelligence;Blockchain;Intelligent systems;Computer security;Human computer interaction;Disruptive technologies,artificial intelligence,disruptive integration;disruptive technologies;AI,,15.0,17.0,,,,,IEEE,IEEE Magazines
166,Detecting Depression Severity by Interpretable Representations of Motion Dynamics,A. Kacem; Z. Hammal; M. Daoudi; J. Cohn,"IMT Lille Douai, Univ. Lille, CNRS, UMR 9189 CRIStAL, F-59000 Lille, France; Robot. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA; IMT Lille Douai, Univ. Lille, Lille, France; Robot. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA",2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018),,2018,,,739,745,"Recent breakthroughs in deep learning using automated measurement of face and head motion have made possible the first objective measurement of depression severity. While powerful, deep learning approaches lack interpretability. We developed an interpretable method of automatically measuring depression severity that uses barycentric coordinates of facial landmarks and a Lie-algebra based rotation matrix of 3D head motion. Using these representations, kinematic features are extracted, preprocessed, and encoded using Gaussian Mixture Models (GMM) and Fisher vector encoding. A multi-class SVM is used to classify the encoded facial and head movement dynamics into three levels of depression severity. The proposed approach was evaluated in adults with history of chronic depression. The method approached the classification accuracy of state-of-the-art deep learning while enabling clinically and theoretically relevant findings. The velocity and acceleration of facial movement strongly mapped onto depression severity symptoms consistent with clinical data and theory.",,978-1-5386-2335-0,10.1109/FG.2018.00116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8373909,Depression Severity Assessment;face and head dynamics;Barycentric coordinates;Lie algebra,Magnetic heads;Shape;Machine learning;Kinematics;Encoding;Face;Dynamics,face recognition;feature extraction;Gaussian processes;image classification;image motion analysis;image representation;learning (artificial intelligence);Lie algebras;matrix algebra;mixture models;psychology;support vector machines;vectors,motion dynamics;automated measurement;objective measurement;deep learning approaches;Lie-algebra based rotation matrix;3D head motion;head movement dynamics;chronic depression;depression severity symptoms;encoded facial movement dynamics,,2.0,32.0,,,,,IEEE,IEEE Conferences
167,Model-Based and Data-Driven Strategies in Medical Image Computing,D. Rueckert; J. A. Schnabel,"Department of Computing, Imperial College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, U.K.",Proceedings of the IEEE,,2020,108,1.0,110,124,"Model-based approaches for image reconstruction, analysis, and interpretation have made significant progress over the past decades. Many of these approaches are based on either mathematical, physical, or biological models. A challenge for these approaches is the modeling of the underlying processes (e.g., the physics of image acquisition or the patho-physiology of a disease) with appropriate levels of detail and realism. With the availability of large amounts of imaging data and machine learning (in particular deep learning) techniques, data-driven approaches have become more widespread for use in different tasks in reconstruction, analysis, and interpretation. These approaches learn statistical models directly from labeled or unlabeled image data and have been shown to be very powerful for extracting clinically useful information from medical imaging. While these data-driven approaches often outperform traditional model-based approaches, their clinical deployment often poses challenges in terms of robustness, generalization ability, and interpretability. In this article, we discuss what developments have motivated the shift from model-based approaches toward data-driven strategies and what potential problems are associated with the move toward purely data-driven approaches, in particular deep learning. We also discuss some of the open challenges for data-driven approaches, e.g., generalization to new unseen data (e.g., transfer learning), robustness to adversarial attacks, and interpretability. Finally, we conclude with a discussion on how these approaches may lead to the development of more closely coupled imaging pipelines that are optimized in an end-to-end fashion.",1558-2256,,10.1109/JPROC.2019.2943836,Engineering and Physical Sciences Research Council; Wellcome Trust; Wellcome Trust/EPSRC IEH Award; Innovate UK; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867900,Artificial neural networks;biomedical imaging;image analysis;image classification;image processing;image reconstruction;image registration;image segmentation;machine learning,Biomedical imaging;Biological system modeling;Computational modeling;Mathematical model;Data models;Image reconstruction,diseases;image reconstruction;learning (artificial intelligence);medical image processing;medical information systems;statistical analysis,data-driven strategies;medical image computing;image reconstruction;mathematical models;physical models;biological models;image acquisition;imaging data;machine learning;deep learning;statistical models;labeled image data;unlabeled image data;medical imaging;model-based approaches;closely coupled imaging pipelines;adversarial attacks,,,123.0,IEEE,,,,IEEE,IEEE Journals
169,iWEP: An Intelligent WLAN Early Warning Platform Using Edge Computing,R. Liu; W. Wang; J. Wang; Z. Ou; H. Qiu; B. Wang; Q. Liu,"College of Computer, National University of Defense Technology, China; College of Computer, National University of Defense Technology, China; College of Computer, National University of Defense Technology, China; College of Computer, National University of Defense Technology, China; College of Computer, National University of Defense Technology, China; College of Computer, National University of Defense Technology, China; College of Computer, National University of Defense Technology, China",2019 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN),,2019,,,384,389,"In the last decades, Wireless Local Area Network (WLAN) has been emerging as one of the most prevailing networking architectures. It is expected that current WLAN technologies will further evolve to obtain much higher performance, more energy efficiency and more robustness. However, the WLAN is still prone to a variety of attacks regardless of the existence of data protection and security association mechanisms. They include but not limited to dictionary attacks against the pre-shared secret key of Wi-Fi Protected Access (WPA)/WPA2, the key reinstallation attack (KRACK) against the handshake procedure of WPA2, etc. Although a brand new WPA3 has been recently standardized by Wi-Fi Alliance to address new security threats, it needs a long time to upgrade currently used access points. Hence, there is a significant gap between security and deployment cost. To fill this gap, we design and implement an intellignet WLAN Early warning Platform (iWEP) to provide an early warning service for clients. Specifically, iWEP adopts intelligence algorithms, e.g., machine learning, to provide the capability of defeating existing popular attacks, including Wired Equivalent Privacy (WEP) secret cracking, WPA/WPA2 dictionary attack, Denial-of-Service and KRACK, by handling behaviour features that are extracted from the compromising procedures in real experimental environments. Moreover, iWEP uses edge computing technology to make a good tradeoff between system performance and WLAN security. Finally, we implement a prototype system of iWEP, and the real results demonstrate its effectiveness.",,978-1-7281-5212-7,10.1109/MSN48538.2019.00079,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066146,"Wireless local area network, early warning platform, edge computing, machine learning",Wireless LAN;Security;Feature extraction;Virtualization;Edge computing;Dictionaries;Machine learning,computer network security;cryptography;data privacy;distributed processing;telecommunication power management;wireless LAN,Wi-Fi Alliance;security threats;access points;deployment cost;intellignet WLAN Early warning Platform;iWEP;early warning service;KRACK;WLAN security;intelligent WLAN Early warning Platform;edge computing;Wireless Local Area Network;prevailing networking architectures;current WLAN technologies;energy efficiency;data protection;security association mechanisms;dictionary attacks;pre-shared secret key;key reinstallation attack;handshake procedure;WPA3,,,14.0,,,,,IEEE,IEEE Conferences
171,Hybrid CAE-VAE for Unsupervised Anomaly Detection in Log File Systems,A. Wadekar; T. Gupta; R. Vijan; F. Kazi,"Veermata Jijabai Technological Institute, Matunga,Electrical Department,Mumbai; Veermata Jijabai Technological Institute, Matunga,Electrical Department,Mumbai; Veermata Jijabai Technological Institute, Matunga,Electrical Department,Mumbai; Veermata Jijabai Technological Institute, Matunga,Electrical Department,Mumbai","2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",,2019,,,1,7,"Anomaly detection is of paramount importance especially in big data systems since these systems log abruptly changing events which generate consequential outliers in their logs. These logs are highly unstructured in nature, hence traditional machine learning methods fail to detect anomalies. Prominent approaches include supervised techniques which require labelled data for their operation and unsupervised techniques that rely on some error metric. Also supervised methods can only capture anomalies present in the dataset, such an approach fails for any new type of anomaly. Hence, the need for unsupervised learning techniques with an easy to interpret anomaly score arises. In this paper, we propose a solution utilizing a hybrid Convolutional Autoencoder-Variational Autoencoder (CAE-VAE) architecture for discrete event sequences which are obtained by processing log files using log keys derived from individual entries. We evaluate our model on Hadoop Distributed File System (HDFS) logs. Unlike most traditional Autoencoder approaches utilizing reconstruction error for anomaly detection, our proposed model derives a likelihood metric which can be interpreted as an anomaly score. We also present a comparative analysis of our models with a supervised CNN model and an unsupervised CAE model and prove empirically how our model gets better results.",,978-1-5386-5906-9,10.1109/ICCCNT45670.2019.8944863,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944863,CNN;CAE;VAE;Anomaly Detection;Big Data;Log Analysis;CAE-VAE,,Big Data;convolutional neural nets;distributed databases;security of data;unsupervised learning,error metric;unsupervised techniques;supervised techniques;traditional machine learning methods;consequential outliers;big data systems;log File systems;unsupervised anomaly detection;hybrid CAE-VAE;unsupervised CAE model;supervised CNN model;traditional Autoencoder approaches;Hadoop Distributed File System logs;log keys;log files;discrete event sequences;hybrid Convolutional Autoencoder-Variational Autoencoder;anomaly score;unsupervised learning techniques;supervised methods,,,28.0,,,,,IEEE,IEEE Conferences
172,Blockchain for AI: Review and Open Research Challenges,K. Salah; M. H. U. Rehman; N. Nizamuddin; A. Al-Fuqaha,"Department of Electrical and Computer Engineering, Khalifa University of Science and Technology, Abu Dhabi, UAE; Department of Computer Science, National University of Computer and Emerging Sciences, Lahore, Pakistan; Department of Electrical and Computer Engineering, Khalifa University of Science and Technology, Abu Dhabi, UAE; Computer Science Department, NEST Research Lab, College of Engineering and Applied Sciences, Western Michigan University, Kalamazoo, MI, USA",IEEE Access,,2019,7,,10127,10149,"Recently, artificial intelligence (AI) and blockchain have become two of the most trending and disruptive technologies. Blockchain technology has the ability to automate payment in cryptocurrency and to provide access to a shared ledger of data, transactions, and logs in a decentralized, secure, and trusted manner. Also with smart contracts, blockchain has the ability to govern interactions among participants with no intermediary or a trusted third party. AI, on the other hand, offers intelligence and decision-making capabilities for machines similar to humans. In this paper, we present a detailed survey on blockchain applications for AI. We review the literature, tabulate, and summarize the emerging blockchain applications, platforms, and protocols specifically targeting AI area. We also identify and discuss open research challenges of utilizing blockchain technologies for AI.",2169-3536,,10.1109/ACCESS.2018.2890507,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598784,Artificial intelligence;machine learning;blockchain;cybersecurity;smart contracts;consensus protocols,Blockchain;Smart contracts;Machine learning;Decision making;Machine learning algorithms;Data mining,artificial intelligence;decision making,trending technologies;disruptive technologies;blockchain technology;AI area;open research challenges;artificial intelligence;blockchain applications;decision-making capabilities,,32.0,141.0,,,,,IEEE,IEEE Journals
173,An exploratory study on the benefits of using natural language for explaining fuzzy rule-based systems,J. M. Alonso; A. Ramos-Soto; E. Reiter; K. van Deemter,"Centro Singular de Investigación en Tecnoloxías da Información (CiTIUS), Universidade de Santiago de Compostela; Centro Singular de Investigación en Tecnoloxías da Información (CiTIUS), Universidade de Santiago de Compostela; Department of Computing Science, University of Aberdeen; Department of Computing Science, University of Aberdeen",2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,2017,,,1,6,"This paper presents an empirical research. It focuses on testing empirically the benefits of providing users, in a specific domain, with textual interpretation of the fuzzy inferences carried out by a fuzzy classifier for a given selection of samples. The hypothesis to test is as follows: “Users understand easier the decision made by a fuzzy system when they are provided with a textual interpretation of the fuzzy inference mechanism that the system carried out”. This hypothesis was successfully tested in a web survey. The application domain was leaf classification. The fuzzy classifiers were built with the GUAJE fuzzy modeling open source software which is aimed at generating interpretable fuzzy systems. The textual interpretation was handmade by an expert who followed the guidelines of the Natural Language Generation approach proposed by Reiter and Dale. Reported results encourage us to go on with a series of additional experiments devoted to deeply explore how Natural Language Generation techniques can contribute to facilitate the understanding of fuzzy systems.",1558-4739,978-1-5090-6034-4,10.1109/FUZZ-IEEE.2017.8015489,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8015489,,Pragmatics;Fuzzy systems;Natural languages;Artificial intelligence;Fuzzy logic;Open source software,fuzzy reasoning;fuzzy systems;knowledge based systems;natural language processing;pattern classification;public domain software;text analysis,fuzzy rule-based systems;fuzzy classifier;fuzzy system;fuzzy inference mechanism;leaf classification;GUAJE fuzzy modeling open source software;textual interpretation;natural language generation approach,,4.0,29.0,,,,,IEEE,IEEE Conferences
174,Fooling Network Interpretation in Image Classification,A. Subramanya; V. Pillai; H. Pirsiavash,UMBC; UMBC; UMBC,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,2019,,,2020,2029,"Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.",2380-7504,978-1-7281-4803-8,10.1109/ICCV.2019.00211,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010911,,Prediction algorithms;Neural networks;Robustness;Tools;Task analysis,decision making;image classification;learning (artificial intelligence);neural nets;security of data,fooling network interpretation;image classification;deep neural networks;adversarial attack algorithms;adversarial patches;standard network interpretation algorithms;Grad-CAM interpretation;patch interpretation;robust network interpretation tools;decision making process,,,35.0,,,,,IEEE,IEEE Conferences
175,How to Prevent Skynet from Forming (A Perspective from Policy-Based Autonomic Device Management),S. Calo; D. Verma; E. Bertino; J. Ingham; G. Cirincione,"IBM Res., Yorktown Heights, NY, USA; IBM Res., Yorktown Heights, NY, USA; Purdue Univ., West Lafayette, IN, USA; UK DSTL, Salisbury, UK; Army Res. Labs., Adelphi, MD, USA",2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS),,2018,,,1369,1376,"Artificial Intelligence (AI) in the context of military systems has frequently been portrayed as dangerous, and as leading to humanity being put in danger by an errant AI system, such as the Skynet imagined in the Terminator movie series. At the same time, the benefits of using AI in such systems are numerous. Therefore, we need to develop techniques that will let military systems benefit from the advances in AI, while ensuring that a system like Skynet never turns against humanity. In this paper, we examine the problem from the perspective of device management, a set of intelligent systems that manage themselves and determine their own policies. We discuss mechanisms that could be used to prevent these systems from becoming malignant.",2575-8411,978-1-5386-6871-9,10.1109/ICDCS.2018.00137,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416400,"AI, military systems, policy-based management, distributed systems, collaborative systems",Security;Sensors;Motion pictures;Actuators;Organizations;Machine learning,artificial intelligence;military computing;military systems,Skynet;artificial intelligence;intelligent systems;Terminator movie series;errant AI system;military systems;policy-based autonomic device management,,1.0,18.0,,,,,IEEE,IEEE Conferences
176,The Effect of Semantic Interaction on Foraging in Text Analysis,J. Wenskovitch; L. Bradel; M. Dowling; L. House; C. North,Virginia Tech Computer Science; Department of Defense; Virginia Tech Computer Science; Virginia Tech Statistics; Virginia Tech Computer Science,2018 IEEE Conference on Visual Analytics Science and Technology (VAST),,2018,,,13,24,"Completing text analysis tasks is a continuous sensemaking loop of foraging for information and incrementally synthesizing it into hypotheses. Past research has shown the advantages of using spatial workspaces as a means for synthesizing information through externalizing hypotheses and creating spatial schemas. However, spatializing the entirety of datasets becomes prohibitive as the number of documents available to the analysts grows, particularly when only a small subset are relevant to the task at hand. StarSPIRE is a visual analytics tool designed to explore collections of documents, leveraging users' semantic interactions to steer (1) a synthesis model that aids in document layout, and (2) a foraging model to automatically retrieve new relevant information. In contrast to traditional keyword search foraging (KSF), “semantic interaction foraging” (SIF) occurs as a result of the user's synthesis actions. To quantify the value of semantic interaction foraging, we use StarSPIRE to evaluate its utility for an intelligence analysis sensemaking task. Semantic interaction foraging accounted for 26% of useful documents found, and it also resulted in increased synthesis interactions and improved sensemaking task performance by users in comparison to only using keyword search.",,978-1-5386-6861-0,10.1109/VAST.2018.8802424,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802424,Human-centered computing;Visualization;Empirical studies in visualization;Human-centered computing;Visualization;Visual analytics,Semantics;Layout;Task analysis;Analytical models;Computational modeling;Data visualization;Visual analytics,data analysis;data visualisation;information retrieval;text analysis,text analysis tasks;spatial workspaces;foraging model;semantic interaction foraging;intelligence analysis sensemaking task;sensemaking task performance;sensemaking loop;StarSPIRE;visual analytics tool;document layout;keyword search foraging;information retrieval,,1.0,53.0,,,,,IEEE,IEEE Conferences
178,Explainable Software Bot Contributions: Case Study of Automated Bug Fixes,M. Monperrus,KTH Royal Institute of Technology,2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering (BotSE),,2019,,,12,15,"In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans. Dry contributions, with no explanation, are often ignored or rejected, because the contribution is not understandable per se, because they are not put into a larger context, because they are not grounded on idioms shared by the core community of developers. We have been operating a program repair bot called Repairnator for 2 years and noticed the problem of ""dry patches"": a patch that does not say which bug it fixes, or that does not explain the effects of the patch on the system. We envision program repair systems that produce an ""explainable bug fix"": an integrated package of at least 1) a patch, 2) its explanation in natural or controlled language, and 3) a highlight of the behavioral difference with examples. In this paper, we generalize and suggest that software bot contributions must explainable, that they must be put into the context of the global software development conversation.",,978-1-7281-2262-5,10.1109/BotSE.2019.00010,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823632,software engineering;program repair;software bot,Bot (Internet);Computer bugs;Maintenance engineering;Open source software;Natural languages;Collaboration,program debugging;software maintenance,software project;program repair bot;dry patches;program repair systems;natural language;controlled language;automated bug fixes;open-source project;software development conversation;software bot contributions;repairnator,,,11.0,,,,,IEEE,IEEE Conferences
179,Database Acquisition for the Lung Cancer Computer Aided Diagnostic Systems,A. Meldo; L. Utkin; A. Lukashin; V. Muliukha; V. Zaborovsky,"Clinical Research Center of Specialized Types of Medical Care (Oncological),St.Petersburg,Russia; Peter the Great St.Petersburg Polytechnic University (SPbPU),St.Petersburg,Russia; Peter the Great St.Petersburg Polytechnic University (SPbPU),St.Petersburg,Russia; Peter the Great St.Petersburg Polytechnic University (SPbPU),St.Petersburg,Russia; Peter the Great St.Petersburg Polytechnic University (SPbPU),St.Petersburg,Russia",2019 25th Conference of Open Innovations Association (FRUCT),,2019,,,220,227,"Most of the used computer aided diagnostic (CAD) systems based on applying the deep learning algorithms are similar from the point of view of data processing stages. The main typical stages are the training data acquisition, pre-processing, segmentation and classification. Homogeneity of a training dataset structure and its completeness are very important for minimizing inaccuracies in the development of the CAD systems. The main difficulties in the medical training data acquisition are concerned with their heterogeneity and incompleteness. Another problem is a lack of a sufficient large amount of data for training deep neural networks which are a basis of the CAD systems. In order to overcome these problems in the lung cancer CAD systems, a new methodology of the dataset acquisition is proposed by using as an example the database called LIRA which has been applied to training the intellectual lung cancer CAD system called by Dr. AIzimov. One of the important peculiarities of the dataset LIRA is the morphological confirmation of diseases. Another peculiarity is taking into account and including “atypical” cases from the point of view of radiographic features. The database development is carried out in the interdisciplinary collaboration of radiologists and data scientists developing the CAD system.",2305-7254,978-952-69244-0-3,10.23919/FRUCT48121.2019.8981537,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8981537,,,cancer;data acquisition;database management systems;image classification;learning (artificial intelligence);lung;medical image processing;neural nets,database development;database acquisition;lung cancer computer aided diagnostic systems;deep learning algorithms;data segmentation;data classification;medical training data acquisition;lung cancer CAD systems;dataset acquisition;LIRA;deep neural networks,,,23.0,,,,,IEEE,IEEE Conferences
180,Feature Enrichment Based Convolutional Neural Network for Heartbeat Classification From Electrocardiogram,Q. Xie; S. Tu; G. Wang; Y. Lian; L. Xu,"Department of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China",IEEE Access,,2019,7,,153751,153760,"Correct heartbeat classification from electrocardiogram (ECG) signals is fundamental to the diagnosis of arrhythmia. The recent advancement in deep convolutional neural network (CNN) has renewed the interest in applying deep learning techniques to improve the accuracy of heartbeat classification. So far, the results are not very exciting. Most of the existing methods are based on ECG morphological information, which makes deep learning difficult to extract discriminative features for classification. Towards an opposite direction of feature extraction or selection, this paper proceeds along a recent proposed direction named feature enrichment (FE). To exploit the advantage of deep learning, we develop a FE-CNN classifier by enriching the ECG signals into time-frequency images by discrete short-time Fourier transform and then using the images as the input to CNN. Experiments on MIT-BIH arrhythmia database show FE-CNN obtains sensitivity (Sen) of 75.6%, positive predictive rate (Ppr) of 90.1%, and F1 score of 0.82 for the detection of supraventricular ectopic (S) beats. Sen, Ppr, and F1 score are 92.8%, 94.5%, and 0.94, respectively, for ventricular ectopic (V) beat detection. The result demonstrates our method outperforms state-of-the-art algorithms including other CNN based methods, without any hand-crafted features, especially F1 score for S beat detection from 0.75 to 0.82. This FE-CNN classifier is simple, effective, and easy to be applied to other types of vital signs.",2169-3536,,10.1109/ACCESS.2019.2948857,National Science and Technology Innovation 2030 Major Project of the Ministry of Science and Technology of China; Shanghai Jiao Tong University; National Natural Science Foundation of China; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8879514,Electrocardiogram;feature enrichment;short-time Fourier transform;convolutional neural network,Electrocardiography;Feature extraction;Heart beat;Deep learning;Heart rate variability;Neural networks;Time-frequency analysis,convolutional neural nets;discrete Fourier transforms;electrocardiography;feature extraction;learning (artificial intelligence);medical signal detection;medical signal processing;signal classification,electrocardiogram signals;deep convolutional neural network;supraventricular ectopic beat detection;ventricular ectopic beat detection;discrete short-time Fourier transform;feature enrichment;correct heartbeat classification;hand-crafted features;CNN based methods;supraventricular ectopic beats;Sen;MIT-BIH arrhythmia database;short-time Fourier transform;time-frequency images;ECG signals;FE-CNN classifier;feature extraction;opposite direction;discriminative features;deep learning techniques;ECG morphological information,,,32.0,CCBY,,,,IEEE,IEEE Journals
181,An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection,H. Homayouni; S. Ghosh; I. Ray; M. G. Kahn,"Colorado State University,Department of Computer Science; Colorado State University,Department of Computer Science; Colorado State University,Department of Computer Science; University of Colorado Denver,Anschutz Medical Campus",2019 IEEE International Conference on Big Data (Big Data),,2019,,,200,205,"Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9006446,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006446,Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning,Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection,Big Data;neural nets;unsupervised learning,interactive learning technique;ADQuaTe2;unsupervised deep learning technique;automated data quality test approach;faulty data records;constraint violations;automated test approaches;semantic constraints;syntactic constraints;heterogeneous data;data quality tests;interactive data quality test approach;fault detection;constraint discovery,,,28.0,,,,,IEEE,IEEE Conferences
182,Industry-Driven Visual Analytics for Understanding Financial Timeseries Models,D. Jonker; R. Brath; S. Langevin,Uncharted Software Inc.; Uncharted Software Inc.; Uncharted Software Inc.,2019 23rd International Conference Information Visualisation (IV),,2019,,,210,215,"Timeseries models are used extensively in financial services, for example, to quantify risk and predict economics. However, analysts also need to comprehend the structure and behavior of these models to better understand and explain results. We present a methodology, derived from extensive industry experience, to aid explanation through integrated interactive visualizations that reveal model structure and behavior of constituent timeseries factors, thereby increasing understanding of the model, the domain and the sensitivities. Expert feedback indicates alignment with mental models.",2375-0138,978-1-7281-2838-2,10.1109/IV.2019.00043,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811947,"timeseries model, timeseries visualization, factor model",Biological system modeling;Analytical models;Predictive models;Data visualization;Economic indicators;Data models;Visual analytics,data visualisation;economics;financial data processing;time series,industry-driven visual analytics;financial services;integrated interactive visualizations;model structure;constituent timeseries factors;financial timeseries models;economics,,,20.0,,,,,IEEE,IEEE Conferences
185,An Explainable Hybrid Model for Bankruptcy Prediction Based on the Decision Tree and Deep Neural Network,T. Chou,"Chaoyang University of Technology 168,Jifeng E. Rd., Wufeng District Taichung,Taiwan",2019 IEEE 2nd International Conference on Knowledge Innovation and Invention (ICKII),,2019,,,122,125,"For investors seeking solutions to optimize their portfolio of assets to generate profits and minimize losses, choosing a sophisticated model to evaluate the risk of corporate financial distress will be crucial to support their asset management and investment decisions. Both the machine learning and the newly developed deep learning techniques have been employed to construct bankruptcy prediction models for decades. However, applying the deep learning models might increase the predictive accuracy in exchange for losing model interpretability, because the structure and parameters of the model are not easy to provide accountability for investors. In this study, a hybrid approach integrating the decision tree with the deep neural network was proposed to provide a compromise solution for investors. The decision tree was adopted as the primary model to provide explainable ability, while the deep neural network was chosen to improve predictive accuracy. The decision fusion of two models was designed with the compensatory and non-compensatory approaches. The hybrid model was implemented by concatenating the deep neural network to the selected branches of decision tree that perform poor predictive accuracy during model training. The empirical results showed that the predictive accuracy of the deep neural network and the decision tree were 80% and 87% respectively, and the overall accuracy was improved to 91% by the hybrid model.",,978-1-7281-0110-1,10.1109/ICKII46306.2019.9042639,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042639,deep neural network;decision tree;model Interpretability,,bankruptcy;decision trees;financial data processing;investment;learning (artificial intelligence);neural nets,model training;deep neural network;decision tree;explainable hybrid model;investors;asset management;investment decisions;bankruptcy prediction models;deep learning models;model interpretability;primary model;assest portfolio;hybrid approach;compensatory approach;noncompensatory approach;predictive accuracy;decision fusion,,,7.0,,,,,IEEE,IEEE Conferences
186,Redefining Data Transparency: A Multidimensional Approach,E. Bertino; S. Merrill; A. Nesen; C. Utz,"Computer Science, Purdue University, United States; Computer Science, Purdue University, United States; Computer Science, Purdue University, United States; Electrical Engineering and Information Technology, Ruhr-Universitat Bochum",Computer,,2019,52,1.0,16,26,"The use of big data combined with powerful machine-learning algorithms raises major concerns over potential adverse effects. Consequently, data transparency is critical for many data-intensive applications. We provide a comprehensive definition, elaborate on various concerns, and articulate an initial road map for critical research challenges.",1558-0814,,10.1109/MC.2018.2890190,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666655,,Law;Data privacy;Machine learning algorithms;Big Data,Big Data;learning (artificial intelligence),big data;potential adverse effects;data transparency;data-intensive applications;multidimensional approach;machine-learning algorithms,,,38.0,,,,,IEEE,IEEE Magazines
187,Bi-directional Causal Graph Learning through Weight-Sharing and Low-Rank Neural Network,H. Huang; C. Xu; S. Yoo,GE Global Research; Stony Brook University; Brookhaven National Laboratory,2019 IEEE International Conference on Data Mining (ICDM),,2019,,,319,328,"Discovering the causal graph in multivariate time series data is of great importance for industrial society, yet challenging due to the unknown nonlinearity in the data. Existing works only explore the data in chronological order, and rely on pre-assumed kernels or certain distribution assumption. In this paper, we present a Bi-directional neural network for Causal Graph Learning (Bi-CGL) through weight-sharing and low-rank neural network. It discovers the causal graph by simultaneously exploring input in forward and reverse chronological order. Both directions approach the same causal graph with shared low-rank approximation, which provides robustness and better accuracy against data noise. Experiments on synthetic and real world datasets prove our Bi-CGL's outperformance over existing baselines.",2374-8486,978-1-7281-4604-1,10.1109/ICDM.2019.00042,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971001,causal graph learning;bi-directional neural network;weight-sharing,,causality;graph theory;learning (artificial intelligence);neural nets;time series,low-rank neural network;multivariate time series data;chronological order;pre-assumed kernels;weight-sharing;shared low-rank approximation;data noise;bi-directional causal graph learning;Bi-CGL;bi-directional neural network,,,38.0,,,,,IEEE,IEEE Conferences
188,The need for fuzzy AI,J. M. Garibaldi,"School of Computer Science, University of Nottingham, Nottingham, NG8 1BB, UK",IEEE/CAA Journal of Automatica Sinica,,2019,6,3.0,610,622,"Artificial intelligence (AI) is once again a topic of huge interest for computer scientists around the world. Whilst advances in the capability of machines are being made all around the world at an incredible rate, there is also increasing focus on the need for computerised systems to be able to explain their decisions, at least to some degree. It is also clear that data and knowledge in the real world are characterised by uncertainty. Fuzzy systems can provide decision support, which both handle uncertainty and have explicit representations of uncertain knowledge and inference processes. However, it is not yet clear how any decision support systems, including those featuring fuzzy methods, should be evaluated as to whether their use is permitted. This paper presents a conceptual framework of indistinguishability as the key component of the evaluation of computerised decision support systems. Case studies are presented in which it has been clearly demonstrated that human expert performance is less than perfect, together with techniques that may enable fuzzy systems to emulate human-level performance including variability. In conclusion, this paper argues for the need for “fuzzy AI” in two senses: (i) the need for fuzzy methodologies (in the technical sense of Zadeh's fuzzy sets and systems) as knowledge-based systems to represent and reason with uncertainty; and (ii) the need for fuzziness (in the non-technical sense) with an acceptance of imperfect performance in evaluating AI systems.",2329-9274,,10.1109/JAS.2019.1911465,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8707102,,Uncertainty;Cognition;Task analysis;Deep learning;Expert systems,decision support systems;fuzzy set theory;fuzzy systems;inference mechanisms;knowledge based systems;uncertainty handling,explicit representations;uncertain knowledge;inference processes;computerised decision support systems;human expert performance;Zadeh's fuzzy sets;knowledge-based systems;fuzzy AI;artificial intelligence;computer scientists;human-level performance,,2.0,,,,,,IEEE,IEEE Journals
189,AI for Testing Today and Tomorrow: Industry Perspectives,T. M. King; J. Arbon; D. Santiago; D. Adamo; W. Chin; R. Shanmugam,Ultimate Software; Test.ai; Ultimate Software; Ultimate Software; Intel Corporation; AutonomIQ,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),,2019,,,81,88,"With modern advances in artificial intelligence (AI) and machine learning and their applications to software testing, the intersection of AI and testing is receiving close attention. The 2018 Annual Western Conference on Software Testing Analysis and Review featured a two-session panel on AI for Software Testing (AIST). The panel brought together six industry experts with experience developing AIST products, services, and research prototypes. Questions sourced from the industrial testing community were used to provoke thought, stimulate conversation, and guide panel discussions. This paper provides a review of the industry panel, which includes discussions on the visions, ideas, thoughts, strategies, directions, and lessons learned developing systems that use AI to test software, applying methods to test AI systems, and designing self-testing systems. Both the testing community survey and the expert panel yielded insightful perspectives on AIST in practice.",,978-1-7281-0492-8,10.1109/AITest.2019.000-3,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718229,Artificial Intelligence;Machine Learning;Software Testing;Industry;Panel;Survey,Artificial intelligence;Software;Manuals;Software testing;Industries;Built-in self-test,learning (artificial intelligence);program testing,AI systems;self-testing systems;testing community survey;expert panel;industry perspectives;artificial intelligence;machine learning;two-session panel;industry experts;AIST products;industrial testing community;guide panel discussions;industry panel;software testing analysis;2018 annual western conference,,,47.0,,,,,IEEE,IEEE Conferences
190,Deploying Artificial Intelligence in the Wireless Infrastructure: the Challenges Ahead,M. Á. Vázquez; J. P. Pallois; M. Debbah; C. Masouros; T. Kenyon; Y. Deng; F. Mekuria; A. Pérez-Neira; J. Erfanian,"Centre Tecnològic de,Telecomunicacions de Catalunya,Castelldefels,Spain; Huawei Technologies,Paris,France; Huawei Research Centre,Paris,France; University College London,London,United Kingdom; King's College London,London,United Kingdom; King's College London,London,United Kingdom; Council for Scientific and Industrial Research,Pretoria,South Africa; Centre Tecnològic de, Univeritat Politécnica de Catalunya,Telecomunicacions de Catalunya,Castelldefels,Spain; Bell Mobility,Ontario,Canada",2019 IEEE 2nd 5G World Forum (5GWF),,2019,,,458,459,"The adoption of artificial intelligence (AI) techniques entails a substantial change in the wireless ecosystem where data as well as their owners become crucial. As a result, the roll out of AI techniques in wireless systems raises a plethora of questions. In this context, we describe the challenges observed by the wireless stakeholders when deploying AI. Furthermore, we introduce the recent discussion in field of ethics that appear when managing wireless communications data.",,978-1-7281-3627-1,10.1109/5GWF.2019.8911693,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911693,Artificial Intelligence;Machine Learning;Wireless Infrastructure,Artificial intelligence;5G mobile communication;Ethics;Wireless networks;Telecommunications;Quality of service,artificial intelligence;telecommunication computing;wireless channels,wireless communications data;wireless stakeholders;wireless systems;AI techniques;wireless ecosystem;artificial intelligence techniques;wireless infrastructure,,,3.0,,,,,IEEE,IEEE Conferences
191,A Cyber-Physical System for Semi-Autonomous Oil&Gas Drilling Operations,Y. Mualla; A. Najjar; O. Boissier; S. Galland; I. Tchappi Haman; R. Vanet,"CIAD, Univ. Bourgogne Franche-Comte, Belfort, France; Umea Univ., Umea, Sweden; IMT Mines St.-Etienne, Univ. Lyon, St. Etienne, France; CIAD, Univ. Bourgogne Franche-Comte, Belfort, France; Univ. of Ngaoundere, Ngaoundere, Cameroon; IMT Mines St.-Etienne, Univ. Lyon, St. Etienne, France",2019 Third IEEE International Conference on Robotic Computing (IRC),,2019,,,514,519,"In Oil&Gas drilling operations and after reaching deep drilled depths, high temperature increases significantly enough to damage the down-hole drilling tools, and the existing mitigation process is insufficient. In this paper, we propose a Cyber-Physical System (CPS) where agents are used to represent the collaborating entities in Oil&Gas fields both up-hole and down-hole. With the proposed CPS, down-hole tools respond to high temperature autonomously with a decentralized collective voting based on the tools' internal decision model while waiting for the cooling performed up-hole by the field engineer. This decision model, driven by the tools' specifications, aims to withstand high temperature. The proposed CPS is implemented using a multiagent simulation environment, and the results show that it mitigates high temperature properly with both the voting and the cooling mechanisms.",,978-1-5386-9245-5,10.1109/IRC.2019.00107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675573,cyber-physical systems;multiagent systems;human robot interaction;oil and gas drilling operations,Tools;Temperature measurement;Actuators;Temperature sensors;Drilling machines;Cooling,cooling;cyber-physical systems;decision making;drilling (geotechnical);drilling machines;high-temperature effects;human-robot interaction;multi-agent systems;natural gas technology;oil drilling,cyber-physical system;down-hole drilling tools;decision model;oil and gas fields;high temperature effects;cooling mechanisms;multiagent systems;human robot interaction,,,24.0,,,,,IEEE,IEEE Conferences
192,Transforming Convolutional Neural Network to an Interpretable Classifier,M. Tamajka; W. Benesova; M. Kompanek,"Institute of Computer Engineering and Applied Informatics Faculty of Informatics and Information Technologies, Slovak University of Technology in Bratislava, Bratislava, 842 16, Slovakia; Institute of Computer Engineering and Applied Informatics Faculty of Informatics and Information Technologies, Slovak University of Technology in Bratislava, Bratislava, 842 16, Slovakia; Institute of Computer Engineering and Applied Informatics Faculty of Informatics and Information Technologies, Slovak University of Technology in Bratislava, Bratislava, 842 16, Slovakia","2019 International Conference on Systems, Signals and Image Processing (IWSSIP)",,2019,,,255,259,"For machine learning based methods to be accepted in practice, these methods should be either directly or indirectly interpretable or should at least provide a reliable estimate of a confidence degree about their prediction. This is of the importance primarily in critical domains like medicine where the decision should be clearly justified. Although deep neural networks achieve remarkable performance in almost any computer vision task, due to their complexity, their predictions remain difficult to interpret and explain. An interpretable classifier does not only provide the prediction but also a kind of evidence or explanation. In this paper, we present an effective way of transforming a sufficiently well trained convolutional neural network (98.50% accuracy on MNIST dataset, which contains small images of digits) to an interpretable classifier. We use hidden activations of training observations as their descriptors. Next, for every unseen observation, we use these descriptors to identify three most similar training samples based on cosine distance of their hidden activations. Using this approach, we are able not only to predict the class of an unknown observation, but also to justify the prediction by providing three most similar training observations in exchange to a slightly decreased accuracy (98.12%).",2157-8702,978-1-7281-3227-3,10.1109/IWSSIP.2019.8787211,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787211,convolutional neural network;deep learning;explainability;interpretability,Training;Databases;Convolutional neural networks;Task analysis;Predictive models;Feature extraction,convolutional neural nets;learning (artificial intelligence);pattern classification;statistical analysis,interpretable classifier;deep neural networks;computer vision task;hidden activations;machine learning based methods;convolutional neural network;cosine distance,,,12.0,,,,,IEEE,IEEE Conferences
193,Interpreting Deep Visual Representations via Network Dissection,B. Zhou; D. Bau; A. Oliva; A. Torralba,"CSAIL, MIT, Cambridge, MA, USA; CSAIL, MIT, Cambridge, MA, USA; CSAIL, MIT, Cambridge, MA, USA; CSAIL, MIT, Cambridge, MA, USA",IEEE Transactions on Pattern Analysis and Machine Intelligence,,2019,41,9.0,2131,2145,"The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. In this work, we describe Network Dissection, a method that interprets networks by providing meaningful labels to their individual units. The proposed method quantifies the interpretability of CNN representations by evaluating the alignment between individual hidden units and visual semantic concepts. By identifying the best alignments, units are given interpretable labels ranging from colors, materials, textures, parts, objects and scenes. The method reveals that deep representations are more transparent and interpretable than they would be under a random equivalently powerful basis. We apply our approach to interpret and compare the latent representations of several network architectures trained to solve a wide range of supervised and self-supervised tasks. We then examine factors affecting the network interpretability such as the number of the training iterations, regularizations, different initialization parameters, as well as networks depth and width. Finally we show that the interpreted units can be used to provide explicit explanations of a given CNN prediction for an image. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into what hierarchical structures can learn.",1939-3539,,10.1109/TPAMI.2018.2858759,DARPA XAI; NSF; NSF; ONR; MIT Big Data Initiative at CSAIL; Toyota Research Institute MIT CSAIL Joint Research Center; Google; Amazon; Nvidia; Facebook; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417924,Convolutional neural networks;network interpretability;visual recognition;interpretable machine learning,Visualization;Detectors;Training;Image color analysis;Task analysis;Image segmentation;Semantics,convolutional neural nets;image classification;image representation;learning (artificial intelligence),network dissection;deep convolutional neural networks;CNN prediction;network depth;hidden units;deep visual representations;interpreted units;network interpretability;network architectures;latent representations;random equivalently powerful basis;deep representations;interpretable labels;visual semantic concepts;meaningful labels;hidden representations,,1.0,57.0,,,,,IEEE,IEEE Journals
194,Social Network Chatbots for Smoking Cessation: Agent and Multi-Agent Frameworks,D. Calvaresi; J. Calbimonte; F. Dubosson; A. Najjar; M. Schumacher,"HES-SO Valais-Wallis,Sierre,Switzerland; HES-SO Valais-Wallis,Sierre,Switzerland; HES-SO Valais-Wallis,Sierre,Switzerland; Luxembourg University,Luxembourg; HES-SO Valais-Wallis,Sierre,Switzerland",2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI),,2019,,,286,292,"Asynchronous messaging is leading human-machine interaction due to the boom of mobile devices and social networks. The recent release of dedicated APIs from messaging platforms boosted the development of computer programs able to conduct conversations, (i.e., chatbots), which have been adopted in several domain-specific contexts. This paper proposes SMAG: a chatbot framework supporting a smoking cessation program (JDF) deployed on a social network. In particular, it details the single-agent implementation, the campaign results, a multi-agent design for SMAG enabling the modelization of personalized behavior and user profiling, and highlighting of coupling chatbot technology with and multi-agent systems.",,978-1-4503-6934-3,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8909619,MAS;agent-based chatbots;smoking cessation,Quality of experience;Facebook;Mood;Couplings,application program interfaces;electronic messaging;human computer interaction;multi-agent systems;social networking (online);software agents,social network chatbots;asynchronous messaging;human-machine interaction;mobile devices;APIs;computer programs;domain-specific contexts;SMAG;smoking cessation program;single-agent implementation;multiagent design;chatbot technology;multiagent systems;user profiling,,,40.0,,,,,IEEE,IEEE Conferences
195,Research on Command Decision Support System AI Problem Decomposition,X. Jin,"Sci. & Technol. on Inf. Syst. Eng. Lab., Nanjing, China",2018 10th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC),,2018,02,,96,100,"The rapid development of Artificial Intelligence technology has led to the victory of AlphaGo, making people see the hope of intelligent military command decision support system. However, battlefield is quite complex, while chessboard is significantly abstracted. It is impossible to copy its method directly, as machine learning from observations directly to orders, or from orders directly to results is too difficult. This paper attempts to decompose the problem of intelligent command decision support system into more specific problems, and analyze which type of AI methods suit which problems, through the local technology breakthrough to achieve the overall level of intelligence to enhance, with a certain guiding significance to development of intelligent command decision support system.",,978-1-5386-5836-9,10.1109/IHMSC.2018.10129,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530190,"Command Decision Support System, Artificial Intelligence, Knowledge System, Machine Learning",Games;Decision making;Planning;Decision support systems;Cognition;Machine learning,command and control systems;decision support systems;learning (artificial intelligence),command decision support system AI problem decomposition;intelligent military command decision support system;artificial intelligence technology;AlphaGo;machine learning,,,12.0,,,,,IEEE,IEEE Conferences
196,Fast Lip Feature Extraction Using Psychologically Motivated Gabor Features,A. Abel; C. Gao; L. Smith; R. Watt; A. Hussain,"Computing Science and Software Engineering, Xi‘an Jiaotong-Liverpool University, Suzhou, China; Computing Science and Software Engineering, Xi‘an Jiaotong-Liverpool University, Suzhou, China; Faculty of Natural Sciences, University of Stirling, Stirling, Scotland; Faculty of Natural Sciences, University of Stirling, Stirling, Scotland; Faculty of Natural Sciences, University of Stirling, Stirling, Scotland",2018 IEEE Symposium Series on Computational Intelligence (SSCI),,2018,,,1033,1040,"The extraction of relevant lip features is of continuing interest in the speech domain. Using end-to-end feature extraction can produce good results, but at the cost of the results being difficult for humans to comprehend and relate to. We present a new, lightweight feature extraction approach, motivated by glimpse based psychological research into racial barcodes. This allows for 3D geometric features to be produced using Gabor based image patches. This new approach can successfully extract lip features with a minimum of processing, with parameters that can be quickly adapted and used for detailed analysis, and with preliminary results showing successful feature extraction from a range of different speakers. These features can be generated online without the need for trained models, and are also robust and can recover from errors, making them suitable for real world speech analysis.",,978-1-5386-9276-9,10.1109/SSCI.2018.8628931,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8628931,image processing;barcodes;gabor;lip-reading;word features,Feature extraction;Lips;Mouth;Psychology;Adaptation models;Shape;Training,feature extraction;psychology,lip feature extraction;Gabor features;3D geometric features;glimpse based psychological research;Gabor based image patches;speech domain;end-to-end feature extraction,,,30.0,,,,,IEEE,IEEE Conferences
197,Graphically Hearing: Enhancing Understanding of Geospatial Data through an Integrated Auditory and Visual Experience,A. Gune; R. De Amicis; B. Simões; C. A. Sanchez; H. O. Demirel,Oregon State University; Oregon State University; Vicomtech; Oregon State University; Oregon State University,IEEE Computer Graphics and Applications,,2018,38,4.0,18,26,"Effective presentation of data is critical to a users understanding of it. In this manuscript, we explore research challenges associated with presenting large geospatial datasets through a multimodal experience. We also suggest an interaction schema that enhances users cognition of geographic information through a user-driven display that visualizes and sonifies geospatial data.",1558-1756,,10.1109/MCG.2018.042731655,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402185,GIS;geospatial data;spatial data infrastructure;glTF;geovisualization;human computer interaction;sonification;computer graphics,Sonification;Visualization;Geospatial analysis;Spatial databases;Cognition;Data visualization;Auditory system,cognition;data visualisation;geographic information systems;hearing;human factors,user-driven display;geospatial data;integrated auditory;visual experience;geospatial datasets;multimodal experience;users cognition;geographic information;geospatial data visualisation,"Algorithms;Audiovisual Aids;Computer Graphics;Databases, Factual;Geographic Information Systems;Humans",,15.0,,,,,IEEE,IEEE Magazines
201,Verified Programs for Frequent Itemset Mining,F. Loulergue; C. Whitney,"Sch. of Inf. Comput. & Cyber Syst., Northern Arizona Univ., Flagstaff, AZ, USA; Sch. of Inf. Comput. & Cyber Syst., Northern Arizona Univ., Flagstaff, AZ, USA","2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",,2018,,,1516,1523,"Frequent itemset mining is one pillar of machine learning and is very important for many data mining applications. There are many different algorithms for frequent itemset mining, but to our knowledge no implementation has been proven correct using computer aided verification. Hu et al. derived on paper an efficient algorithm for this problem, starting from an inefficient functional program and by using program calculation derived an efficient version. Based on their work, we propose a formally verified functional implementation for frequent itemset mining developed with the Coq proof assistant. All the proposed programs are evaluated on classical datasets and are compared to a non verified Java implementation of the Apriori algorithm.",,978-1-5386-9380-3,10.1109/SmartWorld.2018.00262,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560240,Formal verification;Coq proof assistant;data mining;frequent itemset mining;functional programming;parallel programming,,data mining;learning (artificial intelligence);program verification;theorem proving,frequent itemset mining;data mining;verified programs;machine learning;computer aided verification;program calculation;Coq proof assistant,,,25.0,,,,,IEEE,IEEE Conferences
203,Visual Analytics for Explainable Deep Learning,J. Choo; S. Liu,Korea University; Tsinghua University,IEEE Computer Graphics and Applications,,2018,38,4.0,84,92,"Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. This article reviews visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discusses potential challenges and future research directions.",1558-1756,,10.1109/MCG.2018.042731661,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402187,explainable deep learning;interactive visualization;deep learning;computer graphics,Machine learning;Visual analytics;Computational modeling;Predictive models;Data visualization,data visualisation;decision making;learning (artificial intelligence),visual analytics;explainable deep learning;artificial intelligence techniques;deep learning models;internal processes;critical decision-making processes;precision medicine;law enforcement;information visualization;machine learning perspectives,,14.0,18.0,,,,,IEEE,IEEE Magazines
204,ADQuaTe: An Automated Data Quality Test Approach for Constraint Discovery and Fault Detection,H. Homayouni; S. Ghosh; I. Ray,Colorado State University; Colorado State University; Colorado State University,2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI),,2019,,,61,68,"Data quality tests validate the data stored in databases and data warehouses to detect violations of syntactic and semantic constraints. Domain experts grapple with the issues related to the capturing of all the important constraints and checking that they are satisfied. Domain experts often define the constraints in an ad hoc manner based on their knowledge of the application domain and needs of the stakeholders. We propose ADQuaTe, which is an automated data quality test approach that uses an unsupervised machine learning technique to discover constraints that may have been missed by experts. ADQuaTe marks records that violate the constraints as suspicious and explains the violations. We evaluate ADQuaTe on real-world applications using a health data warehouse and a plant diagnosis database to demonstrate that the approach can uncover previously detected as well as new faults in the data.",,978-1-7281-1337-1,10.1109/IRI.2019.00023,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843473,Data quality tests;Database;Data warehouse;Explainable learning;Machine learning;Unsupervised learning,Drugs;Data integrity;Databases;Data warehouses;Data models;Semantics;Self-organizing feature maps,data analysis;data mining;data warehouses;database management systems;fault diagnosis;health care;unsupervised learning,automated data quality test approach;constraint discovery;fault detection;data quality tests;syntactic constraints;semantic constraints;domain experts grapple;application domain;ADQuaTe marks records;health data warehouse;plant diagnosis database;unsupervised machine learning technique,,1.0,30.0,,,,,IEEE,IEEE Conferences
205,Toward Next Generation of Autonomous Systems with AI,D. Prokhorov,"Toyota Motor North America R & D, Ann Arbor, MI, 48105, USA",2019 International Joint Conference on Neural Networks (IJCNN),,2019,,,1,5,I discuss a growing area of research in autonomous driving systems and overview what this means in terms of their testing. I also discuss implications for autonomous decision making systems of the future. (This paper is expected to serve as the basis of my plenary talk at IJCNN 2019.).,2161-4407,978-1-7281-1985-4,10.1109/IJCNN.2019.8851867,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851867,autonomous driving;deep neural network;machine learning;decision making system;testing,Testing;Autonomous vehicles;Decision making;Neural networks;Computational modeling;Lyapunov methods;Tools,artificial intelligence;decision making;program testing;traffic engineering computing,autonomous decision making systems;AI;autonomous driving systems,,,38.0,,,,,IEEE,IEEE Conferences
206,Modeling Adaptive Learning Agents for Domain Knowledge Transfer,M. Höser,Bauhaus Luftfahrt e. V.,2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C),,2019,,,660,665,"The implementation of intelligent agents in industrial applications is often prevented by the high cost of adopting such a system to a particular problem domain. This paper states the thesis that when learning agents are applied to work environments that require domain-specific experience, the agent benefits if it can be further adapted by a supervising domain expert. Closely interacting with the agent, a domain expert should be able to understand its decisions and update the underlying knowledge base as needed. The result would be an agent with individualized knowledge that comes in part from the domain experts. The model of such an adaptive learning agent must take into account the problem domain, the design of the learning agent and the perception of the domain user. Therefore, already in the modeling phase, more attention must be paid to make the learning element of the agent adaptable by an operator. Domain modeling and meta-modeling methods could help to make inner processes of the agent more accessible. In addition, the knowledge gained should be made reusable for future agents in similar environments. To begin with, the existing methods for modeling agent systems and the underlying concepts will be evaluated, based on the requirements for different industrial scenarios. The methods are then compiled into a framework that allows for the description and modeling of such systems in terms of adaptability to a problem domain. Where necessary, new methods or tools will be introduced to close the gap between inconsistent modeling artifacts. The framework shall then be used to build learning agents for real-life scenarios and observe their application in a case study. The results will be used to assess the quality of the adapted knowledge base and compare it to a manual knowledge modeling process.",,978-1-7281-5125-0,10.1109/MODELS-C.2019.00101,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8904822,Adaptive Learning Agents;Multi-modeling;Domain modeling;Knowledge Engineering,,knowledge based systems;knowledge management;learning (artificial intelligence);multi-agent systems,adaptive learning agent;domain user;modeling phase;learning element;domain modeling;meta-modeling methods;modeling agent systems;adapted knowledge base;manual knowledge modeling process;modeling adaptive learning agents;domain knowledge transfer;intelligent agents;domain-specific experience;agent benefits;supervising domain expert,,,43.0,,,,,IEEE,IEEE Conferences
209,"Incorporating Societal (Social) and Ethical Implications into the Design, Development, and Deployment of Technologies",B. Schuelke-Leech; M. Janczarski,"University of Windsor,Windsor,Ontario,Canada; Standards Council of Canada,Ottawa,Ontario,Canada",2019 IEEE International Symposium on Technology and Society (ISTAS),,2019,,,1,6,"Technologies can have significant ethical and societal impacts. Unfortunately, it has been common for engineers and developers to overlook these concerns as being beyond the scope of their technical responsibilities. Autonomous and Intelligent Systems are making it more difficult to ignore these issues. However, it can be challenging to know where to begin when trying to incorporate societal and ethical concerns into the design, development, demonstration, and deployment of a technology, process, algorithm, or application. The purpose of this paper is to outline a process that can be used to ensure the consideration of ethical and societal impacts during the development process.",2158-3412,978-1-7281-5480-0,10.1109/ISTAS48451.2019.8937964,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937964,Ethical and Societal Impacts Technology Development,Stakeholders;Biological system modeling;Artificial intelligence;Economics;Ethics;Cultural differences;Standards,ethical aspects;social aspects of automation;technology management,technical responsibilities;societal concerns;ethical concerns;societal impacts;ethical implications;ethical impacts;social implications;societal implications;autonomous systems;intelligent systems;technology development;technology design;technology deployment,,,23.0,,,,,IEEE,IEEE Conferences
210,Development of a Radiology Decision Support System for the Classification of MRI Brain Scans,A. Y. Zhang; S. S. W. Lam; N. Liu; Y. Pang; L. L. Chan; P. H. Tang,"Nat. Univ. of Singapore, Singapore, Singapore; Singapore Health Services, Singapore, Singapore; Singapore Health Services, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore; Med. Sch., Duke-NUS, Singapore, Singapore; Med. Sch., Duke-NUS, Singapore, Singapore",2018 IEEE/ACM 5th International Conference on Big Data Computing Applications and Technologies (BDCAT),,2018,,,107,115,"Previous studies revealed that the ordering of Magnetic resonance imaging (MRI) brain scans following American College of Radiology (ACR) guidelines showed a higher percentage of brain abnormalities compared to scans that do not. As the process of manually labelling patient orders obtained from a local tertiary hospital in accordance to ACR guidelines is intensive and time consuming, this study aims to develop predictive machine learning models; Logistic Regression (LR), Support Vector Machine (SVM), Random Forest (RF) and XGBoost (XGB), to automate the classification process through text mining methods and derive insights that are useful for future clinical decision-making and resource optimization. Using 1,924 observations as the labelled training data, RF and XGB were found to be the best performing robust models with ROC values of 0.9459 and 0.9508 respectively on the validation set (481 observations). Further exploration into the interpretability of black-box algorithms using the model agnostic LIME (Local Interpretable Model-Agnostic Explanations) framework was used to generate further insights for decisions made using a separate XGB model with respect to individual patients. The LIME framework is a significant first step towards the development of a comprehensive decision support system for patient-level decisions in the ordering of MRI scans.",,978-1-5386-5502-3,10.1109/BDCAT.2018.00021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606641,machine learning;text analytics;text mining;natural language processing;MRI;LIME,Magnetic resonance imaging;Radiology;Machine learning;Biomedical imaging;Guidelines;Machine learning algorithms;Brain modeling,biomedical MRI;brain;data mining;decision making;decision support systems;diagnostic radiography;hospitals;image classification;medical image processing;radiology;random forests;regression analysis;support vector machines;text analysis,MRI brain scans;brain abnormalities;local tertiary hospital;ACR guidelines;predictive machine learning models;classification process;text mining methods;model agnostic LIME;patient-level decisions;MRI scans;magnetic resonance imaging brain scans;American College of Radiology;logistic regression;support vector machine;random forest;clinical decision-making;local interpretable model-agnostic explanations;XGB model;radiology guidelines;radiology decision support system;MRI brain scans classification;XGBoost;resource optimization,,,16.0,,,,,IEEE,IEEE Conferences
212,Deep Electric Pole Anomaly Detection and Unsupervised Description Generation,D. Lee; J. Nam; H. Choi,Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology,2020 IEEE International Conference on Big Data and Smart Computing (BigComp),,2020,,,535,537,"In the past few years, image recognition and computer vision with deep learning have evolved rapidly to become a commercial application that assists traditional human decision making with the engineering of data professionals. However, there are a number of problems with existing deep learning. Deep learning is a black box method that suffers from over-fitting problems more seriously than traditional learning methods and cannot be used to understand the decision-making process in the middle layer. It is used skeptically for applications that require reliability and transparency such as safety. The recent generation model gives a user understanding of the data manifold and is emerging as a new unsupervised learning method. In particular, we focus on the unsupervised learning of these generation models and propose experiments that can be used to detect abnormalities in power distribution facilities. We propose techniques to mitigate dataset shortages by learning and analyzing dataset in power environments with extreme shortages of abnormal dataset, and providing an assessment of the abnormal dataset. The module is still experimental and has no results. But in the future, it is expected to be used as a convergence technology of the power industry and artificial intelligence.",2375-9356,978-1-7281-6034-4,10.1109/BigComp48618.2020.00-10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070448,"Machine learning, Electric pole, Generative modeling",Gallium nitride;Generative adversarial networks;Machine learning;Computer architecture;Data models;Computational modeling;Conferences,computer vision;decision making;image recognition;learning (artificial intelligence);unsupervised learning,unsupervised learning method;generation models;dataset;abnormal dataset;deep electric pole anomaly detection;unsupervised description generation;image recognition;computer vision;deep learning;black box method;traditional learning methods;decision-making process;traditional human decision making;power distribution facilities;convergence technology;power industry;artificial intelligence,,,12.0,,,,,IEEE,IEEE Conferences
213,Outlier Analysis of Airport Delay Distributions in US and China,M. Z. Li; K. Gopalakrishnan; Y. Wang; H. Balakrishnan,"Massachusetts Institute of Technology,Department of Aeronautics and Astronautics,Cambridge,MA,USA; Massachusetts Institute of Technology,Department of Aeronautics and Astronautics,Cambridge,MA,USA; College of Civil Aviation, Nanjing University of Aeronautics and Astronautics,Nanjing,China,211106; Massachusetts Institute of Technology,Department of Aeronautics and Astronautics,Cambridge,MA,USA",2020 International Conference on Artificial Intelligence and Data Analytics for Air Transportation (AIDA-AT),,2020,,,1,12,"Outlier detection is a key component of several machine learning approaches. However, many existing techniques, especially for multi-dimensional signals, are not interpretable and do not explain why a specific classification was assigned to a particular data point. Another limitation is that most methods only consider the magnitude or intensity of the signal, and not its spatial distribution. We present a spectral approach to identify outliers based on the spatial distribution of a signal across the nodes of a graph without any explicit assumptions on the underlying probability distribution of the signal. By applying these techniques to airport delays, we not only identify outliers in the spatial distribution of delays, but also gain insights into the delay dynamics. Specifically, we compare spatial delay distributions in the US and China during the period 2012-17, and identify several interesting characteristics pertaining to critical airports for outlier detection. We characterize typical variabilities in the delay distributions, and the frequency of occurrence of outliers. Our results highlight the differences between the operational dynamics of the US and Chinese air transportation systems, and contribute to performance benchmarking between different airspace systems.",,978-1-7281-5380-3,10.1109/AIDA-AT48540.2020.9049208,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9049208,US/China air transportation;flight delays;Graph Signal Processing;outlier analysis;aviation disruption,,airports;data analysis;delays;learning (artificial intelligence);probability;statistical distributions,airport delay distributions;US;outlier detection;machine learning;multidimensional signals;specific classification;particular data point;spatial distribution;probability distribution;delay dynamics;spatial delay distributions;China,,,32.0,,,,,IEEE,IEEE Conferences
215,Interpretable Multiview Early Warning System Adapted to Underrepresented Student Populations,A. Cano; J. D. Leonard,"Department Computer Science, Virginia Commonwealth University, Richmond, VA, USA; Department Computer Science, Virginia Commonwealth University, Richmond, VA, USA",IEEE Transactions on Learning Technologies,,2019,12,2.0,198,211,"Early warning systems have been progressively implemented in higher education institutions to predict student performance. However, they usually fail at effectively integrating the many information sources available at universities to make more accurate and timely predictions, they often lack decision-making reasoning to motivate the reasons behind the predictions, and they are generally biased toward the general student body, ignoring the idiosyncrasies of underrepresented student populations (determined by socio-demographic factors such as race, gender, residency, or status as a freshmen, transfer, adult, or first-generation students) that traditionally have greater difficulties and performance gaps. This paper presents a multiview early warning system built with comprehensible Genetic Programming classification rules adapted to specifically target underrepresented and underperforming student populations. The system integrates many student information repositories using multiview learning to improve the accuracy and timing of the predictions. Three interfaces have been developed to provide personalized and aggregated comprehensible feedback to students, instructors, and staff to facilitate early intervention and student support. Experimental results, validated with statistical analysis, indicate that this multiview learning approach outperforms traditional classifiers. Learning outcomes will help instructors and policy-makers to deploy strategies to increase retention and improve academics.",1939-1382,,10.1109/TLT.2019.2911079,Amazon AWS Machine Learning Research; VCU Presidential Research Quest Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691619,Educational data mining;early prediction;student performance;multi-view learning;genetic programming,Alarm systems;Data mining;Sociology;Statistics;Education;Data models;Genetic programming,data mining;decision making;educational administrative data processing;educational institutions;further education;genetic algorithms;learning (artificial intelligence);pattern classification;statistical analysis,early intervention;student support;multiview learning approach;interpretable multiview early warning system adapted;higher education institutions;student performance;information sources;decision-making reasoning;general student body;socio-demographic factors;first-generation students;underperforming student populations;student information repositories;genetic programming classification rules;statistical analysis,,1.0,80.0,,,,,IEEE,IEEE Journals
216,ADMIRING: Adversarial Multi-network Mining,Q. Zhou; L. Li; N. Cao; L. Ying; H. Tong,"University of Illinois at Urbana-Champaign; Amazon; Tongji University; University of Michigan, Ann Arbor; University of Illinois at Urbana-Champaign",2019 IEEE International Conference on Data Mining (ICDM),,2019,,,1522,1527,"Multi-sourced networks naturally appear in many application domains, ranging from bioinformatics, social networks, neuroscience to management. Although state-of-the-art offers rich models and algorithms to find various patterns when input networks are given, it has largely remained nascent on how vulnerable the mining results are due to the adversarial attacks. In this paper, we address the problem of attacking multi-network mining through the way of deliberately perturbing the networks to alter the mining results. The key idea of the proposed method (Admiring) is effective influence functions on the Sylvester equation defined over the input networks, which plays a central and unifying role in various multi-network mining tasks. The proposed algorithms bear two main advantages, including (1) effectiveness, being able to accurately quantify the rate of change of the mining results in response to attacks; and (2) generality, being applicable to a variety of multi-network mining tasks ( e.g., graph kernel, network alignment, cross-network node similarity) with different attacking strategies (e.g., edge/node removal, attribute alteration).",2374-8486,978-1-7281-4604-1,10.1109/ICDM.2019.00201,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970779,Adversarial attacks;Sylvester Equation;Multi network mining,,bioinformatics;data mining;graph theory;network theory (graphs);social networking (online),ADMIRING;adversarial multinetwork mining;multisourced networks;social networks;state-of-the-art offers rich models;mining results;adversarial attacks;multinetwork mining tasks;network alignment;cross-network node similarity,,,25.0,,,,,IEEE,IEEE Conferences
217,Explainable Recommendation Using Review Text and a Knowledge Graph,T. Suzuki; S. Oyama; M. Kurihara,Hokkaido University; Hokkaido University/RIKEN; Hokkaido University,2019 IEEE International Conference on Big Data (Big Data),,2019,,,4638,4643,"Recommender systems using a knowledge graph can comprehensively organize users and items and their attributes and thereby improve recommendation performance. In addition, the relationship between users and items can be easily interpreted on the basis of entities and relations, thus giving explanations to recommendations. The algorithms and knowledge graphs used for generating explanations have not utilized review text. We have developed a recommendation method for predicting interactions between users and items using a knowledge graph and review text. The underlying user-item relationships are reflected and explanations are generated by predicting user-item interactions from the paths between a user and an item. The modeling is done using a recurrent neural network or a factorization machine. Items' aspects that interest users are extracted from review text and leveraged using an attention-like mechanism. Since the path between a user and an item can be easily interpreted, and the important aspects between a user and an item can be interpreted by observing the attention weight, the proposed model can generate a reasonable recommendation explanation. Testing using a real-world dataset demonstrated that the proposed model can explain the recommendations.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9005590,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005590,Recommendation;Knowledge Graph;Explainability;Review text;Recurent Neural Network,Big Data;Machine-to-machine communications;Conferences,graph theory;recommender systems;recurrent neural nets;text analysis,attention-like mechanism;factorization machine;recurrent neural network;knowledge graphs;recommender systems;user-item interactions;user-item relationships;review text,,,15.0,,,,,IEEE,IEEE Conferences
218,Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision,Y. Liu; A. Jourabloo; X. Liu,"Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,2018,,,389,398,"Face anti-spoofing is crucial to prevent face recognition systems from a security breach. Previous deep learning approaches formulate face anti-spoofing as a binary classification problem. Many of them struggle to grasp adequate spoofing cues and generalize poorly. In this paper, we argue the importance of auxiliary supervision to guide the learning toward discriminative and generalizable cues. A CNN-RNN model is learned to estimate the face depth with pixel-wise supervision, and to estimate rPPG signals with sequence-wise supervision. The estimated depth and rPPG are fused to distinguish live vs. spoof faces. Further, we introduce a new face anti-spoofing database that covers a large range of illumination, subject, and pose variations. Experiments show that our model achieves the state-of-the-art results on both intra- and cross-database testing.",2575-7075,978-1-5386-6420-9,10.1109/CVPR.2018.00048,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578146,,Face;Three-dimensional displays;Databases;Face recognition;Lighting;Shape;Two dimensional displays,face recognition;feedforward amplifiers;feedforward neural nets;image classification;learning (artificial intelligence),learning deep models;auxiliary supervision;face recognition systems;adequate spoofing cues;face depth;pixel-wise supervision;sequence-wise supervision;spoof faces;face antispoofing database;deep learning approaches,,47.0,51.0,,,,,IEEE,IEEE Conferences
219,Feature Learning and Analysis for Cleanliness Classification in Restrooms,L. Jayasinghe; N. Wijerathne; C. Yuen; M. Zhang,"SUTD-MIT International Design Centre, Singapore University of Technology and Design, Singapore; SUTD-MIT International Design Centre, Singapore University of Technology and Design, Singapore; SUTD-MIT International Design Centre, Singapore University of Technology and Design, Singapore; National ASIC System Engineering Technology Research Center, Southeast University, Nanjing, China",IEEE Access,,2019,7,,14871,14882,"In order to revamp the cleaning contract from the head-count basis into a performance basis, a fair and unbiased cleanliness classification is necessary. However, the perception of cleanliness is very subjective to the observer. Hence, it is not an easy task to quantify the cleanliness. This paper presents an application of principal component analysis (PCA) in conjunction with convolutional neural networks (CNN) to identify the cleanliness of a restroom up to three levels; namely, dirty, average, and clean. The proposed method includes an application specific data augmentation algorithm and a PCA-based feature analysis schema to select the best suited CNN model for our dataset. Since this study focused on a specific application, we benchmark the performances of the proposed method performances with the state-of-the-art computer vision algorithms on our dataset. Moreover, our study shows a machine learning approach toward automating the inspection process of a restroom.",2169-3536,,10.1109/ACCESS.2019.2894006,SUTD-MIT International Design Center and NSFC; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620324,Image classification;deep learning;principle component analysis;data augmentation;feature learning;internet of things,Feature extraction;Convolution;Cleaning;Principal component analysis;Standards;Computer architecture;Sensors,cleaning;computer vision;convolutional neural nets;image classification;learning (artificial intelligence);principal component analysis,restroom;feature learning;principal component analysis;convolutional neural networks;data augmentation algorithm;PCA;CNN model;machine learning;feature analysis;cleanliness classification;computer vision,,,35.0,,,,,IEEE,IEEE Journals
220,A Wearable Sleep Position Tracking System Based on Dynamic State Transition Framework,S. Jeon; T. Park; A. Paul; Y. Lee; S. H. Son,"Department of Information and Communication Engineering, DGIST, Daegu, South Korea; Department of Robotics Engineering, Hanyang University, Seoul, South Korea; School of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Department of Physical Medicine and Rehabilitation, School of Medicine, Kyungpook National University, Daegu, South Korea; Department of Information and Communication Engineering, DGIST, Daegu, South Korea",IEEE Access,,2019,7,,135742,135756,"Sleep monitoring is vital as sleep plays an important role in recovering physical and mental health. To have a sound sleep, one has to avoid bad sleep positions associated with personal health conditions. However, most of the existing sleep trackers merely show quantitative information about sleep patterns and duration at each sleep stage, overlooking the importance of sleep positions upon sleep quality. To accurately keep track of sleep positions, we propose a wearable sleep position tracking system consisting of two wristbands and one chest-band. We suggest a two-level classifier specialized for sleep motion based on Dynamic State Transition (DST)-framework. The DST-framework is designed to process the spatio-temporal sleep motion data collected via accelerometer/gyro sensing and classify twelve sleep position (SP) motions from four sleep positions. Our experimental results demonstrate that the proposed system effectively and accurately classify twelve SP motions for tracking sleep positions, and hence, serves as a key building block for comprehensive sleep care applications related to sleep positions.",2169-3536,,10.1109/ACCESS.2019.2942608,"National Research Foundation; ICT Research and Development program of MSIP/IITP, (Resilient Cyber-Physical Systems Research); Ministry of Science, ICT and Future Planning; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845586,Sleep position;sleep quality;sleep monitoring;wearable devices,Sensors;Biomedical monitoring;Sleep apnea;Tracking;Monitoring;Cameras,accelerometers;medical signal processing;patient monitoring;sleep,wearable sleep position tracking system;sleep monitoring;sound sleep;bad sleep positions;existing sleep trackers;sleep patterns;sleep stage;sleep quality;spatio-temporal sleep motion data;sleep position motions;comprehensive sleep care applications;dynamic state transition-framework;accelerometer-gyro sensing;sleep position motions,,,50.0,CCBY,,,,IEEE,IEEE Journals
221,Self-Assessing and Communicating Manipulation Proficiency Through Active Uncertainty Characterization,M. S. Lee,Carnegie Mellon University,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,2019,,,724,726,"Autonomous manipulation has the potential to improve the quality of life of many by assisting in routine household tasks such as cooking, cleaning, and organizing. However, for safe, dependable, and effective operation alongside humans, both the robot and the human must have an accurate and reliable assessment of the robot's proficiency at completing the relevant tasks. Such an assessment helps to ensure that the robot does not engage in tasks that it cannot handle and instead engages in tasks that are well-aligned with the robot's abilities. This proposal thus investigates how a robot can actively assess both its proficiency and its confidence in that assessment through appropriate measures of uncertainty that can be efficiently and effectively communicated to a human. The experiments examine how a user's trust and subsequent use of a robot vary as a result of the robot's self-assessment of proficiency.",2167-2148,978-1-5386-8555-6,10.1109/HRI.2019.8673083,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673083,Trust Alignment;Proficiency;Confidence;Uncertainty;Introspection,Uncertainty;Task analysis;Robot sensing systems;Sensitivity analysis;Proposals;Conferences,domestic appliances;home automation;mobile robots;reliability,reliable assessment;autonomous manipulation;robots self-assessment;household tasks;uncertainty characterization,,,23.0,,,,,IEEE,IEEE Conferences
222,o-glasses: Visualizing X86 Code From Binary Using a 1D-CNN,Y. Otsubo; A. Otsuka; M. Mimura; T. Sakaki,"National Police Agency, Tokyo, Japan; Institute of information Security, Kanagawa, Japan; Institute of information Security, Kanagawa, Japan; Institute for Future Initiatives (IFI), The University of Tokyo, Tokyo, Japan",IEEE Access,,2020,8,,31753,31763,"Malicious document files used in targeted attacks often contain a small program called shellcode. It is often hard to prepare a runnable environment for dynamic analysis of these document files because they exploit specific vulnerabilities. In these cases, it is necessary to identify the position of the shellcode in each document file to analyze it. If the exploit code uses executable scripts such as JavaScript and Flash, it is not so hard to locate the shellcode. On the other hand, it is sometimes almost impossible to locate the shellcode when it does not contain any JavaScript or Flash but consists of native x86 code only. Binary fragment classification is often applied to visualize the location of regions of interest, and shellcode must contain at least a small fragment of x86 native code even if most of it is obfuscated, such as a decoder for the obfuscated body of the shellcode. In this paper, we propose a novel method, o-glasses, to visualize the shellcode by recognizing the x86 native code using a specially designed one-dimensional convolutional neural network (1d-CNN). The fragment size needs to be as small as the minimum size of the x86 native code in the whole shellcode. Our results show that a 16-instruction-sequence (approximately 48 bytes on average) is sufficient for the code fragment visualization. Our method, o-glasses (1d-CNN), outperforms other methods in that it recognizes x86 native code with a surprisingly high F-measure rate (about 99.95%).",2169-3536,,10.1109/ACCESS.2020.2972358,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8986651,Binary analysis;CNN;machine learning;MLP;shellcode;visualization,,document handling;program diagnostics;program visualisation;security of data,1D-CNN;malicious document files;shellcode;binary fragment classification;code fragment visualization;x86 native code visualization;o-glasses method;dynamic analysis;one-dimensional convolutional neural networks,,,30.0,CCBY,,,,IEEE,IEEE Journals
223,Some Research Problems in Biometrics: The Future Beckons,A. Ross; S. Banerjee; C. Chen; A. Chowdhury; V. Mirjalili; R. Sharma; T. Swearingen; S. Yadav,"Michigan State University,East Lansing,MI,USA,48824; Michigan State University,East Lansing,MI,USA,48824; Michigan State University,East Lansing,MI,USA,48824; Michigan State University,East Lansing,MI,USA,48824; Michigan State University,East Lansing,MI,USA,48824; Michigan State University,East Lansing,MI,USA,48824; Michigan State University,East Lansing,MI,USA,48824; Michigan State University,East Lansing,MI,USA,48824",2019 International Conference on Biometrics (ICB),,2019,,,1,8,"The need for reliably determining the identity of a person is critical in a number of different domains ranging from personal smartphones to border security; from autonomous vehicles to e-voting; from tracking child vaccinations to preventing human trafficking; from crime scene investigation to personalization of customer service. Biometrics, which entails the use of biological attributes such as face, fingerprints and voice for recognizing a person, is being increasingly used in several such applications. While biometric technology has made rapid strides over the past decade, there are several fundamental issues that are yet to be satisfactorily resolved. In this article, we will discuss some of these issues and enumerate some of the exciting challenges in this field.",2376-4201,978-1-7281-3640-0,10.1109/ICB45273.2019.8987307,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987307,,,biometrics (access control);data acquisition;data privacy;feature extraction,biometrics;biological attributes;biometric recognition;semiautomated recognition;biometric data acquisition;quality enhancement;feature extraction;matching;data privacy;pattern recognition,,,32.0,,,,,IEEE,IEEE Conferences
224,Generating Mock Skeletons for Lightweight Web-Service Testing,T. Bhagya; J. Dietrich; H. Guesgen,"Massey University, New Zealand; Victoria University of Wellington, New Zealand; Massey University, New Zealand",2019 26th Asia-Pacific Software Engineering Conference (APSEC),,2019,,,181,188,"Modern application development allows applications to be composed using lightweight HTTP services. Testing such an application requires the availability of services that the application makes requests to. However, access to dependent services during testing may be restrained. Simulating the behaviour of such services is, therefore, useful to address their absence and move on application testing. This paper examines the appropriateness of Symbolic Machine Learning algorithms to automatically synthesise HTTP services' mock skeletons from network traffic recordings. These skeletons can then be customised to create mocks that can generate service responses suitable for testing. The mock skeletons have human-readable logic for key aspects of service responses, such as headers and status codes, and are highly accurate.",2640-0715,978-1-7281-4648-5,10.1109/APSEC48747.2019.00033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945647,"HTTP, Web services, REST, service oriented computing, mocking, service virtualisation, application testing, symbolic machine learning",,learning (artificial intelligence);program testing;transport protocols;Web services,modern application development;lightweight HTTP services;application testing;service responses;symbolic machine learning algorithms;lightweight Web-service testing;service availability;HTTP services mock skeletons;network traffic recordings;human-readable logic,,,44.0,,,,,IEEE,IEEE Conferences
225,Arguing Machines: Human Supervision of Black Box AI Systems That Make Life-Critical Decisions,L. Fridman; L. Ding; B. Jenik; B. Reimer,Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT),2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,2019,,,1335,1343,"We consider the paradigm of a black box AI system that makes life-critical decisions. We propose an “arguing machines” framework that pairs the primary AI system with a secondary one that is independently trained to perform the same task. We show that disagreement between the two systems, without any knowledge of underlying system design or operation, is sufficient to improve the accuracy of the overall system given human supervision over disagreements. We demonstrate this system in two applications: (1) image classification and (2) large-scale real-world semi-autonomous driving. For the first application, we apply this framework to image classification achieving a reduction from 8.0% to 2.8% top-5 error on ImageNet. For the second application, we apply this framework to Tesla Autopilot and demonstrate the ability to predict 90.4% of system disengagements that were labeled by human as challenging.",2160-7516,978-1-7281-2506-0,10.1109/CVPRW.2019.00173,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025671,,Task analysis;Machine learning;Neural networks;Autonomous vehicles;Visualization,automobiles;control engineering computing;distributed control;image classification;learning (artificial intelligence);mobile robots;telerobotics,system disengagements;human supervision;black box AI system;life-critical decisions;arguing machines framework;primary AI system;system design;image classification;autonomous driving;ImageNet;Tesla Autopilot,,,36.0,,,,,IEEE,IEEE Conferences
226,Teaching Clustering Algorithms With EduClust: Experience Report and Future Directions,J. Fuchs; P. Isenberg; A. Bezerianos; M. Miller; D. A. Keim,"University of Konstanz; Université Paris-Saclay, CNRS, Inria, LRI; Université Paris-Saclay, CNRS, Inria, LRI; University of Konstanz; University of Konstanz",IEEE Computer Graphics and Applications,,2020,40,2.0,98,102,"We share our experiences teaching university students about clustering algorithms using EduClust, an online visualization we developed. EduClust supports professors in preparing teaching material and students in visually and interactively exploring cluster steps and the effects of changing clustering parameters. We used EduClust for two years in our computer science lectures on clustering algorithms and share our experience integrating the online application in a data science curriculum. We also point to opportunities for future development.",1558-1756,,10.1109/MCG.2020.2970560,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020216,,Clustering algorithms;Education;Animation;Software algorithms;Cognition;Data visualization;Software tools,computer aided instruction;computer science education;data visualisation;educational institutions;pattern clustering;teaching,EduClust;teaching clustering algorithms;online visualization;teaching material;teaching University students;computer science lectures;data science curriculum,,,4.0,IEEE,,,,IEEE,IEEE Magazines
227,Time Series Classification by Sequence Learning in All-Subsequence Space,T. L. Nguyen; S. Gsponer; G. Ifrim,"Insight Centre for Data Analytics, Univ. Coll. Dublin, Dublin, Ireland; Insight Centre for Data Analytics, Univ. Coll. Dublin, Dublin, Ireland; Insight Centre for Data Analytics, Univ. Coll. Dublin, Dublin, Ireland",2017 IEEE 33rd International Conference on Data Engineering (ICDE),,2017,,,947,958,"Existing approaches to time series classification can be grouped into shape-based (numeric) and structure-based (symbolic). Shape-based techniques use the raw numeric time series with Euclidean or Dynamic Time Warping distance and a 1-Nearest Neighbor classifier. They are accurate, but computationally intensive. Structure-based methods discretize the raw data into symbolic representations, then extract features for classifiers. Recent symbolic methods have outperformed numeric ones regarding both accuracy and efficiency. Most approaches employ a bag-of-symbolic-words representation, but typically the word-length is fixed across all time series, an issue identified as a major weakness in the literature. Also, there are no prior attempts to use efficient sequence learning techniques to go beyond single words, to features based on variable-length sequences of words or symbols. We study an efficient linear classification approach, SEQL, originally designed for classification of symbolic sequences. SEQL learns discriminative subsequences from training data by exploiting the all-subsequence space using greedy gradient descent. We explore different discretization approaches, from none at all to increasing smoothing of the original data, and study the effect of these transformations on the accuracy of SEQL classifiers. We propose two adaptations of SEQL for time series data, SAX-VSEQL, can deal with X-axis offsets by learning variable-length symbolic words, and SAX-VFSEQL, can deal with X-axis and Y-axis offsets, by learning fuzzy variable-length symbolic words. Our models are linear classifiers in rich feature spaces. Their predictions are based on the most discriminative subsequences learned during training, and can be investigated for interpreting the classification decision.",2375-026X,978-1-5090-6543-1,10.1109/ICDE.2017.142,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930038,,Time series analysis;Training;Optimization;Benchmark testing;Measurement;Data analysis,feature extraction;learning (artificial intelligence);pattern classification;time series,time series classification;sequence learning;all-subsequence space;raw numeric time series;Euclidean time warping distance;dynamic time warping distance;1-nearest neighbor classifier;feature extraction;bag-of-symbolic-word representation;linear classification approach;symbolic sequence classification;greedy gradient descent;SEQL classifiers;SAX-VSEQL;SAX-VFSEQL;fuzzy variable-length symbolic words,,5.0,28.0,,,,,IEEE,IEEE Conferences
228,Do End-Users Want Explanations? Analyzing the Role of Explainability as an Emerging Aspect of Non-Functional Requirements,L. Chazette; O. Karras; K. Schneider,Leibniz Universität Hannover; Leibniz Universität Hannover; Leibniz Universität Hannover,2019 IEEE 27th International Requirements Engineering Conference (RE),,2019,,,223,233,"Software systems are getting more and more complex. Their ubiquitous presence makes users more dependent on them and their correctness in many aspects of daily life. Thus, there is a rising need to make software systems and their decisions more comprehensible. This seems to call for more transparency in software-supported decisions. Therefore, transparency is gaining importance as a non-functional requirement. However, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it. Integrating explanations in software to leverage systems' opacity has been discussed often. Yet, an important first step is to understand user requirements with respect to explainable software behavior: Are users really interested in transparency, and are explanations considered an adequate mechanism to achieve it? We conducted a survey with 107 end-users to assess their opinion on the current status of transparency in software systems, and what they consider main advantages and disadvantages of explanations embedded in software. The overall attitude towards embedded explanations was positive. However, we also identified potential disadvantages. We assess the relation between explanations and transparency and analyze its possible impact on software quality.",2332-6441,978-1-7281-3912-8,10.1109/RE.2019.00032,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920547,Software Transparency;Explanations;Non-Functional Requirements;Qualitative Evaluation;Interpretability,Software systems;Decision making;Stakeholders;Measurement;Software quality;Privacy,software quality,emerging aspect;nonfunctional requirement;software systems;software-supported decisions;abstract quality aspect;transparency needs;leverage systems;user requirements;explainable software behavior;end-users;embedded explanations;software quality,,,42.0,,,,,IEEE,IEEE Conferences
229,BEEF: Balanced English Explanations of Forecasts,S. Grover; C. Pulice; G. I. Simari; V. S. Subrahmanian,"Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computer Science, Dartmouth College, Hanover, NH, USA; Department of Computer Science and Engineering, Institute for Computer Science and Engineering (CONICET), Universidad Nacional del Sur (UNS), Bahía Blanca, Argentina; Department of Computer Science, Dartmouth College, Hanover, NH, USA",IEEE Transactions on Computational Social Systems,,2019,6,2.0,350,364,"The problem of understanding the reasons behind why different machine learning classifiers make specific predictions is a difficult one, mainly because the inner workings of the algorithms underlying such tools are not amenable to the direct extraction of succinct explanations. In this paper, we address the problem of automatically extracting balanced explanations from predictions generated by any classifier, which include not only why the prediction might be correct but also why it could be wrong. Our framework, called Balanced English Explanations of Forecasts, can generate such explanations in natural language. After showing that the problem of generating explanations is NP-complete, we focus on the development of a heuristic algorithm, empirically showing that it produces high-quality results both in terms of objective measures-with statistically significant effects shown for several parameter variations-and subjective evaluations based on a survey completed by 100 anonymous participants recruited via Amazon Mechanical Turk.",2329-924X,,10.1109/TCSS.2019.2902490,Office of Naval Research; Universidad Nacional del Sur; H2020 Marie Skłodowska-Curie Actions; Agencia Nacional de Promoción Científica y Tecnológica; Consejo Nacional de Investigaciones Científicas y Técnicas; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668423,Decision support systems;knowledge engineering;machine learning,Natural languages;Computational geometry;Data models;Predictive models;Computer science;Machine learning;Prediction algorithms,computational complexity;learning (artificial intelligence);pattern classification,NP-complete problem;Amazon Mechanical Turk;machine learning classifiers;balanced english explanations;heuristic algorithm;natural language;classifier;balanced explanations;succinct explanations;direct extraction;inner workings;BEEF,,1.0,29.0,,,,,IEEE,IEEE Journals
230,A Performance Driven Micro Services-Based Architecture/System for Analyzing Noisy IoT Data,M. Vrbaski; M. Bolic; S. Majumdar,"School of Electrical Engineering and Computer Science, University of Ottawa, 800 King Edward Avenue, Ottawa, Canada, KIN 6N5; School of Electrical Engineering and Computer Science, University of Ottawa, 800 King Edward Avenue, Ottawa, Canada, KIN 6N5; Systems and Computers Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, Canada, KIS 5B6","2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",,2019,,,173,177,"The Internet of Things (IoT) technology presents a complex and challenging paradigm where a huge amount of noisy raw sensor data is collected in order to observe and detect critical events occurring on the system, and generate alarms when required. The biggest challenge of the IoT systems is that the systems collect a massive amount of uncertain data from diverse IoT devices connected through the network. In addition, some events are inferred from other events and uncertainty is propagated from parent events to the inferred events, which additionally contributes to overall system uncertainty. The observed complex events are a complex relationship of primitive events that are produced by IoT devices and collected in IoT systems. A survey performed on existing prior arts on quantifying uncertainty for complex events concluded that proposed existing solutions are unable to scale under heavy loads of incoming data. This paper presents a micro-service based notification methodology that uses complex event recognition (both complex event processing and probabilistic programming) to handle IoT systems uncertainty. In addition, the paper analyzes and recommends existing big data platforms for processing complex events in IoT systems. The current focus of our work includes research and development of the optimized deadline-based and cost-effective resource allocation algorithm in Apache Spark for Uncertain IoT Notification systems.",,978-1-7281-0912-1,10.1109/CCGRID.2019.00031,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752834,"IoT, complex events, probabilistic programing, AI, big data, deadline based, Spark, micro services",,Big Data;data analysis;Internet of Things;resource allocation;software architecture,performance driven microservices-based architecture/system;noisy raw sensor data;critical events;uncertain data;diverse IoT devices;microservice based notification methodology;complex event recognition;complex event processing;big data platforms;Internet of Things technology;noisy IoT data analysis;uncertain IoT notification systems,,,13.0,,,,,IEEE,IEEE Conferences
231,NeuroMask: Explaining Predictions of Deep Neural Networks through Mask Learning,M. Alzantot; A. Widdicombe; S. Julier; M. Srivastava,UCLA; UCL; UCL; UCLA,2019 IEEE International Conference on Smart Computing (SMARTCOMP),,2019,,,81,86,"Deep Neural Networks (DNNs) deliver state-of-the-art performance in many image recognition and understanding applications. However, despite their outstanding performance, these models are black-boxes and it is hard to understand how they make their decisions. Over the past few years, researchers have studied the problem of providing explanations of why DNNs predicted their results. However, existing techniques are either obtrusive, requiring changes in model training, or suffer from low output quality. In this paper, we present a novel method, NeuroMask, for generating an interpretable explanation of classification model results. When applied to image classification models, NeuroMask identifies the image parts that are most important to classifier results by applying a mask that hides/reveals different parts of the image, before feeding it back into the model. The mask values are tuned by minimizing a properly designed cost function that preserves the classification result and encourages producing an interpretable mask. Experiments using state-of-art Convolutional Neural Networks for image recognition on different datasets (CIFAR-10 and ImageNet) show that NeuroMask successfully localizes the parts of the input image which are most relevant to the DNN decision. By showing a visual quality comparison between NeuroMask explanations and those of other methods, we find NeuroMask to be both accurate and interpretable.",,978-1-7281-1689-1,10.1109/SMARTCOMP.2019.00033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784063,neural networks;deep learning;image recognition;interpretability,Computational modeling;Predictive models;Training;Cost function;Neural networks;Computer science;Image recognition,convolutional neural nets;image classification;learning (artificial intelligence),mask learning;image recognition;black-boxes;model training;low output quality;interpretable explanation;image classification models;image parts;mask values;interpretable mask;input image;NeuroMask explanations;deep neural networks;cost function;convolutional neural networks;DNN decision,,,22.0,,,,,IEEE,IEEE Conferences
232,Visualization of Histopathological Decision Making Using a Roadbook Metaphor,B. Pohn; M. Mayer; R. Reihs; A. Holzinger; K. Zatloukal; H. Müller,"Medical University Graz, Austria; Medical University Graz, Austria; Medical University Graz, Austria; Medical University Graz, Austria; Medical University Graz, Austria; Medical University Graz, Austria",2019 23rd International Conference Information Visualisation (IV),,2019,,,392,397,"Since pathology is supported by information technology new opportunities and questions have arisen. The digital age enables analyzing histopathological data with artificial intelligence methods to reveal further information and correlations. In this paper existing approaches to visualization of medical decision processes are presented as well as the relevance of explainability in decision making. The first step for implementing decision-paths in systems is to retrace an experienced pathologist's diagnosis finding process. Recording a route through a landscape composed of human tissue in terms of a roadbook is one possible approach to collect information on how diagnoses are found. Choosing the roadbook metaphor provides a simple schema, that holds basic directions enriched with metadata regarding landmarks on a rally - in the context of pathology such landmarks provide information on the decision finding process.",2375-0138,978-1-7281-2838-2,10.1109/IV.2019.00073,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812126,explainability;artificial-intelligence;medical-decision-vizualisation;digital-pathology-roadbook,Medical diagnostic imaging;Pathology;Data visualization;Visualization;Decision making;Microscopy;Portals,artificial intelligence;decision making;diseases;medical information systems;meta data,roadbook metaphor;information technology new opportunities;digital age;histopathological data;artificial intelligence methods;medical decision processes;decision making;decision-paths;experienced pathologist;human tissue;decision finding process;histopathological decision making,,,32.0,,,,,IEEE,IEEE Conferences
234,Explaining robot actions,M. Lomas; R. Chevalier; E. V. Cross; R. C. Garrett; J. Hoare; M. Kopack,"Lockheed Martin Advanced Technology Laboratories, 3 Executive Campus, Suite 600, Cherry Hill, NJ 08002; Lockheed Martin Advanced Technology Laboratories, 3 Executive Campus, Suite 600, Cherry Hill, NJ 08002; Lockheed Martin Advanced Technology Laboratories, 3 Executive Campus, Suite 600, Cherry Hill, NJ 08002; Lockheed Martin Advanced Technology Laboratories, 3 Executive Campus, Suite 600, Cherry Hill, NJ 08002; Lockheed Martin Advanced Technology Laboratories, 3 Executive Campus, Suite 600, Cherry Hill, NJ 08002; Lockheed Martin Advanced Technology Laboratories, 3 Executive Campus, Suite 600, Cherry Hill, NJ 08002",2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,2012,,,187,188,"To increase human trust in robots, we have developed a system that provides insight into robotic behaviors by enabling a robot to answer questions people pose about its actions (e.g., Q: “Why did you turn left there?” A: “I detected a person at the end of the hallway.”). Our focus is on generation of this explanation in human-understandable terms despite the mathematical, robot-specific representation and planning system used by the robot to make its decisions and execute its actions. We present our work to date on this topic, including system design and experiments, and discuss areas for future work.",2167-2148,978-1-4503-1063-5,10.1145/2157689.2157748,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249519,Robotic Actions;Natural Communications;Trust;Human-Robot Partnering;Explanations,Robot kinematics;Humans;Planning;Semantics;Artificial intelligence;Mathematical model,behavioural sciences;decision making;human-robot interaction;object detection;path planning,human trust;robot behavior;person detection;robot specific representation;robot planning system;decision making;robot action,,3.0,6.0,,,,,IEEE,IEEE Conferences
235,Pollux: Interactive Cluster-First Projections of High-Dimensional Data,J. Wenskovitch; C. North,Virginia Tech Discovery Analytics Center; Virginia Tech Discovery Analytics Center,2019 IEEE Visualization in Data Science (VDS),,2019,,,38,47,"Semantic interaction is a technique relying upon the interactive semantic exploration of data. When an analyst manipulates data items within a visualization, an underlying model learns from the intent underlying these interactions, updating the parameters of the model controlling the visualization. In this work, we propose, implement, and evaluate a model which defines clusters within this data projection, then projects these clusters into a two-dimensional space using a “proximity≈similarity” metaphor. These clusters act as targets against which data values can be manipulated, providing explicit user-driven cluster membership assignments to train the underlying models. Using this cluster-first approach can improve the speed and efficiency of laying out a projection of high-dimensional data, with the tradeoff of distorting the global projection space.",,978-1-7281-5047-5,10.1109/VDS48975.2019.8973381,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8973381,Dimension reduction;clustering;semantic interaction;exploratory data analysis;Human-centered computing;Visualization;Visual analytics,,data analysis;pattern clustering,data items;data projection;two-dimensional space;data values;explicit user-driven cluster membership assignments;cluster-first approach;high-dimensional data;global projection space;interactive cluster;semantic interaction;proximity-similarity metaphor;Pollux,,,56.0,,,,,IEEE,IEEE Conferences
236,Augmented Intelligence with Ontology of Semantic Objects,A. C. M. Fong; G. Hong; B. Fong,"Western Michigan University,Department of Computer Science,Kalamazoo,MI,USA,49008; Western Michigan University,Department of Computer Science,Kalamazoo,MI,USA,49008; Auckland University of Technology,Auckland,New Zealand",2019 International Conference on contemporary Computing and Informatics (IC3I),,2019,,,1,4,"The most recent AI winter began to thaw several years ago with the advent of deep learning neural networks. Headline grabbing successes propel renewed interests and investments in AI research. Rather than trying to imitate human intelligence and behaviors or excelling in a narrowly defined task, researchers are aiming for general AI. In this paper, we take a different approach, one that aims to multiply human cognitive power through augmented intelligence. This is achieved through ontological reasoning using semantic objects as the atomic level of knowledge representation of real-world concepts and phenomena. Through preliminary experiments, we validate the applicability and efficacy of our approach.",,978-1-7281-5529-6,10.1109/IC3I46837.2019.9055577,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055577,intelligent computing;artificial intelligence;augmented intelligence;knowledge engineering;ontological reasoning;semantic objects,,cognitive systems;knowledge representation;learning (artificial intelligence);neural nets;ontologies (artificial intelligence),augmented intelligence;AI winter;deep learning neural networks;AI research;human intelligence;human cognitive power;ontological reasoning;semantic object ontology;knowledge representation,,,22.0,,,,,IEEE,IEEE Conferences
237,Prediction of Hemorrhagic Transformation Severity in Acute Stroke From Source Perfusion MRI,Y. Yu; D. Guo; M. Lou; D. Liebeskind; F. Scalzo,"Department of NeurologyUniversity of California; Department of NeurologyUniversity of California; Department of NeurologySecond Affiliated Hospital, Zhejiang University; Department of NeurologyUniversity of California; Department of Neurology, University of California, Los Angeles, CA, USA",IEEE Transactions on Biomedical Engineering,,2018,65,9.0,2058,2065,"Objective: Hemorrhagic transformation (HT) is the most severe complication of reperfusion therapy in acute ischemic stroke (AIS) patients. Management of AIS patients could benefit from accurate prediction of upcoming HT. While prediction of HT occurrence has recently provided encouraging results, the prediction of the severity and territory of the HT could bring valuable insights that are beyond current methods. Methods: This study tackles these issues and aims to predict the spatial occurrence of HT in AIS from perfusion-weighted magnetic resonance imaging (PWI) combined with diffusion weighted imaging. In all, 165 patients were included in this study and analyzed retrospectively from a cohort of AIS patients treated with reperfusion therapy in a single stroke center. Results: Machine learning models are compared within our framework; support vector machines, linear regression, decision trees, neural networks, and kernel spectral regression were applied to the dataset. Kernel spectral regression performed best with an accuracy of $\text{83.7} \pm \text{2.6}\%$. Conclusion: The key contribution of our framework formalize HT prediction as a machine learning problem. Specifically, the model learns to extract imaging markers of HT directly from source PWI images rather than from pre-established metrics. Significance: Predictions visualized in terms of spatial likelihood of HT in various territories of the brain were evaluated against follow-up gradient recalled echo and provide novel insights for neurointerventionalists prior to endovascular therapy.",1558-2531,,10.1109/TBME.2017.2783241,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8231213,Endovascular therapy;hemorrhagic transformation;machine learning;magnetic resonance imaging;stroke,Medical treatment;Artificial intelligence;Decision trees;Hemorrhaging;Support vector machines,biomedical MRI;brain;decision trees;diseases;learning (artificial intelligence);medical image processing;patient treatment;regression analysis;support vector machines,support vector machines;linear regression;decision trees;neural networks;spatial likelihood;endovascular therapy;source PWI images;machine learning problem;framework formalize HT prediction;kernel spectral regression;Machine learning models;single stroke center;diffusion weighted imaging;perfusion-weighted magnetic resonance imaging;HT occurrence;AIS patients;acute ischemic stroke patients;reperfusion therapy;Hemorrhagic transformation severity,,,38.0,,,,,IEEE,IEEE Journals
238,Towards Self-Explainable Cyber-Physical Systems,M. Blumreiter; J. Greenyer; F. J. Chiyah Garcia; V. Klös; M. Schwammberger; C. Sommer; A. Vogelsang; A. Wortmann,Hamburg University of Technology; Leibniz Universität Hannover; Heriot-Watt University; Technische Universität Berlin; University of Oldenburg; Paderborn University; Technische Universität Berlin; RWTH Aachen University,2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C),,2019,,,543,548,"With the increasing complexity of Cyber-Physical Systems, their behavior and decisions become increasingly difficult to understand and comprehend for users and other stakeholders. Our vision is to build self-explainable systems that can, at run-time, answer questions about the system's past, current, and future behavior. As hitherto no design methodology or reference framework exists for building such systems, we propose the Monitor, Analyze, Build, Explain (MAB-EX) framework for building self-explainable systems that leverage requirements-and explainability models at run-time. The basic idea of MAB-EX is to first Monitor and Analyze a certain behavior of a system, then Build an explanation from explanation models and convey this EXplanation in a suitable way to a stakeholder. We also take into account that new explanations can be learned, by updating the explanation models, should new and yet un-explainable behavior be detected by the system.",,978-1-7281-5125-0,10.1109/MODELS-C.2019.00084,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8904796,Explainability;self-adaptive systems;cyber physical systems,,cyber-physical systems;software engineering,stakeholder;design methodology;MAB-EX;explainability models;un-explainable behavior;self-explainable cyber-physical systems,,,18.0,,,,,IEEE,IEEE Conferences
240,"LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization",A. Walch; M. Schwärzler; C. Luksch; E. Eisemann; T. Gschwandtner,VRVis Forschungs GmbH; TU Delft; VRVis Forschungs GmbH; TU Delft; Visual Analytics research divisionTU Wien,IEEE Transactions on Visualization and Computer Graphics,,2020,26,1.0,569,578,"LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that can deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree. This provenance tree integrates snapshot visualizations of how well a design meets the given quality criteria weighted by the designer's preferences. This integration facilitates the analysis of quality improvements over the course of a modeling workflow as well as the comparison of alternative design solutions. We evaluate our approach with three lighting designers to illustrate its usefulness.",1941-0506,,10.1109/TVCG.2019.2934658,BMVIT; BMDW; Styria; SFG; Vienna Business Agency; COMET; VIDI NextView; NWO Vernieuwingsimpuls; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807288,guidance;3D modeling;lighting design;provenance;global illumination,Lighting;Visualization;Solid modeling;Data visualization;Three-dimensional displays;History;Computational modeling,CAD;interactive systems;lighting;solid modelling,LightGuider;interactive lighting design;quality visualization;guidance-based approach;interleaved 3D modeling operations;light transport simulations;lighting-design improvements;specialized provenance tree;quality improvements;workflow modeling,,,48.0,,,,,IEEE,IEEE Journals
241,How Do Engineers Perceive Difficulties in Engineering of Machine-Learning Systems? - Questionnaire Survey,F. Ishikawa; N. Yoshioka,National Institute of Informatics; National Institute of Informatics,2019 IEEE/ACM Joint 7th International Workshop on Conducting Empirical Studies in Industry (CESI) and 6th International Workshop on Software Engineering Research and Industrial Practice (SER&IP),,2019,,,2,9,"There is increasing interest in machine learning (ML) techniques and their applications in recent years. Although there has been intensive support by frameworks and libraries for the implementation of ML-based systems, investigation into engineering disciplines and methods is still at the early phase. The most pressing issue in this field is identifying the essential challenges for the software engineering research community as engineering of ML-based systems requires novel approaches due to the essentially different nature of ML-based systems. In this paper, we analyze the results of a questionnaire administered to 278 people who have worked on ML-based systems in practice, clarify the essential difficulties and their causes as perceived by practitioners, and suggest potential research directions.",2575-4793,978-1-7281-2264-9,10.1109/CESSER-IP.2019.00009,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8836142,"software engineering, machine learning, artificial intelligence, questionnaire survey",Testing;Software;Software engineering;Deep learning;Conferences,learning (artificial intelligence);software engineering,questionnaire survey;machine-learning systems engineering;ML-based systems engineering;machine learning techniques;software engineering research community,,2.0,16.0,,,,,IEEE,IEEE Conferences
243,"Building a Machine Learning Model for the SOC, by the Input from the SOC, and Analyzing it for the SOC",A. Sopan; M. Berninger; M. Mulakaluri; R. Katakam,"FireEye, Inc.; FireEye, Inc.; FireEye, Inc.; FireEye, Inc.",2018 IEEE Symposium on Visualization for Cyber Security (VizSec),,2018,,,1,8,"This work demonstrates an ongoing effort to employ and explain machine learning model predictions for classifying alerts in Security Operations Centers (SOC). Our ultimate goal is to reduce analyst workload by automating the process of decision making for investigating alerts using the machine learning model in cases where we can completely trust the model. This way, SOC analysts will be able to focus their time and effort to investigate more complex cases of security alerts. To achieve this goal, we developed a system that shows the prediction for an alert and the prediction explanation to security analysts during their daily workflow of investigating individual security alerts. Another part of our system presents the aggregated model analytics to the managers and stakeholders to help them understand the model and decide, on when to trust the model and let the model make the final decision. Using our prediction explanation visualization, security analysts will be able to classify oncoming alerts more efficiently and gain insight into how a machine learning model generates predictions. Our model performance analysis dashboard helps decision makers analyze the model in signature level granularity and gain more insights about the model.",2639-4332,978-1-5386-8194-7,10.1109/VIZSEC.2018.8709231,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709231,Cyber security;Machine Learning;Information Visualization;Security Operations Center,Analytical models;Predictive models;Machine learning;Data models;Security;Data visualization;Computational modeling,data visualisation;decision making;learning (artificial intelligence);security of data,security analysts;individual security alerts;aggregated model analytics;prediction explanation visualization;machine learning model;model performance analysis dashboard;SOC analysts;security operations centers;signature level granularity,,,20.0,,,,,IEEE,IEEE Conferences
244,"Digital Twin: Values, Challenges and Enablers From a Modeling Perspective",A. Rasheed; O. San; T. Kvamsdal,"Department of Engineering Cybernetics, Norwegian University of Science and Technology, Trondheim, Norway; Mechanical and Aerospace Engineering, Oklahoma State University, Stillwater, OK, USA; Department of Mathematics and Cybernetics, SINTEF Digital, Trondheim, Norway",IEEE Access,,2020,8,,21980,22012,"Digital twin can be defined as a virtual representation of a physical asset enabled through data and simulators for real-time prediction, optimization, monitoring, controlling, and improved decision making. Recent advances in computational pipelines, multiphysics solvers, artificial intelligence, big data cybernetics, data processing and management tools bring the promise of digital twins and their impact on society closer to reality. Digital twinning is now an important and emerging trend in many applications. Also referred to as a computational megamodel, device shadow, mirrored system, avatar or a synchronized virtual prototype, there can be no doubt that a digital twin plays a transformative role not only in how we design and operate cyber-physical intelligent systems, but also in how we advance the modularity of multi-disciplinary systems to tackle fundamental barriers not addressed by the current, evolutionary modeling practices. In this work, we review the recent status of methodologies and techniques related to the construction of digital twins mostly from a modeling perspective. Our aim is to provide a detailed coverage of the current challenges and enabling technologies along with recommendations and reflections for various stakeholders.",2169-3536,,10.1109/ACCESS.2020.2970143,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972429,Digital twin;artificial intelligence;machine learning;big data cybernetics;hybrid analysis and modeling,Digital twin;Real-time systems;Monitoring;Solid modeling;Biological system modeling;Big Data;Buildings,Big Data;cyber-physical systems;virtual reality,Big Data cybernetics;virtual representation;cyber-physical intelligent systems;digital twinning;modeling perspective;digital twin,,1.0,550.0,CCBY,,,,IEEE,IEEE Journals
246,Reverse Engineering of Content Delivery Algorithms for Social Media Systems,I. Rassameeroj; S. F. Wu,"University of California Davis,Department of Computer Science,Davis,CA,USA; University of California Davis,Department of Computer Science,Davis,CA,USA","2019 Sixth International Conference on Social Networks Analysis, Management and Security (SNAMS)",,2019,,,196,203,Social media systems have become a primary platform to consume and exchange information nowadays. As a black box social algorithms were designed and trained to pick up filter and rank the most relevant and desired content to be delivered to each individual one of us. However these modern social algorithms were typically complicated and its development involved easily more than one hundred software engineers. Therefore we including the social media providers do not really know how these social algorithms work such that we are unsure about the quality of the delivered content or the existence of any bias or disparity? In this paper we explore content delivery algorithms on Facebook fan pages and communities. We empirically defined four hypotheses which originally were from Facebook features social network concepts and what we have observed from our SINCERE data set. We took a heuristic approach to statistically analyze our data set to uncover probabilistically how Facebook push user-created contents among users. Each hypothesis is represented by an explainable logical expression rule. We used each of the top 100 posts with the largest number of comments in ABC News CNN and Fox News fan pages. Our main contribution is to validate each hypothesis against empirical data such that together we provided a partial explanation for the content delivery algorithm used by Facebook.,,978-1-7281-2946-4,10.1109/SNAMS.2019.8931859,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931859,content delivery algorithms;social algorithms;news media;reverse engineering;Facebook fan pages,,probability;reverse engineering;social networking (online);statistical analysis,reverse engineering;content delivery algorithm;social media systems;black box social algorithms;Facebook;user-created contents;information exchange;software engineers;Facebook fan pages;Facebook communities;statistical analysis;probability;logical expression rule,,,14.0,,,,,IEEE,IEEE Conferences
247,Computational Intelligence in Maritime Security and Defense: Challenges and Opportunities,M. Cococcioni,"Department of Information Engineering, Largo Lucio Lazzarino, 1, University of Pisa, Pisa, 56122, Italy",2018 IEEE Symposium Series on Computational Intelligence (SSCI),,2018,,,1964,1967,"Computational Intelligence (CI) has a great potential in Security & Defense (S&D) applications. Nevertheless, such potential seems to be still under exploited. In this work we first review CI applications in the maritime domain, done in the past decades by NATO Nations. Then we discuss challenges and opportunities for CI in S&D. Finally we argue that a review of the academic training of military officers is highly recommendable, in order to allow them to understand, model and solve new problems, using CI techniques.",,978-1-5386-9276-9,10.1109/SSCI.2018.8628770,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8628770,,Artificial intelligence;Training;Security;Computational intelligence;Oceans;Simulated annealing,evolutionary computation;groupware;knowledge based systems;military computing;neural nets,computational Intelligence;Security & Defense applications;review CI applications;maritime domain;NATO Nations;CI techniques,,,28.0,,,,,IEEE,IEEE Conferences
249,Mitigating Challenges in the Elicitation and Analysis of Transparency Requirements,L. Chazette,Leibniz Universität Hannover,2019 IEEE 27th International Requirements Engineering Conference (RE),,2019,,,470,475,"Software systems are getting more and more complex, with an increasing integration of machine-learning based decisions. The ubiquitous presence of these systems makes users more dependent on them and their correctness in many aspects of daily life. Thus, there is a rising need to make software systems and their decisions more comprehensible. This seems to call for more transparency in software-supported decisions. Therefore, transparency requirements have to be understood, elicited and translated to lower level requirements. However, there is a lack of understanding about the requirements engineering process for transparency and how the different roles, e.g., UX designers, data scientists, and other stakeholders, have to interact in this process. In order to fill this gap, the requirements engineering process for transparency requirements needs to be thoroughly investigated. For this purpose, I intend to conduct empirical studies with practitioners and other stakeholders to gain a deeper understanding of the process and its associated problems. Based on the results, additional research will be conducted to investigate and propose solutions with the purpose of supporting requirements engineering when transparency is required.",2332-6441,978-1-7281-3912-8,10.1109/RE.2019.00064,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920445,Software Transparency;Non-Functional Requirements;Requirements Analysis;Requirements Elicitation,Requirements engineering;Stakeholders;Bibliographies;Software systems;Privacy;Usability,decision making;formal specification;learning (artificial intelligence);software maintenance;systems analysis,software systems;software-supported decisions;transparency requirements;lower level requirements;requirements engineering process;supporting requirements engineering;mitigating challenges;machine-learning based decisions,,,29.0,,,,,IEEE,IEEE Conferences
250,Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics,Y. Ma; T. Xie; J. Li; R. Maciejewski,"School of Computing, Informatics & Decision Systems Engineering, Arizona State University; School of Computing, Informatics & Decision Systems Engineering, Arizona State University; Department of Electrical and Computer EngineeringUniversity of Virginia; School of Computing, Informatics & Decision Systems Engineering, Arizona State University",IEEE Transactions on Visualization and Computer Graphics,,2020,26,1.0,1075,1085,"Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.",1941-0506,,10.1109/TVCG.2019.2934631,U.S. Department of Homeland Security; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812988,Adversarial machine learning;data poisoning;visual analytics,Analytical models;Machine learning;Visual analytics;Training;Predictive models,data analysis;data visualisation;learning (artificial intelligence),data analysis;multifaceted visualization;artificial intelligence;adversarial attacks;visual analytics;adversarial machine learning,,,70.0,,,,,IEEE,IEEE Journals
251,Software Transparency as a Key Requirement for Self-Driving Cars,L. M. Cysneiros; M. Raffi; J. C. Sampaio do Prado Leite,"York Univ., Toronto, ON, Canada; York Univ., Toronto, ON, Canada; Pontificia Univ. Catolica Rio de Janeiro, Rio de Janeiro, Brazil",2018 IEEE 26th International Requirements Engineering Conference (RE),,2018,,,382,387,"Self-Driving cars is a fast-growing area of study both in academia and industry. It is part of a broader domain which involves the development of software for Highly Automated Vehicles (HAV) and notions extracted from Artificial Intelligence/Autonomous Systems (AI/AS). There are many challenges that must be overcome to deliver self-driving cars in a manner that is readily accepted by consumers and society. Studies have shown that although many people are comfortable with the idea of AI helping them to operate their houses or schedule appointments, not many people are comfortable with the idea of cars being driven by AI algorithms. At the same time, insurance companies are concerned about vehicle liability issues and how to demonstrate who/what caused an accident. We believe that self-driving cars that demonstrate transparency in their operations will increase consumer trust which is pivotal to its acceptance and will pave the way for its commercialization and daily use. In this work, we investigate how to pursue the elicitation and modeling of transparency as a Non-Functional Requirement (NFR) to produce self-driving cars that are more robust.",2332-6441,978-1-5386-7418-5,10.1109/RE.2018.00-21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8491154,"Autonomous Systems, Non-Functional Requirements, Highly Automated Vehicles, and Transparency",Automobiles;Autonomous automobiles;Software;Industries;Autonomous vehicles,artificial intelligence;automobiles;data privacy;formal specification;intelligent transportation systems,self-driving cars;software transparency;highly automated vehicles;artificial intelligence;autonomous systems;AI algorithms;insurance companies;vehicle liability issues;consumer trust;nonfunctional requirement,,3.0,34.0,,,,,IEEE,IEEE Conferences
252,Implementing a Robust Explanatory Bias in a Person Re-identification Network,E. Bekele; W. E. Lawson; Z. Horne; S. Khemlani,"Nat. Res. Council, Washington, DC, USA; Naval Res. Lab., Washington, DC, USA; Arizona State Univ., Phoenix, AZ, USA; Naval Res. Lab., Washington, DC, USA",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,2018,,,2246,22467,"Deep learning improved attributes recognition significantly in recent years. However, many of these networks remain ""black boxes"" and providing a meaningful explanation of their decisions is a major challenge. When these networks misidentify a person, they should be able to explain this mistake. The ability to generate explanations compelling enough to serve as useful accounts of the system's operations at a very high human-level is still in its infancy. In this paper, we utilize person re-identification (re-ID) networks as a platform to generate explanations. We propose and implement a framework that can be used to explain person re-ID using soft-biometric attributes. In particular, the resulting framework embodies a cognitively validated explanatory bias: people prefer and produce explanations that concern inherent properties instead of extrinsic influences. This bias is pervasive in that it affects the fitness of explanations across a broad swath of contexts, particularly those that concern conflicting or anomalous observations. To explain person re-ID, we developed a multiattribute residual network that treats a subset of its features as either inherent or extrinsic. Using these attributes, the system generates explanations based on inherent properties when the similarity of two input images is low, and it generates explanations based on extrinsic properties when the similarity is high. We argue that such a framework provides a blueprint for how to make the decisions of deep networks comprehensible to human operators. As an intermediate step, we demonstrate state-of-the-art attribute recognition performance on two pedestrian datasets (PETA and PA100K) and a face-based attribute dataset (CelebA). The VIPeR dataset is then used to generate explanations for re-ID with a network trained on PETA attributes.",2160-7516,978-1-5386-6100-0,10.1109/CVPRW.2018.00291,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575462,,Measurement;Feature extraction;Clothing;Cognition;Network architecture,biometrics (access control);image recognition;learning (artificial intelligence);neural nets,multiattribute residual network;deep networks;face-based attribute dataset;robust explanatory bias;soft-biometric attributes;attribute recognition performance;person re-identification network;deep learning;black boxes;re-ID network;pedestrian datasets,,,19.0,,,,,IEEE,IEEE Conferences
254,Model Checking Human-Agent Collectives for Responsible AI,D. B. Abeywickrama; C. Cirstea; S. D. Ramchurn,"University of Southampton,School of Electronics and Computer Science,Southampton,UK; University of Southampton,School of Electronics and Computer Science,Southampton,UK; University of Southampton,School of Electronics and Computer Science,Southampton,UK",2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),,2019,,,1,8,"Humans and agents often need to work together and agree on collective decisions. Ensuring that autonomous systems work responsibly is complex especially when encountering dilemmas. This paper proposes a novel, systematic model checking approach to responsible decision making by a human-agent collective to ensure it is safe, controllable and ethical. Our approach, which is based on the MCMAS model checker, verifies the permissibility of an agent's actions by checking the decision-making behaviour against the logical formulae specified for safety, controllability and ethical behaviour. The verification results through counterexamples and simulation results can provide a judgement, and an explanation to the AI engineer of the reasons actions are refused or allowed.",1944-9437,978-1-7281-2622-7,10.1109/RO-MAN46459.2019.8956429,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8956429,,,formal verification;multi-agent systems;temporal logic,human-agent collectives;systematic model checking approach;MCMAS model checker;decision-making behaviour;controllability;ethical behaviour;AI engineer,,,25.0,,,,,IEEE,IEEE Conferences
256,Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections,M. El-Assady; R. Kehlbeck; C. Collins; D. Keim; O. Deussen,"University of Konstanz, Germany; University of Konstanz, Germany; Ontario Tech University, Canada; University of Konstanz, Germany; University of Konstanz, Germany",IEEE Transactions on Visualization and Computer Graphics,,2020,26,1.0,1001,1011,"We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decisionmaking process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.",1941-0506,,10.1109/TVCG.2019.2934654,DFG/SPP-1999 VALIDA; SFB/Transregio 161; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807224,Topic Model Optimization;Word Embedding;Mixed-Initiative Refinement;Guided Visual Analytics;Semantic Mapping,Semantics;Analytical models;Computational modeling;Visual analytics;Machine learning;Task analysis,data analysis;data visualisation;document handling;interactive systems;learning (artificial intelligence);natural language processing;vectors,guided topic model refinement;word-embedding projections;topic modeling;interactive visual analytics;user-refined concepts;user interactions;vector space model;visual knowledge externalization;learning process;semantic concept spaces;topic model quality,,2.0,59.0,,,,,IEEE,IEEE Journals
259,Trusting Intelligent Machines: Deepening Trust Within Socio-Technical Systems,P. Andras; L. Esterle; M. Guckert; T. A. Han; P. R. Lewis; K. Milanovic; T. Payne; C. Perret; J. Pitt; S. T. Powers; N. Urquhart; S. Wells,"Computing and Mathematics, Keele University, Keele, Staffordshire, United Kingdom; Computer Science and Artificial Intelligence Lab, Aston University, Birmingham, United Kingdom; Fachbereich Mathematik Naturwissenschaften und Datenverarbeitung, Technische Hochschule Mittelhessen, Giessen, Germany; Computing, Teesside University, Middlesbrough, United Kingdom; Computer Science and Artificial Intelligence Lab, Aston University, Birmingham, United Kingdom; Electrical and Electronic Engineering, Imperial College London, United Kingdom; Computer Science, University of Liverpool, Liverpool, United Kingdom; Computing, Edinburgh Napier University, Edinburgh, United Kingdom; Electrical and Electronic Engineering, Imperial College London, United Kingdom; Computing, Edinburgh Napier University, Edinburgh, United Kingdom; Computing, Edinburgh Napier University, Edinburgh, United Kingdom; Computing, Edinburgh Napier University, Edinburgh, United Kingdom",IEEE Technology and Society Magazine,,2018,37,4.0,76,83,"Intelligent machines have reached capabilities that go beyond a level that a human being can fully comprehend without sufficiently detailed understanding of the underlying mechanisms. The choice of moves in the game Go (generated by Deep Mind?s Alpha Go Zero [1]) are an impressive example of an artificial intelligence system calculating results that even a human expert for the game can hardly retrace [2]. But this is, quite literally, a toy example. In reality, intelligent algorithms are encroaching more and more into our everyday lives, be it through algorithms that recommend products for us to buy, or whole systems such as driverless vehicles. We are delegating ever more aspects of our daily routines to machines, and this trend looks set to continue in the future. Indeed, continued economic growth is set to depend on it. The nature of human-computer interaction in the world that the digital transformation is creating will require (mutual) trust between humans and intelligent, or seemingly intelligent, machines. But what does it mean to trust an intelligent machine? How can trust be established between human societies and intelligent machines?",1937-416X,,10.1109/MTS.2018.2876107,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8558724,,Artificial intelligence;Ethics;Cognition;Intelligent systems;Game theory;Economics;Multi-agent systems,artificial intelligence;human factors;knowledge based systems;multi-agent systems;social aspects of automation;software agents,digital transformation;intelligent machines;artificial intelligence system;human societies;human-computer interaction;intelligent algorithms;socio-technical systems;trust,,3.0,38.0,,,,,IEEE,IEEE Magazines
260,"A vision for human-machine mutual understanding, trust establishment, and collaboration",C. R. B. Azevedo; K. Raizer; R. Souza,Ericsson Research - Affiliated Brazilian Branch; Ericsson Research - Affiliated Brazilian Branch; Ericsson Research - Affiliated Brazilian Branch,2017 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA),,2017,,,1,3,"Human-machine interactions are likely to require synergistic multidisciplinary research efforts for supporting a paradigm shift towards collaborative-oriented use cases. An essential aspect of collaboration is trust and in order to establish it there is need for human-machine mutual understanding (HMMU). We argue that achieving HMMU will require evolving from an approach that reduces human factors as uncontrollable environmental elements, to one that repositions human emotions not only as a central part of an integrated control paradigm, but also as interpretable and steerable through appropriate information flows and mutual learning cycles. On the strategic decision-making side, we argue conflict resolution will require anticipating multiple trade-off situations that include human factors. On the operational level, symbiotic human-machine cognitive architectures should embed detected human emotions as inputs in shared machine control models. Trust measurements will play the role of mediating task coordination by pinpointing and dynamically composing appropriate situation-aware interaction protocols. In addition to a vision for HMMU, this paper proposes a multidisciplinary research strategy that attempts to unify the isolated efforts of different communities. The proposed vision is contextualized within a high-level research roadmap to support near and long-term activities in HMMU.",2379-1675,978-1-5090-6380-2,10.1109/COGSIMA.2017.7929606,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7929606,Human-Machine Interaction;Autonomous Systems;Situation-Awareness;Cognitive Agents;Trust;Shared Control,Collaboration;Hidden Markov models;Autonomous systems;Robots;Man-machine systems;Human factors;Computer architecture,decision making;groupware;human computer interaction;learning (artificial intelligence);software architecture,human-machine mutual understanding;trust establishment;human-machine interactions;collaborative-oriented use cases;HMMU;information flows;mutual learning cycles;strategic decision making;symbiotic human-machine cognitive architectures;situation-aware interaction protocols,,3.0,21.0,,,,,IEEE,IEEE Conferences
261,A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations,J. Krause; A. Dasgupta; J. Swartz; Y. Aphinyanaphongs; E. Bertini,NYU Tandon School of Engineering; Pacific Northwest National Laboratory; NYU School of Medicine; NYU School of Medicine; NYU Tandon School of Engineering,2017 IEEE Conference on Visual Analytics Science and Technology (VAST),,2017,,,162,172,"Human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions. To this end, we propose a visual analytics workflow to help data scientists and domain experts explore, diagnose, and understand the decisions made by a binary classifier. The approach leverages “instance-level explanations”, measures of local feature relevance that explain single instances, and uses them to build a set of visual representations that guide the users in their investigation. The workflow is based on three main visual representations and steps: one based on aggregate statistics to see how data distributes across correct / incorrect decisions; one based on explanations to understand which features are used to make these decisions; and one based on raw data, to derive insights on potential root causes for the observed patterns. The workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed. The case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes, thus experts can generate useful hypotheses on how a model can be improved.",,978-1-5386-3163-8,10.1109/VAST.2017.8585720,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585720,Machine Learning;Interpretation;Visual Analytics,Data models;Analytical models;Predictive models;Machine learning;Collaboration;Visual analytics,data analysis;data mining;data visualisation;learning (artificial intelligence);pattern classification;statistical analysis,machine learning models;visual diagnostics;binary classifier;instance-level explanations;visual analytics workflow;data scientists;local feature relevance;aggregate statistics;human-in-the-loop data analysis applications;visual representations,,5.0,33.0,,,,,IEEE,IEEE Conferences
262,Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification,P. Law; R. C. Basole; Y. Wu,Georgia Institute of Technology; Georgia Institute of Technology; Visa Research,IEEE Transactions on Visualization and Computer Graphics,,2019,25,1.0,427,437,"Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.",1941-0506,,10.1109/TVCG.2018.2864526,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440115,Pairwise comparison;novices;data analysis;automatic insight generation,Data analysis;Data visualization;Tools;Law enforcement;Visualization;Systems operation;Knowledge discovery,data analysis;data visualisation;decision making;recommender systems,object group;data table;minimal specification;Duet's explanations;low-level operations;pairwise comparison;visual analysis system;interpretation barriers;groups recommendation;data analysis novices;Duet recommendation;textual descriptions;qualitative evaluation,,3.0,51.0,,,,,IEEE,IEEE Journals
263,Visual Analytics for Root Cause Analysis in Self-Organizing Industrial Systems,M. Kiermeier; S. Feld,"Mobile and Distributed Systems Group, LMU Munich; Mobile and Distributed Systems Group, LMU Munich",2018 IEEE 16th International Conference on Industrial Informatics (INDIN),,2018,,,315,320,"Root cause analysis (RCA) is a central task for quality assurance in manufacturing plants. By tracing back anomalies to its actual trigger, recurrent misbehavior can be eliminated, which improves the system's future performance. In self-organizing industrial systems (SOIS), however, where the system adapts its behavior to the current circumstances and requests, new challenges arise for RCA. For example, the system decides dynamically at runtime how to route the work-pieces through the factory. This high degree of freedom of the system causes a state space explosion, which makes it difficult to formalize explicit connections. In addition, there are new dependency relationships resulting from the online decision making process and its influencing factors, which have to be taken into account for RCA. Accordingly, in this paper, we present first of all a taxonomy of possible root causes in such SOIS. Thereby, we focus in particular on possible error sources resulting from the online decision making process. Based on this, corresponding backtracking approaches are presented, whereby automatable and non-automatable procedures are distinguished. The latter becomes relevant in case that a component of the online decision making system is not evaluable automatably due to the state space explosion. To trace back anomalies anyway, we propose here a visual analytics solution. A corresponding proof of concept which implements the necessary functions for an expert-based assessment is presented in this paper.",2378-363X,978-1-5386-4829-2,10.1109/INDIN.2018.8471969,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8471969,,Decision making;Visual analytics;Taxonomy;Sensors;Production systems;Runtime;Explosions,data visualisation;decision making;manufacturing data processing;quality assurance,RCA;state space explosion;SOIS;online decision making system;visual analytics solution;root cause analysis;quality assurance;manufacturing plants;recurrent misbehavior;self-organizing industrial systems;backtracking approach,,,20.0,,,,,IEEE,IEEE Conferences
264,Non-Invasive Meningitis Diagnosis Using Decision Trees,V. M. Lelis; E. Guzmán; M. Belmonte,"Federal Institute of Education, Science and Technology of Bahia, Salvador, Brazil; Departmento de Lenguajes y Ciencia de la Computación. E.T.S. Ingeniería Informática. Universidad de Málaga, Málaga, Spain; Departmento de Lenguajes y Ciencia de la Computación. E.T.S. Ingeniería Informática. Universidad de Málaga, Málaga, Spain",IEEE Access,,2020,8,,18394,18407,"Meningitis is one of the pandemic diseases that many less developed countries suffer, primarily due to the lack of economic resources to face it. The more severe types of meningitis, Meningococcal Disease, MD, demand immediate medical attention since delays increase the risk of mortality. This paper presents an open and integrated Clinical Decision Support System to assist physicians in the different stages of meningitis diagnostics through observable symptoms. Our system integrates three intelligent components which try to give support to physicians in early diagnostics of meningitis. These components are based on interpretable tree-based machine learning models and knowledge-engineering techniques. A dataset of 26,228 records of patients with a meningitis diagnosis in Brazil was used to construct and evaluate the system. The performance indicators of the decision models exhibit an outstanding classification performance for MD meningitis with a classification accuracy of 94.3%. In order to test the correct diagnosis of the system, an evaluation study with real patients' data was performed. The experimental results concluded that excluding meningitis cases based only on observable symptoms is much more complicated than diagnosing it. However, the system properly diagnosed 88% of meningitis cases from the real database.",2169-3536,,10.1109/ACCESS.2020.2966397,Universidad de Málaga; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957562,Medical diagnosis;meningitis diagnostic models;tree-based machine learning,Microorganisms;Medical diagnostic imaging;Diseases;Machine learning;Tools;Decision trees,decision support systems;decision trees;developing countries;diseases;learning (artificial intelligence);medical diagnostic computing;patient diagnosis;pattern classification,interpretable tree;machine learning;knowledge-engineering;meningitis diagnostics;clinical decision support system;Meningococcal Disease;developing countries;pandemic diseases;Decision trees;noninvasive meningitis diagnosis;meningitis cases;MD meningitis,,,43.0,CCBY,,,,IEEE,IEEE Journals
265,Attention-Based Multi-Task Learning in Pharmacovigilance,S. Zhang; S. Dev; J. Voyles; A. S. Rao,"Artificial Intelligence Accelerator PricewaterhouseCoopers Advisory, New York, NY, USA; Artificial Intelligence Accelerator PricewaterhouseCoopers Advisory, New York, NY, USA; Artificial Intelligence Accelerator PricewaterhouseCoopers Advisory, New York, NY, USA; Artificial Intelligence Accelerator PricewaterhouseCoopers Advisory, New York, NY, USA",2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),,2018,,,2324,22328,"Pharmacovigilance (PV) is the process of monitoring and assessing adverse events (AEs). Currently, the primary method of analysis in PV is manual inspection by individual case managers. However, this is largely unsustainable due to the increased volume of cases over the past few years. Since AE processing involves several sub-tasks, such as annotation and classification, our paper explores a novel solution to PV. In this paper, we propose a multi-task learning (MTL) model, where the hidden layers are shared, to jointly learn the tasks (Named Entity Recognition (NER), Classification). The results of our paper demonstrate that MTL is able to outperform our baseline classification model and equal the baseline model for annotation/NER.11Disclaimer: This content is for general information purposes only, and should not be used as a substitute for consultation with professional advisors.",,978-1-5386-5488-0,10.1109/BIBM.2018.8621286,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621286,machine learning;adverse events;clinical text;multi-task learning;pharmacovigilance;attention-based model,Task analysis;Drugs;FDA;Natural language processing;Recurrent neural networks;Monitoring,learning (artificial intelligence);medical information systems;pattern classification,PV;AE processing;baseline classification model;pharmacovigilance;MTL model;adverse events;annotation-NER;named entity recognition;attention-based multitask learning model,,,36.0,,,,,IEEE,IEEE Conferences
266,Current and future methodologies of after action review in simulation-based training,S. Hanoun; S. Nahavandi,"Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Geelong, Vic 3217, Australia; Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Geelong, Vic 3217, Australia",2018 Annual IEEE International Systems Conference (SysCon),,2018,,,1,6,"After Action Review (AAR) is a crucial component in training and particularly in Simulation-based Training (SBT). The effectiveness of AAR is highly dependent on the ability to adequately track and measure the individuals performance during the training exercise scenario. Evaluation and analysis of the individuals captured performance gives importance to AAR and can support developing automated AAR tools where the relationships and connections between actions and events can be automatically determined. In this paper, we regard AAR in SBT with three goals in mind. Firstly, to shed some light on current AAR practices in different SBT application domains. Secondly, to present different AAR methodologies that are followed. Finally, to recommend on future methodologies that are worth adopting to enable better AAR practices in the future.",2472-9647,978-1-5386-3664-0,10.1109/SYSCON.2018.8369516,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8369516,,Training;Task analysis;Tools;Observers;Planning;Data models;Time measurement,computer based training,training exercise scenario;automated AAR tools;after action review;simulation-based training;SBT application domains;AAR methodologies,,1.0,20.0,,,,,IEEE,IEEE Conferences
267,"Unsupervised Machine Learning for Networking: Techniques, Applications and Research Challenges",M. Usama; J. Qadir; A. Raza; H. Arif; K. A. Yau; Y. Elkhatib; A. Hussain; A. Al-Fuqaha,"Information Technology University (ITU)-Punjab, Lahore, Pakistan; Information Technology University (ITU)-Punjab, Lahore, Pakistan; National University of Science and Technology (NUST), Islamabad, Pakistan; National University of Science and Technology (NUST), Islamabad, Pakistan; Sunway University, Subang Jaya, Malaysia; The School of Computing and Communications, Lancaster University, Lancaster, U.K.; School of Computing, Edinburgh Napier University, Edinburgh, U.K.; Information and Computing Technology (ICT) Division, College of Science and Engineering (CSE), Hamad Bin Khalifa University, Doha, Qatar",IEEE Access,,2019,7,,65579,65615,"While machine learning and artificial intelligence have long been applied in networking research, the bulk of such works has focused on supervised learning. Recently, there has been a rising trend of employing unsupervised machine learning using unstructured raw network data to improve network performance and provide services, such as traffic engineering, anomaly detection, Internet traffic classification, and quality of service optimization. The growing interest in applying unsupervised learning techniques in networking stems from their great success in other fields, such as computer vision, natural language processing, speech recognition, and optimal control (e.g., for developing autonomous self-driving cars). In addition, unsupervised learning can unconstrain us from the need for labeled data and manual handcrafted feature engineering, thereby facilitating flexible, general, and automated methods of machine learning. The focus of this survey paper is to provide an overview of applications of unsupervised learning in the domain of networking. We provide a comprehensive survey highlighting recent advancements in unsupervised learning techniques, and describe their applications in various learning tasks, in the context of networking. We also provide a discussion on future directions and open research issues, while identifying potential pitfalls. While a few survey papers focusing on applications of machine learning in networking have previously been published, a survey of similar scope and breadth is missing in the literature. Through this timely review, we aim to advance the current state of knowledge, by carefully synthesizing insights from previous survey papers, while providing contemporary coverage of the recent advances and innovations.",2169-3536,,10.1109/ACCESS.2019.2916648,Qatar National Library (QNL); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713992,Machine learning;deep learning;unsupervised learning;computer networks,Unsupervised learning;Deep learning;Anomaly detection;Internet of Things;Quality of service,computer networks;Internet;unsupervised learning,learning tasks;survey paper;networking stems;unsupervised learning techniques;Internet traffic classification;network performance;unstructured raw network data;unsupervised machine;supervised learning;networking research;machine learning,,3.0,321.0,,,,,IEEE,IEEE Journals
269,Personalised self-explanation by robots: The role of goals versus beliefs in robot-action explanation for children and adults,F. Kaptein; J. Broekens; K. Hindriks; M. Neerincx,"Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands",2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),,2017,,,676,682,"A good explanation takes the user who is receiving the explanation into account. We aim to get a better understanding of user preferences and the differences between children and adults who receive explanations from a robot. We implemented a Nao-robot as a belief-desire-intention (BDI)-based agent and explained its actions using two different explanation styles. Both are based on how humans explain and justify their actions to each other. One explanation style communicates the beliefs that give context information on why the agent performed the action. The other explanation style communicates the goals that inform the user of the agent's desired state when performing the action. We conducted a user study (19 children, 19 adults) in which a Nao-robot performed actions to support type 1 diabetes mellitus management. We investigated the preference of children and adults for goalversus belief-based action explanations. From this, we learned that adults have a significantly higher tendency to prefer goal-based action explanations. This work is a necessary step in addressing the challenge of providing personalised explanations in human-robot and human-agent interaction.",1944-9437,978-1-5386-3518-6,10.1109/ROMAN.2017.8172376,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8172376,,Robots;Psychology;Pediatrics;Diabetes;Intelligent systems;Games;Aging,human factors;human-robot interaction;multi-agent systems,human-agent interaction;personalised self-explanation;robot-action explanation;user preferences;explanation style;Nao-robot performed actions;human-robot interaction;explanation styles;goal versus belief-based action explanations;belief-desire-intention-based agent,,1.0,29.0,,,,,IEEE,IEEE Conferences
270,Generative And Encoded Anomaly Detectors,T. H. Emerson; J. A. Edelberg; T. Doster; N. Merrill; C. C. Olson,"Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW,Washington,DC,USA,20375; Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW,Washington,DC,USA,20375; Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW,Washington,DC,USA,20375; Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW,Washington,DC,USA,20375; Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW,Washington,DC,USA,20375",2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS),,2019,,,1,5,We present two fully unsupervised deep learning approaches for hyperspectral anomaly detection. In one approach we formulate the anomaly detection problem as an adversarial game where a generator network learns the distribution of the hyperspectral background pixels comprising a single hyperspectral image and the output of the corresponding discriminator network yields a detection statistic. The other approach formulates the detection statistic as the error between an input hyperspectral pixel and the reconstruction of that pixel by an autoencoder network trained on the image. Both methods leverage a sub-sampling scheme that allows for unsupervised training and testing on the same data set. Our approaches are validated on a four-class synthetic hyperspectral data set and compared to a statistical approach (RX) and a geometric approach (skelton kernel principal component analysis). The proposed Generative Anomaly Detector algorithm achieves top performance on the data set while the autoencoder detection scheme also demonstrates performance gains relative to the comparison algorithms. Benefits and drawbacks of the approaches are discussed and highlight the many potential directions for future work.,2158-6276,978-1-7281-5294-3,10.1109/WHISPERS.2019.8920850,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920850,Anomaly Detection;Generative Adversarial Networks;Autoencoder;Hyperspectral Imaging,Hyperspectral imaging;Detectors;Anomaly detection;Training;Gallium nitride;Machine learning;Image reconstruction,hyperspectral imaging;image classification;object detection;principal component analysis;unsupervised learning,autoencoder detection scheme;fully unsupervised deep learning approaches;hyperspectral anomaly detection;anomaly detection problem;adversarial game;generator network;hyperspectral background pixels;single hyperspectral image;corresponding discriminator network;detection statistic;hyperspectral pixel;autoencoder network;sub-sampling scheme;four-class synthetic hyperspectral data;statistical approach;geometric approach;skelton kernel principal component analysis;generative anomaly detector algorithm,,,30.0,,,,,IEEE,IEEE Conferences
271,A Review of Intelligent Cybersecurity with Bayesian Networks,M. J. Pappaterra; F. Flammini,"Linnaeus University,Växjö,Sweden; Linnaeus University,Växjö,Sweden","2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",,2019,,,445,452,"Cybersecurity threats have surged in the past decades. Experts agree that conventional security measures will soon not be enough to stop the propagation of more sophisticated and harmful cyberattacks. Recently, there has been a growing interest in mastering the complexity of cybersecurity by adopting methods borrowed from Artificial Intelligence (AI) in order to support automation. In this paper, we provide a brief survey and some hints about Bayesian Network applications to intelligent cybersecurity in order to enable quantitative threat assessment for superior risk analysis and situation awareness.",2577-1655,978-1-7281-4569-3,10.1109/SMC.2019.8913864,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913864,,Artificial intelligence;Bayes methods;Computer crime;Hidden Markov models;Internet,artificial intelligence;belief networks;risk analysis;security of data,cybersecurity threats;harmful cyberattacks;intelligent cybersecurity;quantitative threat assessment;Bayesian network applications;Bayesian networks,,,32.0,,,,,IEEE,IEEE Conferences
272,Software Engineering for Machine Learning: A Case Study,S. Amershi; A. Begel; C. Bird; R. DeLine; H. Gall; E. Kamar; N. Nagappan; B. Nushi; T. Zimmermann,"Microsoft Research, United States; Microsoft Research, United States; Microsoft Research, United States; Microsoft Research, United States; University of Zurich, Switzerland; Microsoft Research, United States; Microsoft Research, United States; Microsoft Research, United States; Microsoft Research, United States",2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),,2019,,,291,300,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",,978-1-7281-1760-7,10.1109/ICSE-SEIP.2019.00042,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804457,artifical intelligence;machine learning;software engineering;process;data,,learning (artificial intelligence);software prototyping,nine-stage workflow process;Microsoft teams;large-scale AI solutions;machine learning applications;software teams;data science tools;agile-like software engineering processes,,7.0,37.0,,,,,IEEE,IEEE Conferences
273,Extracting Topic Related Keywords by Backtracking CNN Based Text Classifier,J. Cha; J. Lee,Sungkyunkwan University; Sungkyunkwan University,2018 Joint 10th International Conference on Soft Computing and Intelligent Systems (SCIS) and 19th International Symposium on Advanced Intelligent Systems (ISIS),,2018,,,93,96,"In the last decades, many studies have been done to extract keywords from text and they show remarkable performance. Most of these studies use a rule-based methodology. They usually focus on Part of Speech(POS), collocations, co-occurrences and dependency of words. However, considering the topic of text is very important key for extracting keywords. Thus, in this paper, we proposed a keywords extracting method using Convolutional Neural Network(CNN) based text classifier which has sufficient information about the topic of text. Experimental results show that using topic related information in CNN text classifier model can improve the quality of keywords extraction. Also proposed method is advantageous in that it requires only a deep learning model differently from existing methods.",,978-1-5386-2633-7,10.1109/SCIS-ISIS.2018.00026,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716115,"keywords extraction, CNN, Grad-Cam",Text categorization;Feature extraction;Visualization;Convolution;Data models;Convolutional neural networks,backtracking;convolutional neural nets;learning (artificial intelligence);natural language processing;text analysis,topic related information;CNN text classifier model;keywords extraction;topic related keywords;CNN based text classifier;rule-based methodology;co-occurrences;convolutional neural network;backtracking;collocations;part-of-speech;dependency-of-words,,,13.0,,,,,IEEE,IEEE Conferences
274,Fuzzy Sets in Data Analysis: From Statistical Foundations to Machine Learning,I. Couso; C. Borgelt; E. Hullermeier; R. Kruse,"Statistics, University of Oviedo, Gijon, Spain; Computer and Information Science, University of Konstanz, Konstanz, Germany; Computer Science and Engineering, Paderborn University, Germany; Computer Science and Engineering, Otto von Guericke University, Magdeburg, Germany",IEEE Computational Intelligence Magazine,,2019,14,1.0,31,44,"Basic ideas and formal concepts from fuzzy sets and fuzzy logic have been used successfully in various branches of science and engineering. This paper elaborates on the use of fuzzy sets in the broad field of data analysis and statistical sciences, including modern manifestations such as data mining and machine learning. In the fuzzy logic community, this branch of research has recently gained in importance, especially due to the emergence of data science as a new scientific discipline, and the increasing relevance of machine learning as a key methodology of modern artificial intelligence. This development has been accompanied by an internal shift from largely knowledge-based to strongly data-driven fuzzy modeling and systems design. Reflecting on the historical dimension and evolution of the area, we discuss the role of fuzzy logic in data analysis and related fields, highlight existing contributions of fuzzy sets in these fields, and outline interesting directions for future work.",1556-6048,,10.1109/MCI.2018.2881642,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610265,,"Fuzzy sets;Random variables;Fuzzy logic;Data analysis;Machine learning;Data models;Data mining;Zadeh, Lotfi",data analysis;data mining;fuzzy logic;fuzzy set theory;learning (artificial intelligence);statistical analysis,data analysis;machine learning;fuzzy sets;statistical sciences;data mining;fuzzy logic community;data science;data-driven fuzzy modeling,,1.0,122.0,,,,,IEEE,IEEE Magazines
276,Trust calibration within a human-robot team: Comparing automatically generated explanations,N. Wang; D. V. Pynadath; S. G. Hill,"University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; U.S. Army Research Laboratory, Aberdeen Proving Ground, MD USA",2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,2016,,,109,116,"Trust is a critical factor for achieving the full potential of human-robot teams. Researchers have theorized that people will more accurately trust an autonomous system, such as a robot, if they have a more accurate understanding of its decision-making process. Studies have shown that hand-crafted explanations can help maintain trust when the system is less than 100% reliable. In this work, we leverage existing agent algorithms to provide a domain-independent mechanism for robots to automatically generate such explanations. To measure the explanation mechanism's impact on trust, we collected self-reported survey data and behavioral data in an agent-based online testbed that simulates a human-robot team task. The results demonstrate that the added explanation capability led to improvement in transparency, trust, and team performance. Furthermore, by observing the different outcomes due to variations in the robot's explanation content, we gain valuable insight that can help lead to refinement of explanation algorithms to further improve human-robot trust calibration.",2167-2148,978-1-4673-8370-7,10.1109/HRI.2016.7451741,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451741,,Robot sensing systems;Buildings;Decision making;Reliability;Uncertainty;Artificial intelligence,human-robot interaction,human-robot team;comparing automatically generated explanations;autonomous system;decision-making process;hand crafted explanations;domain independent mechanism;behavioral data;robot explanation content;human-robot trust calibration,,23.0,49.0,,,,,IEEE,IEEE Conferences
277,TeleGam: Combining Visualization and Verbalization for Interpretable Machine Learning,F. Hohman; A. Srinivasan; S. M. Drucker,Georgia Institute of Technology; Georgia Institute of Technology; Microsoft Research,2019 IEEE Visualization Conference (VIS),,2019,,,151,155,"While machine learning (ML) continues to find success in solving previously-thought hard problems, interpreting and exploring ML models remains challenging. Recent work has shown that visualizations are a powerful tool to aid debugging, analyzing, and interpreting ML models. However, depending on the complexity of the model (e.g., number of features), interpreting these visualizations can be difficult and may require additional expertise. Alternatively, textual descriptions, or verbalizations, can be a simple, yet effective way to communicate or summarize key aspects about a model, such as the overall trend in a model's predictions or comparisons between pairs of data instances. With the potential benefits of visualizations and verbalizations in mind, we explore how the two can be combined to aid ML interpretability. Specifically, we present a prototype system, TeleGam, that demonstrates how visualizations and verbalizations can collectively support interactive exploration of ML models, for example, generalized additive models (GAMs). We describe TELEGAM's interface and underlying heuristics to generate the verbalizations. We conclude by discussing how TeleGam can serve as a platform to conduct future studies for understanding user expectations and designing novel interfaces for interpretable ML.",,978-1-7281-4941-7,10.1109/VISUAL.2019.8933695,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933695,Human-centered computing;Visual Analytics,Data models;Data visualization;Predictive models;Analytical models;Shape;Natural languages;Additives,data visualisation;interactive systems;learning (artificial intelligence);program debugging;user interfaces,TeleGam;visualization;verbalization;interpretable machine learning;hard problems;ML models;verbalizations;ML interpretability;interactive exploration;generalized additive models;heuristics;interpretable ML,,,41.0,,,,,IEEE,IEEE Conferences
280,An Ontology-Based Interpretable Fuzzy Decision Support System for Diabetes Diagnosis,S. El-Sappagh; J. M. Alonso; F. Ali; A. Ali; J. Jang; K. Kwak,"Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Centro Singular de Investigacion en Tecnoloxias da Informacion, Universidade de Santiago de Compostela, Santiago, Spain; Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Department of Biochemistry, School of Medicine, Inha University, Incheon, South Korea; Department of Information and Communication Engineering, Inha University, Incheon, South Korea",IEEE Access,,2018,6,,37371,37394,"Diabetes is a serious chronic disease. The importance of clinical decision support systems (CDSSs) to diagnose diabetes has led to extensive research efforts to improve the accuracy, applicability, interpretability, and interoperability of these systems. However, this problem continues to require optimization. Fuzzy rule-based systems are suitable for the medical domain, where interpretability is a main concern. The medical domain is data-intensive, and using electronic health record data to build the FRBS knowledge base and fuzzy sets is critical. Multiple variables are frequently required to determine a correct and personalized diagnosis, which usually makes it difficult to arrive at accurate and timely decisions. In this paper, we propose and implement a new semantically interpretable FRBS framework for diabetes diagnosis. The framework uses multiple aspects of knowledge-fuzzy inference, ontology reasoning, and a fuzzy analytical hierarchy process (FAHP) to provide a more intuitive and accurate design. First, we build a two-layered hierarchical and interpretable FRBS; then, we improve this by integrating an ontology reasoning process based on SNOMED CT standard ontology. We incorporate FAHP to determine the relative medical importance of each sub-FRBS. The proposed system offers numerous unique and critical improvements regarding the implementation of an accurate, dynamic, semantically intelligent, and interpretable CDSS. The designed system considers the ontology semantic similarity of diabetes complications and symptoms concepts in the fuzzy rules' evaluation process. The framework was tested using a real data set, and the results indicate how the proposed system helps physicians and patients to accurately diagnose diabetes mellitus.",2169-3536,,10.1109/ACCESS.2018.2852004,National Research Foundation of Korea; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8403911,Clinical decision support system;diabetes diagnosis;fuzzy inference system;ontology reasoning;fuzzy interpretability,Diabetes;Cognition;Ontologies;Medical diagnostic imaging;Diseases;Semantics,analytic hierarchy process;decision support systems;diseases;fuzzy reasoning;fuzzy set theory;knowledge based systems;medical diagnostic computing;medical information systems;ontologies (artificial intelligence);optimisation;patient diagnosis,SNOMED CT standard ontology;ontology reasoning process;fuzzy analytical hierarchy process;knowledge-fuzzy inference;semantically interpretable FRBS framework;fuzzy sets;FRBS knowledge base;electronic health record data;medical domain;fuzzy rule-based systems;clinical decision support systems;diabetes diagnosis;ontology-based interpretable fuzzy decision support system;fuzzy rules;patients diagnosis;physicians;fuzzy rules evaluation process;optimization;chronic disease;diabetes mellitus diagnosis;CDSS,,4.0,52.0,,,,,IEEE,IEEE Journals
281,Can Analytics as a Service Save the Online Discussion Culture? - The Case of Comment Moderation in the Media Industry,J. Brunk; M. Niemann; D. M. Riehle,University of Münster - ERCIS; University of Münster - ERCIS; University of Münster - ERCIS,2019 IEEE 21st Conference on Business Informatics (CBI),,2019,01,,472,481,"In recent years, online public discussions face a proliferation of racist, politically, and religiously motivated hate comments, threats, and insults. With the failure of purely manual moderation, platform operators started searching for semi-automated or even completely automated approaches for comment moderation. One promising option to (semi-) automate the moderation process is the application of Natural Language Processing and Machine Learning (ML) techniques. In this paper we describe the challenges, that currently prevent the application of these techniques and therefore the development of (semi-) and automated solutions. As most of the challenges (e.g., curation of big datasets) require huge financial investments, only big players, such as Google or Facebook, will be able to invest in them. Many of the smaller and medium-sized internet companies will fall behind. To allow this bulk of (media) companies to stay competitive, we design a novel Analytics as a Service (AaaS) offering that will also allow small and medium sized enterprises to profit from ML decision support. We then use the identified challenges to evaluate the conceptual design of the business model and highlight areas of future research to enable the instantiation of the AaaS platform.",2378-1971,978-1-7281-0650-2,10.1109/CBI.2019.00061,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8808072,comment moderation;machine learning;hate speech;abusive language;moderation;business model,Media;Companies;Law;Machine learning;Labeling,cloud computing;investment;learning (artificial intelligence);natural language processing;small-to-medium enterprises;social networking (online),purely manual moderation;platform operators;comment moderation;moderation process;automated solutions;big datasets;huge financial investments;big players;medium-sized internet companies;online discussion culture;media industry;public discussions;religiously motivated hate comments;analytics as a service;natural language processing;machine learning;AaaS;small and medium sized enterprises;ML decision support,,,89.0,,,,,IEEE,IEEE Conferences
282,Explainable Sentiment Analysis with Applications in Medicine,C. Zucco; H. Liang; G. D. Fatta; M. Cannataro,"Data Analytics Research Center, University “Magna Græcia” Viale Europa, Catanzaro, 88100, Italy; Department of Computer Science, University of Reading, Reading, RG6 6AY, UK; Department of Computer Science, University of Reading, Reading, RG6 6AY, UK; Data Analytics Research Center, University “Magna Græcia” Viale Europa, Catanzaro, 88100, Italy",2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),,2018,,,1740,1747,"Sentiment Analysis can help to extract knowledge related to opinions and emotions from user generated text information. It can be applied in medical field for patients monitoring purposes. With the availability of large datasets, deep learning algorithms have become a state of the art also for sentiment analysis. However, deep models have the drawback of not being non human-interpretable, raising various problems related to model's interpretability. Very few work have been proposed to build models that explain their decision making process and actions. In this work, we review the current sentiment analysis approaches and existing explainable systems. Moreover, we present a critical review of explainable sentiment analysis models and discussed the insight of applying explainable sentiment analysis in the medical field.",,978-1-5386-5488-0,10.1109/BIBM.2018.8621359,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621359,explainable models;AI;Machine Learning;Deep Learning,Sentiment analysis;Task analysis;Analytical models;Deep learning;Recurrent neural networks,data mining;decision making;learning (artificial intelligence);medical information systems;natural language processing;patient monitoring;sentiment analysis,medical field;deep learning algorithms;deep models;explainable sentiment analysis models;user generated text information;sentiment analysis approaches;explainable systems,,1.0,79.0,,,,,IEEE,IEEE Conferences
283,Optimizing Variational Graph Autoencoder for Community Detection,J. J. Choong; X. Liu; T. Murata,"Tokyo Institute of Technology,Department of Computer Science,Tokyo,Japan; National Institute of Advanced,Industrial Science and Technology,Tokyo,Japan; Tokyo Institute of Technology,Department of Computer Science,Tokyo,Japan",2019 IEEE International Conference on Big Data (Big Data),,2019,,,5353,5358,"Variational Graph Autoencoders (VGAE) has recently been a popular framework of choice for learning representations on graphs. Its inception has allowed models to achieve state-of-the-art performances for challenging tasks such as link prediction, rating prediction and node clustering. However, a fundamental flaw exists in Variational Autoencoder (VAE) based approaches. Specifically, the objective function of VAE (reconstruction loss), deviates from its primary objective (i.e clustering). In this paper, we attempt to address this issue by introducing two significant changes to Variational Graph Autoencoder for Community Detection (VGAECD). Firstly, we introduce a simplified graph convolution encoder to increase convergence speed and reduce computational time. Secondly, a dual variational objective is introduced to encourage learning of the primary objective. The outcome is a faster converging model with competitive community detection performance.",,978-1-7281-0858-2,10.1109/BigData47090.2019.9006123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006123,community detection;graph neural network;variational autoencoder;network embedding,Optimization;Machine learning;Training;Benchmark testing;Convergence;Machine learning algorithms;Task analysis,network theory (graphs);neural nets;optimisation;pattern clustering,simplified graph convolution encoder;dual variational objective;competitive community detection performance;rating prediction;node clustering;variational autoencoder based approaches;variational graph autoencoder;VGAE,,,39.0,,,,,IEEE,IEEE Conferences
284,Towards the Development of Artificial Intelligence-based Systems: Human-Centered Functional Requirements and Open Problems,T. M. Fagbola; S. C. Thakur,"Durban University of Technology,KZN e-Skills CoLab,Durban,South Africa,4000; Durban University of Technology,KZN e-Skills CoLab,Durban,South Africa,4000",2019 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS),,2019,,,200,204,"The increasing capability of AI-powered systems, including self-aware and unmanned systems, at automating simple to sophisticated tasks and their wide areas of real-world interventions in boosting productivity and enhancing competitiveness has offered transformative potentials leading to better quality of life. These systems have lately become an inseparable part of human lives. However, incorrect use leading to unintended consequences, safety, fairness, trustworthiness are major concerns of these emerging ubiquitous systems. In this paper, an attempt was made to concisely present and discuss key human-centered functional requirement specifications of emerging Artificial intelligence-based systems especially interpretability, explainability, fairness, transparency and security. Some emerging toolkits and open libraries for developing and evaluating AI-based systems are also discussed. A number of open problems with respect to managing the tradeoff among these requirements and systems' performance are presented to guide future researches in this direction.",2189-8723,978-1-7281-3380-5,10.1109/ICIIBMS46890.2019.8991505,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8991505,Explainability;Ethics;Safety;Trustworthiness;Fairness;Transparency;Artificial Intelligent Systems,,artificial intelligence;formal specification;ubiquitous computing,artificial intelligence-based systems;open problems;AI-powered systems;unmanned systems;ubiquitous systems;functional requirement specifications;security;human-centered functional requirements,,,37.0,,,,,IEEE,IEEE Conferences
285,"Spatial Structured Prediction Models: Applications, Challenges, and Techniques",Z. Jiang,"Department of Computer Science, The University of Alabama, Tuscaloosa, AL, USA",IEEE Access,,2020,8,,38714,38727,"Spatial structure patterns are prevalent in many real-world data and applications. For example, in biochemistry, the geometric topology of a molecular surface indicates protein functions; in hydrology, irregular geographic terrains and topography on the Earth's surface control water flows and distributions; in civil engineering, wetland parcels in remote sensing imagery are often made up of contiguous patches. Spatial structured prediction aims to learn a prediction model whose input and output data contain a spatial structure. Modeling spatial structural information in prediction models is critical for interdisciplinary applications due to two reasons. First, explicit spatial structural information often indicates the underlying physical process, and thus enhances model interpretability. Second, spatial structural constraints also have positive side-effects of enhancing model robustness against noise and obstacles and regularizing model learning when training labels are limited. However, spatial structured prediction also poses several unique challenges, such as the existence of implicit spatial structure in continuous space, structural complexity in geometric topology, and high computational costs. Over the years, various techniques have been proposed for spatial structured prediction in different applications. This paper aims to provide an overview of the spatial structured prediction problem. We provide a taxonomy of techniques based on the underlying approaches. We also discuss several future research directions. The paper can potentially not only help interdisciplinary researchers find relevant techniques but also help machine learning researchers identify new research opportunities.",2169-3536,,10.1109/ACCESS.2020.2975584,National Science Foundation; University Corporation for Atmospheric Research; Camgian Microsystems; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006803,Machine learning;structured prediction;spatial structured model;geometric deep learning;interdisciplinary applications,Predictive models;Spatial databases;Computational modeling;Surface topography;Machine learning;Topology;Roads,data analysis;geometry;learning (artificial intelligence);topology,spatial structured prediction models;spatial structure patterns;spatial structural information;spatial structural constraints;structural complexity;geometric topology;machine learning,,,150.0,CCBY,,,,IEEE,IEEE Journals
286,A Generative Policy Model for Connected and Autonomous Vehicles,D. Cunnington; I. Manotas; M. Law; G. d. Mel; S. Calo; E. Bertino; A. Russo,"IBM Research,Hursley,UK; IBM T.J. Watson Research,NY,USA; Imperial College London,London,UK; IBM Research,Hursley,UK; IBM T.J. Watson Research,NY,USA; Purdue University,West Lafayette,USA; Imperial College London,London,UK",2019 IEEE Intelligent Transportation Systems Conference (ITSC),,2019,,,1558,1565,"Artificial Intelligence is rapidly enhancing human capability by providing support and guidance on a wide variety of tasks. However, one of the main challenges for autonomous systems is effectively managing the decisions and interactions between multiple entities in a dynamic environment. Policies are frequently used in cyber-physical systems to define target goals and constraints, such as maximising security whilst preventing communication to unauthorised systems. In this paper we introduce an approach for learning high-level policy models for future Connected and Autonomous Vehicles (CAVs). Since CAVs are required to operate in complex, safety-critical environments with a wide range of varying contextual conditions, high-level policies can help systems achieve their goals whilst adhering to varying environmental constraints. We present a Generative Policy Model (GPM) that enables a CAV to observe, learn, and adapt high-level policy models using local knowledge shared by related entities in the environment such as other CAVs, when reliable communication to traditional policy management systems may not be available. Within the proposed CAV GPM architecture, we utilise a novel context-free grammar bounded by a set of context-sensitive annotations called Answer Set Grammars (ASGs) and perform an evaluation of CAV policy generation in varying contexts. We also release the CAVPolicy dataset of annotated policies to enable future research in this area.",,978-1-5386-7024-8,10.1109/ITSC.2019.8916782,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8916782,,Vehicles;Adaptation models;Task analysis;Monitoring;Vehicle dynamics;Reliability;Computer architecture,artificial intelligence;computer network management;context-free grammars;learning (artificial intelligence);logic programming;mobile computing;ubiquitous computing,generative policy model;artificial intelligence;human capability;autonomous systems;multiple entities;dynamic environment;cyber-physical systems;target goals;unauthorised systems;high-level policy models;complex safety-critical environments;high-level policies;CAV GPM architecture;CAV policy generation;annotated policies,,2.0,26.0,,,,,IEEE,IEEE Conferences
287,Adapting SQuaRE for Quality Assessment of Artificial Intelligence Systems,H. Kuwajima; F. Ishikawa,"Denso Corporation, Japan; National Institute of Informatics, Japan",2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),,2019,,,13,18,"More and more software practitioners are tackling towards industrial applications of artificial intelligence (AI) systems, especially those based on machine learning (ML). However, many of existing principles and approaches to traditional software systems do not work effectively for the system behavior obtained by training not by logical design. In addition, unique kinds of requirements are emerging such as fairness and explainability. To provide clear guidance to understand and tackle these difficulties, we present an analysis on what quality concepts we should evaluate for AI systems. We base our discussion on ISO/IEC 25000 series, known as SQuaRE, and identify how it should be adapted for the unique nature of ML and Ethics guidelines for trustworthy AI from European Commission. We thus provide holistic insights for quality of AI systems by incorporating the ML nature and AI ethics to the traditional software quality concepts.",,978-1-7281-5138-0,10.1109/ISSREW.2019.00035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8990311,machine learning;artificial intelligence;software quality;SQuaRE;ethics,,IEC standards;ISO standards;learning (artificial intelligence);quality management;software quality,quality assessment;artificial intelligence systems;machine learning;system behavior;logical design;explainability;ML nature;AI ethics;software quality concepts,,,14.0,,,,,IEEE,IEEE Conferences
289,Explainability as a Non-Functional Requirement,M. A. Köhl; K. Baum; M. Langer; D. Oster; T. Speith; D. Bohlender,Saarland University; Saarland University; Saarland University; Saarland University; Saarland University; RWTH Aachen University,2019 IEEE 27th International Requirements Engineering Conference (RE),,2019,,,363,368,"Recent research efforts strive to aid in designing explainable systems. Nevertheless, a systematic and overarching approach to ensure explainability by design is still missing. Often it is not even clear what precisely is meant when demanding explainability. To address this challenge, we investigate the elicitation, specification, and verification of explainablity as a Non-Functional Requirement (NFR) with the long-term vision of establishing a standardized certification process for the explainability of software-driven systems in tandem with appropriate development techniques. In this work, we carve out different notions of explainability and high-level requirements people have in mind when demanding explainability, and sketch how explainability concerns may be approached in a hypothetical hiring scenario. We provide a conceptual analysis which unifies the different notions of explainability and the corresponding explainability demands.",2332-6441,978-1-7281-3912-8,10.1109/RE.2019.00046,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920711,explainable systems;requirements specification;requirements elicitation;terminology;certified explainability,Decision making;Organizational aspects;Personnel;Stakeholders;Standardization;Certification;Terminology,certification;decision making;organisational aspects;personnel;program verification;standardisation,explainability demands;overarching approach;systematic approach;explainable systems;explainability concerns;high-level requirements people;nonfunctional requirement;demanding explainability,,1.0,46.0,,,,,IEEE,IEEE Conferences
291,Virtualized-Fault Injection Testing: A Machine Learning Approach,H. Khosrowjerdi; K. Meinke; A. Rasmusson,"Sch. of Comput. Sci., KTH R. Inst. of Technol., Stockholm, Sweden; Sch. of Comput. Sci., KTH R. Inst. of Technol., Stockholm, Sweden; Scania CV AB, Sodertalje, Sweden","2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)",,2018,,,297,308,"We introduce a new methodology for virtualized fault injection testing of safety critical embedded systems. This approach fully automates the key steps of test case generation, fault injection and verdict construction. We use machine learning to reverse engineer models of the system under test. We use model checking to generate test verdicts with respect to safety requirements formalised in temporal logic. We exemplify our approach by implementing a tool chain based on integrating the QEMU hardware emulator, the GNU debugger GDB and the LBTest requirements testing tool. This tool chain is then evaluated on two industrial safety critical applications from the automotive sector.",,978-1-5386-5012-7,10.1109/ICST.2018.00037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367057,Model based testing;black box testing;learning based testing;fault injection;virtual hardware;emulation platform;QEMU;GDB;machine learning;requirements testing;temporal logic;automotive software,Testing;Safety;Hardware;Software;Tools;Computer architecture;Machine learning,embedded systems;formal verification;learning (artificial intelligence);program debugging;program testing;reverse engineering;safety-critical software;software fault tolerance;temporal logic;virtualisation,model checking;temporal logic;QEMU hardware emulator;GNU debugger GDB;LBTest requirements testing tool;industrial safety critical applications;test verdicts;machine learning;test case generation;safety critical embedded systems;virtualized fault injection testing,,1.0,50.0,,,,,IEEE,IEEE Conferences
292,"Machine Learning for Security and the Internet of Things: The Good, the Bad, and the Ugly",F. Liang; W. G. Hatcher; W. Liao; W. Gao; W. Yu,"Department of Computer and Information Sciences, Towson University, Towson, MD, USA; Department of Computer and Information Sciences, Towson University, Towson, MD, USA; Department of Computer and Information Sciences, Towson University, Towson, MD, USA; Department of Computer and Information Sciences, Towson University, Towson, MD, USA; Department of Computer and Information Sciences, Towson University, Towson, MD, USA",IEEE Access,,2019,7,,158126,158147,"The advancement of the Internet of Things (IoT) has allowed for unprecedented data collection, automation, and remote sensing and actuation, transforming autonomous systems and bringing smart command and control into numerous cyber physical systems (CPS) that our daily lives depend on. Simultaneously, dramatic improvements in machine learning and deep neural network architectures have enabled unprecedented analytical capabilities, which we see in increasingly common applications and production technologies, such as self-driving vehicles and intelligent mobile applications. Predictably, these technologies have seen rapid adoption, which has left many implementations vulnerable to threats unforeseen or undefended against. Moreover, such technologies can be used by malicious actors, and the potential for cyber threats, attacks, intrusions, and obfuscation that are only just being considered, applied, and countered. In this paper, we consider the good, the bad, and the ugly use of machine learning for cybersecurity and CPS/IoT. In detail, we consider the numerous benefits (good use) that machine learning has brought, both in general, and specifically for security and CPS/IoT, such as the improvement of intrusion detection mechanisms and decision accuracy in CPS/IoT. More pressing, we consider the vulnerabilities of machine learning (bad use) from the perspectives of security and CPS/IoT, including the ways in which machine learning systems can be compromised, misled, and subverted at all stages of the machine learning life-cycle (data collection, pre-processing, training, validation, implementation, etc.). Finally, the most concerning, a growing trend has been the utilization of machine learning in the execution of cyberattacks and intrusions (ugly use). Thus, we consider existing mechanisms with the potential to improve target acquisition and existing threat patterns, as well as those that can enable novel attacks yet to be seen.",2169-3536,,10.1109/ACCESS.2019.2948912,National Science Foundation; University System of Maryland through the Wilson H. Elkins Professorship Award; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8879591,Security;machine learning;cyber physical systems;Internet of Things;applications;distributed environments,Machine learning;Computer security;Internet of Things;Training;Data collection,cyber-physical systems;Internet of Things;learning (artificial intelligence);neural net architecture;security of data,machine learning systems;autonomous systems;cyber physical systems;production technologies;Internet of Things;unprecedented data collection;remote sensing;smart command and control;CPS;deep neural network architectures;cyber threats;cyber attacks;cybersecurity;intrusion detection mechanisms;decision accuracy;machine learning life-cycle;threat patterns,,,192.0,CCBY,,,,IEEE,IEEE Journals
294,Cybersecurity in the Era of Data Science: Examining New Adversarial Models,B. Yener; T. Gal,"Computer Science, Renssellaer Polytechnic Institute, Troy, New York United States; Morgan Stanley, United States",IEEE Security & Privacy,,2019,17,6.0,46,53,"The ever-increasing volume, variety, and velocity of threats dictates a big data problem in cybersecurity and necessitates deployment of AI and machine-learning (ML) algorithms. The limitations and vulnerabilities of AI/ML systems, combined with complexity of data, introduce a new adversarial model, which is defined and discussed in this article.",1558-4046,,10.1109/MSEC.2019.2907097,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8705390,,Data models;Malware;Artificial intelligence;Computer security;Big Data;Real-time systems,Big Data;learning (artificial intelligence);security of data,cybersecurity;data science;big data problem;machine-learning;adversarial model;AI/ML systems,,,18.0,,,,,IEEE,IEEE Magazines
295,A Novel Graphical Lasso Based Approach Towards Segmentation Analysis in Energy Game-Theoretic Frameworks,H. P. Das; I. C. Konstantakopoulos; A. B. Manasawala; T. Veeravalli; H. Liu; C. J. Spanos,"University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),,2019,,,1702,1709,"Energy game-theoretic frameworks have emerged to be a successful strategy to encourage energy efficient behavior in large scale by leveraging human-in-the-loop strategy. A number of such frameworks have been introduced over the years which formulate the energy saving process as a competitive game with appropriate incentives for energy efficient players. However, prior works involve an incentive design mechanism which is dependent on knowledge of utility functions for all the players in the game, which is hard to compute especially when the number of players is high, common in energy game-theoretic frameworks. Our research proposes that the utilities of players in such a framework can be grouped together to a relatively small number of clusters, and the clusters can then be targeted with tailored incentives. The key to above segmentation analysis is to learn the features leading to human decision making towards energy usage in competitive environments. We propose a novel graphical lasso based approach to perform such segmentation, by studying the feature correlations in a real-world energy social game dataset. To further improve the explainability of the model, we perform causality study using grangers causality. Proposed segmentation analysis results in characteristic clusters demonstrating different energy usage behaviors. We also present avenues to implement intelligent incentive design using proposed segmentation method.",,978-1-7281-4550-1,10.1109/ICMLA.2019.00277,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999336,"Segmentation Analysis, Energy Game-Theoretic Frameworks, Graphical Lasso, Smart Building",Games;Buildings;Decision making;Hidden Markov models;Clustering algorithms;Elbow;Distortion,behavioural sciences computing;computer games;decision making;energy conservation;game theory,graphical lasso;segmentation analysis;energy game-theoretic frameworks;energy efficient behavior;energy saving process;competitive game;energy efficient players;real-world energy social game dataset;energy usage,,,32.0,,,,,IEEE,IEEE Conferences
297,Adaptivity Challenges in Games and Simulations: A Survey,R. Lopes; R. Bidarra,"Computer Graphics Group, Delft University of Technology, Delft, Netherlands; Computer Graphics Group, Delft University of Technology, Delft, Netherlands",IEEE Transactions on Computational Intelligence and AI in Games,,2011,3,2.0,85,99,"In computer games and simulations, content is often rather static and rigid. As a result, its prescripted nature can lead to predictable and impersonal gameplay, while alienating unconventional players. Adaptivity in games has therefore been recently proposed to overcome these shortcomings and make games more challenging and appealing. In this paper, we survey present research on game adaptivity, identifying, and discussing the main challenges, and pointing out some of the most promising directions ahead. We first survey the purposes of adaptivity, as the principles that could steer an adaptation and generation engine. From this perspective, we proceed to thoroughly discuss adaptivity's targets and methods. Current advances and successes in this emerging field point to many yet unexplored research opportunities. Among them, we discuss the use of gameplay expectations, learning preferences, and assessment data in the integrated adaptation of game worlds, scenarios, and quests. We conclude that, among other methods, procedural content generation and semantic modeling can powerfully combine to create offline customized content and online adjustments to game worlds, scenarios, and quests. These and other promising methods, deserving ample research efforts, can therefore, be expected to significantly contribute towards making games and simulations even more unpredictable, effective, and fun.",1943-0698,,10.1109/TCIAIG.2011.2152841,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5765665,Adaptive games;declarative modeling;online adaptivity;player assessment;player modeling;procedural content generation;semantic modeling,Games;Adaptation model;Predictive models;Learning systems;Computational modeling;IEEE Transactions on Computational Intelligence and AI in Games;Brain modeling,computer games,computer games;simulation;game adaptivity;generation engine;gameplay expectation;learning preference;procedural content generation;semantic modeling,,81.0,78.0,,,,,IEEE,IEEE Journals
298,Self-explanations of a cognitive agent by citing goals and emotions,F. Kaptein; J. Broekens; K. Hindriks; M. Neerincx,"Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands",2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),,2017,,,81,82,"This paper presents a cognitive (belief-desire-intention based) agent that can self-explain its behaviour based on its goals and emotions. We implement a cognitive agent, embodied by a nao-robot or virtual avatar thereof, to play a quiz with its user. During the interaction the agent intelligently selects questions to optimally educate the user. We show how the simulation of emotions can be used to generate end-user explanations of the agent's behaviour. With this we provide a first proof of concept showing the value of using simulated emotions in addition to goals for generating agent behaviour explanations.",,978-1-5386-0680-3,10.1109/ACIIW.2017.8272592,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8272592,,Avatars;Pediatrics;Diabetes;Intelligent systems;Biomedical monitoring;Multi-agent systems;Psychology,avatars;cognition;humanoid robots;multi-agent systems;software agents,cognitive agent;belief-desire-intention based;nao-robot;virtual avatar;end-user explanations;simulated emotions;agent behaviour;self-explanations,,,11.0,,,,,IEEE,IEEE Conferences
299,Physics-Guided Neural Network with Model Discrepancy Based on Upper Troposphere Wind Prediction,K. Fukui; J. Tanaka; T. Tomita; M. Numao,Osaka University; Osaka University; Kumamoto University; Osaka University,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),,2019,,,414,419,"In this paper, we focus on a method that integrates a physical model into a neural network. This study proposes a neural network that can predict two components, namely outputs based on a physical model and its model discrepancy. To achieve such a goal, we propose a novel neural network architecture and associated loss functions designed based on a target physical model. The physical model is used as a regularizer of spatial behavior where output from the neural network is used as an intermediate variable. Then, the model discrepancy is defined as its residual to the observation value. We also propose a network architecture which has Shared and Non-Shared networks, and the neural network can be trained by alternate optimization. We constructed the proposed method with wind prediction in the upper troposphere based on thermal wind equations as an example. The experimental results demonstrate that the proposed method can achieve higher predictive accuracy than normal convolutional neural network or using thermal wind equation, also the obtained model discrepancy expresses convergence and divergence of wind vectors.",,978-1-7281-4550-1,10.1109/ICMLA.2019.00078,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999049,"Deep Neural Network, Model Discrepancy, Theory guided Data Science, Climate Prediction",Mathematical model;Predictive models;Wind forecasting;Neural networks;Data models;Machine learning;Terrestrial atmosphere,atmospheric techniques;atmospheric temperature;neural nets;troposphere;wind,physics-guided neural network;upper troposphere wind prediction;target physical model;NonShared networks;normal convolutional neural network;neural network architecture;thermal wind equations;wind vectors,,,14.0,,,,,IEEE,IEEE Conferences
300,The ABCs of Assured Autonomy,J. M. Mueller,"Johns Hopkins University Applied Physics Laboratory,National Security Analysis Department,Laurel,MD,USA,20723",2019 IEEE International Symposium on Technology and Society (ISTAS),,2019,,,1,5,"Each passing day seems to bring new instances of automation of routine tasks and the addition of artificial intelligence or machine learning algorithms to new domains. Autonomous systems are a diverse class of technologies ranging from AI driven natural language processing and image recognition to closed-form control systems for aircraft autopiloting. Coincident with this explosion in autonomy is a related drumbeat of stories of AI failures. These failures have ranged from tragedies resulting in bystander deaths to humorous examples of video game flaws being exploited. Widespread deployment and use of autonomous systems will depend on society trusting that these systems will perform as expected. While bias from incomplete training data is a well-trod area for improvement toward reducing AI failures, it is unclear if this bias is a sufficient or merely a necessary condition for loss of trust. What else may be needed for public assurance in autonomous systems? Here we identify three features of diverse autonomous systems that serve as a foundation for assured autonomy. These features are: the accuracy with which the algorithm senses and perceives the environment in a manner relatable to humans; a reduction in bias driven by the training data and algorithmic bias; and the complexity of the algorithmic process in terms of the ability to reverse engineer the decision-making processes. Building from this foundation, future autonomous systems can begin to reverse the loss of trust starting to be seen with respect to these technologies.",2158-3412,978-1-7281-5480-0,10.1109/ISTAS48451.2019.8938010,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938010,Assured Autonomy;Artificial Intelligence;Trust;Bias,Autonomous systems;Complexity theory;Training data;Control systems;Automation;Image recognition,learning (artificial intelligence);social aspects of automation;technology acceptance model,assured autonomy;artificial intelligence;AI failures;incomplete training data;public assurance;autonomous systems;routine tasks automation;societal acceptance,,,33.0,,,,,IEEE,IEEE Conferences
302,Technology Assisted Review of Images using Machine Vision,T. Schoinas; D. Esbati,"Data and Technology, Ankura Consulting Group, LLC., London, United Kingdom; Data and Technology, Ankura Consulting Group, LLC., London, United Kingdom",2018 IEEE International Conference on Big Data (Big Data),,2018,,,3310,3316,"Technology Assisted Review, the term used in the legal industry for a family of functions including document clustering and classification using machine learning, has traditionally focused on textual raw data and excluded image files, mainly due to the lack of image analytics technology with the ability to produce satisfactory results. The emergence of deep learning and convolutional neural networks and consequent advances in the field of computer vision have generated the potential to incorporate image processing to legal industry workflows. We exploit the well-known VGG16 model, pretrained with ImageNet pictures, to encode images, which we then cluster using standard methods. Finally, we apply transfer learning to -and touch on fine tuning of - the pretrained network to perform binary classification of a test dataset. We test our methodology on scenarios of similar looking image classes as well as normal and very low responsiveness (positive class) rate. In parallel, we examine the process from a review cost savings perspective.",,978-1-5386-5035-6,10.1109/BigData.2018.8622027,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622027,machine vision;convolutional neural networks;deep learning;technology assisted review;electronic discovery,Feature extraction;Visualization;Training;Law;Industries;Computer vision,computer vision;convolutional neural nets;document image processing;learning (artificial intelligence);pattern classification,image processing;machine learning;image files;document classification;computer vision;convolutional neural networks;deep learning;image analytics technology;textual raw data;document clustering;machine vision;technology assisted review;review cost savings perspective;legal industry workflows,,1.0,12.0,,,,,IEEE,IEEE Conferences
303,Towards Interpretable Object Detection by Unfolding Latent Structures,T. Wu; X. Song,NC State University; None,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,2019,,,6032,6042,"This paper first proposes a method of formulating model interpretability in visual understanding tasks based on the idea of unfolding latent structures. It then presents a case study in object detection using popular two-stage region-based convolutional network (i.e., R-CNN) detection systems. The proposed method focuses on weakly-supervised extractive rationale generation, that is learning to unfold latent discriminative part configurations of object instances automatically and simultaneously in detection without using any supervision for part configurations. It utilizes a top-down hierarchical and compositional grammar model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold the space of latent part configurations of regions of interest (RoIs). It presents an AOGParsing operator that seamlessly integrates with the RoIPooling/RoIAlign operator widely used in R-CNN and is trained end-to-end. In object detection, a bounding box is interpreted by the best parse tree derived from the AOG on-the-fly, which is treated as the qualitatively extractive rationale generated for interpreting detection. In experiments, Faster R-CNN is used to test the proposed method on the PASCAL VOC 2007 and the COCO 2017 object detection datasets. The experimental results show that the proposed method can compute promising latent structures without hurting the performance. The code and pretrained models are available at https://github.com/iVMCL/iRCNN.",2380-7504,978-1-7281-4803-8,10.1109/ICCV.2019.00613,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008311,,Neural networks;Visualization;Grammar;Object detection;Task analysis;Training;Predictive models,convolutional neural nets;grammars;learning (artificial intelligence);object detection;trees (mathematics),region-based convolutional network;directed acyclic AND-OR Graph;interpretable object detection;latent structures;COCO 2017 object detection datasets;qualitatively extractive rationale;latent part configurations;AOG;compositional grammar model;object instances;latent discriminative part configurations;weakly-supervised extractive rationale generation;R-CNN;visual understanding tasks;model interpretability;unfolding latent structures,,,71.0,,,,,IEEE,IEEE Conferences
304,Multimodal Explanations: Justifying Decisions and Pointing to the Evidence,D. H. Park; L. A. Hendricks; Z. Akata; A. Rohrbach; B. Schiele; T. Darrell; M. Rohrbach,NA; NA; NA; NA; NA; NA; NA,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,2018,,,8779,8788,"Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches.",2575-7075,978-1-5386-6420-9,10.1109/CVPR.2018.00915,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579013,,Visualization;Task analysis;Activity recognition;Image segmentation;Knowledge discovery;Predictive models,data visualisation;decision making;explanation;feature extraction;image recognition;information retrieval;learning (artificial intelligence);text analysis,textual explanation;multimodal explanation models;unimodal approaches;deep models;image-based visualization;attention weights;text-based generation;post-hoc justifications;multimodal approach;complementary explanatory strengths;attention visualization;visual justifications;classification decision;activity recognition tasks;visual question answering tasks;textual justification models;visual explanation;decisions justification;textual rationale generation,,16.0,42.0,,,,,IEEE,IEEE Conferences
305,Detect Malicious IP Addresses using Cross-Protocol Analysis,Y. Huang; J. Negrete; A. Wosotowsky; J. Wagener; E. Peterson; A. Rodriguez; C. Fralick,"McAfee LLC,Office Of CTO,Hillsboro,OR,USA,97229; McAfee LLC,Enterprise Business Group,Brentwood,CA,USA,94513; McAfee LLC,Enterprise Business Group,Brentwood,CA,USA,94513; McAfee LLC,Enterprise Business Group,Brentwood,CA,USA,94513; McAfee LLC,Enterprise Business Group,Brentwood,CA,USA,94513; McAfee LLC,Enterprise Business Group,Brentwood,CA,USA,94513; McAfee LLC,Office Of CTO,Lubbock,TX,USA,79403",2019 IEEE Symposium Series on Computational Intelligence (SSCI),,2019,,,664,672,"From the fundamentals of the domain name system (DNS) system, to the websites we browse, the files we download, and emails we receive, every aspect of our online lives involves connections to internet resources. As a result, the Internet protocol (IP) Address is a pivotal component for risk assessment of online exchanges. Our goal in this study is to develop large- scale classification of malicious IPs that leverages cross-protocol telemetry to produce accurate and context-aware risk assessment. We developed an IP reputation system for generic IP addresses based on real-world data. We added interpretability to our machine learning solution to infer a malicious IP address. Our results show that the cross-protocol analysis achieves exceptional testing performance and is effective in real-world application to detect malicious IP addresses.",,978-1-7281-2485-8,10.1109/SSCI44817.2019.9003003,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003003,web security;spam detection;network security;machine learning;Interpretability,IP networks;Network security;Blacklisting;Unsolicited e-mail;Transport protocols;Machine learning;Internet,computer network security;Internet;IP networks;learning (artificial intelligence);transport protocols,cross-protocol analysis;domain name system system;online lives;internet resources;Internet protocol Address;online exchanges;context-aware risk assessment;IP reputation system;generic IP addresses;malicious IP address;cross-protocol telemetry;machine learning solution,,,24.0,,,,,IEEE,IEEE Conferences
306,AIR5: Five Pillars of Artificial Intelligence Research,Y. Ong; A. Gupta,"Agency for Science, Technology and Research (A*STAR), Singapore; Singapore Institute of Manufacturing Technology (SIMTech), Agency for Science, Technology and Research (A*STAR), Singapore",IEEE Transactions on Emerging Topics in Computational Intelligence,,2019,3,5.0,411,415,"In this paper, we provide an overview of what we consider to be some of the most pressing research questions currently facing the fields of artificial and computational intelligence (AI and CI). While AI spans a range of methods that enable machines to learn from data and operate autonomously, CI serves as a means to this end by finding its niche in algorithms that are inspired by complex natural phenomena (including the working of the brain). In this paper, we demarcate the key issues surrounding these fields using five unique Rs, namely, rationalizability, resilience, reproducibility, realism, and responsibility. Notably, just as air serves as the basic element of biological life, the term AIR5-cumulatively referring to the five aforementioned Rs-is introduced herein to mark some of the basic elements of artificial life, for sustainable AI and CI. A brief summary of each of the Rs is presented, highlighting their relevance as pillars of future research in this arena.",2471-285X,,10.1109/TETCI.2019.2928344,Data Science and Artificial Intelligence Research Centre of the School of Computer Science and Engineering; Nanyang Technological University; SIMTech-NTU Joint Lab on Complex Systems; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8782800,Artifical intelligence;rationalizability;realism;reproducibility;resilience;responsibility,Data models;Training;Computational intelligence;Predictive models;Resilience;Artificial neural networks,artificial intelligence;artificial life;learning (artificial intelligence),artificial intelligence research;computational intelligence;complex natural phenomena;unique Rs;basic element;biological life;artificial life;sustainable AI;research questions;AIR5,,,65.0,Traditional,,,,IEEE,IEEE Journals
308,Model-Agnostic Interpretability with Shapley Values,A. Messalas; Y. Kanellopoulos; C. Makris,"Computer Engineering & Informatics Dept. University of Patras,Patras,Greece; Code4Thought,Patras,Greece; Computer Engineering & Informatics Dept. University of Patras,Patras,Greece","2019 10th International Conference on Information, Intelligence, Systems and Applications (IISA)",,2019,,,1,7,"The ability to explain in understandable terms, why a machine learning model makes a certain prediction is becoming immensely important, as it ensures trust and transparency in the decision process of the model. Complex models, such as ensemble or deep learning models, are hard to interpret. Various methods have been proposed that deal with this matter. Shapley values provide accurate explanations, as they assign each feature an importance value for a particular prediction. However, the exponential complexity of their calculation is dealt efficiently only in decision tree-based models. Another method is surrogate models, which emulate a black-box model's behavior and provide explanations effortlessly, since they are constructed to be interpretable. Surrogate models are model-agnostic, but they produce only approximate explanations, which cannot always be trusted. We propose a method that combines these two approaches, so that we can take advantage of the model-agnostic part of the surrogate models, as well as the explanatory power of the Shapley values. We introduce a new metric, Topj Similarity, that measures the similitude of two given explanations, produced by Shapley values, in order to evaluate our work. Finally, we recommend ways on how this method could be improved further.",,978-1-7281-4959-2,10.1109/IISA.2019.8900669,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900669,Machine learning;interpretability;explanations;FATML;XAI;transparency;Shapley values;Surrogate,,decision trees;game theory;learning (artificial intelligence);neural nets,surrogate models;black-box model;Shapley values;model-agnostic interpretability;machine learning model;ensemble learning models;deep learning models;decision tree,,,38.0,,,,,IEEE,IEEE Conferences
310,Is Artificial Intelligence New to Multimedia?,S. Chen,Florida International University,IEEE MultiMedia,,2019,26,2.0,5,7,"The field of multimedia covers a broad range of research and technologies that aim to develop solutions for individual disciplines as well as multidisciplinary domains. From the beginning, multimedia research has employed Artificial Intelligence (AI) techniques to address various challenges in this area. Specifically, AI has been extensively used for multimedia retrieval, management, and analysis. Over the past few years, multimedia has continued to utilize AI to bring revolutionary advancement in numerous applications and domains, mainly thanks to the recent progress in machine learning algorithms and computing powers. However, many challenges remain, which calls for the need to extend Multimedia AI to tackle these problems and to lead to great opportunities in the near future.",1941-0166,,10.1109/MMUL.2019.2914982,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735904,,Artificial intelligence;Multimedia databases;Data models;Multimedia computing;Streaming media;Security,artificial intelligence;multimedia computing,machine learning algorithms;multidisciplinary domains;multimedia research;artificial intelligence techniques;multimedia AI,,,12.0,,,,,IEEE,IEEE Magazines
313,FairSight: Visual Analytics for Fairness in Decision Making,Y. Ahn; Y. Lin,University of Pittsburgh; University of Pittsburgh,IEEE Transactions on Visualization and Computer Graphics,,2020,26,1.0,1086,1095,"Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions - understanding, measuring, diagnosing and mitigating biases - that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.",1941-0506,,10.1109/TVCG.2019.2934262,NSF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805420,Fairness in Machine Learning;Visual Analytic,Decision making;Machine learning;Tools;Pipelines;Visual analytics;Machine learning algorithms;Task analysis,data analysis;data visualisation;decision making;learning (artificial intelligence),fairness-aware decision pipeline;FairSight;data-driven decision making;fair decision making;visual analytic system;ranking decisions;machine learning,,,43.0,,,,,IEEE,IEEE Journals
314,Explainable LSTM Model for Anomaly Detection in HDFS Log File using Layerwise Relevance Propagation,A. Patil; A. Wadekar; T. Gupta; R. Vijan; F. Kazi,"Veermata Jijabai Technological Institute,Centre of Excellence (CoE) in Complex and Non-linear Dynamical Systems (CNDS),Mumbai,India; Veermata Jijabai Technological Institute,Centre of Excellence (CoE) in Complex and Non-linear Dynamical Systems (CNDS),Mumbai,India; Veermata Jijabai Technological Institute,Centre of Excellence (CoE) in Complex and Non-linear Dynamical Systems (CNDS),Mumbai,India; Veermata Jijabai Technological Institute,Centre of Excellence (CoE) in Complex and Non-linear Dynamical Systems (CNDS),Mumbai,India; Veermata Jijabai Technological Institute,Centre of Excellence (CoE) in Complex and Non-linear Dynamical Systems (CNDS),Mumbai,India",2019 IEEE Bombay Section Signature Conference (IBSSC),,2019,,,1,6,"Anomaly detection has always been of utmost importance especially in log file systems. Many different supervised techniques have been explored to deal with this problem. Deep Learning approaches have shown huge promise in log file anomaly detection systems due to their superior ability to learn high level features and non-linearities eliminating the need for any domain specific knowledge or special pre-processing. But this increased performance comes at the cost of inexplicability of the outcomes resulting from the black-box nature of such models. In this paper, we propose a solution utilizing a LSTM-LRP (Long Short Term Memory - Layerwise Relevance Propagation) architecture for discrete event sequences which are obtained by processing log files using log keys derived from individual entries. We extend the idea of LSTM-LRP, used in NLP problems to Log file Systems. The model is evaluated on Hadoop Distributed File System (HDFS) logs where an interpretation for every timestep and every feature is provided. Our major concern in this paper is the interpretation of the results over accuracy of the model. This not only offers an interpretation of the outcomes but also helps build trust in the model by making sure that spurious correlations are avoided making it suitable for real life applications.",,978-1-5386-7401-7,10.1109/IBSSC47189.2019.8973044,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8973044,LSTM;LRP;Anomaly Detection;Big Data;Log Analysis;LSTM-LRP,,distributed databases;explanation;feature extraction;parallel processing;recurrent neural nets;supervised learning,LSTM-LRP;long short term memory;log keys;explainable LSTM model;log file anomaly detection systems;layerwise relevance propagation;HDFS log file;supervised techniques;deep learning;discrete event sequences;Hadoop distributed file system logs;explanable Al,,,22.0,,,,,IEEE,IEEE Conferences
315,Shades of Knowledge-Infused Learning for Enhancing Deep Learning,A. Sheth; M. Gaur; U. Kursuncu; R. Wickramarachchi,University of South Carolina; University of South Carolina; University of South Carolina; University of South Carolina,IEEE Internet Computing,,2019,23,6.0,54,63,"Deep Learning has already proven to be the primary technique to address a number of problems. It holds further promise in solving more challenging problems if we can overcome obstacles, such as the lack of quality training data and poor interpretability. The exploitation of domain knowledge and application semantics can enhance existing deep learning methods by infusing relevant conceptual information into a statistical, data-driven computational approach. This will require resolving the impedance mismatch due to different representational forms and abstractions between symbolic and statistical AI techniques. In this article, we describe a continuum that comprises of three stages for infusion of knowledge into the machine/deep learning architectures. As this continuum progresses across these three stages, it starts with shallow infusion in the form of embeddings, and attention and knowledge-based constraints improve with a semideep infusion. Toward the end reflecting deeper incorporation of knowledge, we articulate the value of incorporating knowledge at different levels of abstractions in the latent layers of neural networks. While shallow infusion is well studied and semideep infusion is in progress, we consider Deep Infusion of Knowledge as a new paradigm that will significantly advance the capabilities and promises of deep learning.",1941-0131,,10.1109/MIC.2019.2960071,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970629,,Task analysis;Computational modeling;Deep learning;Training;Adaptation models;Knowledge engineering,learning (artificial intelligence);neural nets;statistical analysis,symbolic AI techniques;statistical AI techniques;shallow infusion;knowledge-based constraints;semideep infusion;quality training data;poor interpretability;domain knowledge;application semantics;deep learning methods;relevant conceptual information;statistical data-driven computational approach;representational forms;knowledge-infused learning,,1.0,15.0,IEEE,,,,IEEE,IEEE Magazines
316,From Classification to Definition: The Changing Nature of Human Adjudication,R. Singh; A. Ravindra Bodhe; P. Kanuparthi; A. Ananthakrishnan; J. Pitt,"RV Coll. of Eng., Bangalore, India; RV Coll. of Eng., Bangalore, India; RV Coll. of Eng., Bangalore, India; RV Coll. of Eng., Bangalore, India; Intell. & Self-Organizing Syst., Imperial Coll. London, London, UK",IEEE Technology and Society Magazine,,2019,38,4.0,55,62,"It is a feature of social coordination to operate within a system of formalized rules and regulations [1]. The purpose may be to coordinate expectations through norms [2], through the use of language in the context of an institution (Speech Act theory [3]), or to voluntarily constrain behavior in order to resolve a collective action problem, including in an institutional setting [4]. It is also perhaps a unique feature of human ingenuity not only to invent rules, but also to invent prosocial qualities associated with complying with those rules, often in the form of social capital [5].",1937-416X,,10.1109/MTS.2019.2948441,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924581,,Games;Reactive power;Decision making;Classification;Social implications of technology;Social factors,multi-agent systems;social sciences computing,human adjudication;social coordination;formalized rules;Speech Act theory;collective action problem;institutional setting;human ingenuity;prosocial qualities;social capital;changing nature;voluntarily constrain behavior,,,37.0,,,,,IEEE,IEEE Magazines
317,Explaining Distributed Neural Activations via Unsupervised Learning,S. Kolouri; C. E. Martin; H. Hoffmann,"HRL Labs., LLC, Malibu, CA, USA; HRL Labs., LLC, Malibu, CA, USA; HRL Labs., LLC, Malibu, CA, USA",2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,2017,,,1670,1678,"Recent work has demonstrated the emergence of semantic object-part detectors in activation patterns of convolutional neural networks (CNNs), but did not account for the distributed multi-layer neural activations in such networks. In this work, we propose a novel method to extract distributed patterns of activations from a CNN and show that such patterns correspond to high-level visual attributes. We propose an unsupervised learning module that sits above a pre-trained CNN and learns distributed activation patterns of the network. We utilize elastic non-negative matrix factorization to analyze the responses of a pretrained CNN to an input image and extract salient image regions. The corresponding patterns of neural activations for the extracted salient regions are then clustered via unsupervised deep embedding for clustering (DEC) framework. We demonstrate that these distributed activations contain high-level image features that could be explicitly used for image classification.",2160-7516,978-1-5386-0733-6,10.1109/CVPRW.2017.213,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014947,,Feature extraction;Visualization;Unsupervised learning;Laboratories;Object detection;Semantics;Distributed databases,feature extraction;feedforward neural nets;image classification;matrix decomposition;object detection;pattern clustering;unsupervised learning,unsupervised learning;semantic object-part detectors;convolutional neural networks;distributed multilayer neural activations;distributed pattern extraction;high-level visual attributes;unsupervised learning module;pretrained CNN;distributed activation patterns;elastic nonnegative matrix factorization;input image regions;salient image regions;unsupervised deep embedding for clustering framework;high-level image features;image classification;DEC,,1.0,29.0,,,,,IEEE,IEEE Conferences
318,Reinforcement Learning with Explainability for Traffic Signal Control,S. G. Rizzo; G. Vantini; S. Chawla,"Qatar Computing Research Institute (QCRI),Doha,Qatar; Qatar Computing Research Institute (QCRI),Doha,Qatar; Qatar Computing Research Institute (QCRI),Doha,Qatar",2019 IEEE Intelligent Transportation Systems Conference (ITSC),,2019,,,3567,3572,"Deep reinforcement learning has recently provided promising results on the traffic light control optimization problem, by training neural network agents to select the traffic light phase. These agents learn complex models by optimizing a simple objective, such as the average traffic speed, but are considered opaque when it comes to explaining their decisions. Nevertheless, explanations are required in transferring this technology in the real world, especially in complex scenarios with nontrivial phases, such as in the case of signalized roundabouts with entry and circulatory traffic lights. In this paper, after training a Policy Gradient agent on a signalized roundabout with 11 phases and real traffic data, we analyze the relation between the agent phase preferences and the actual traffic, and we assess the agent capability of reacting to the current detectors state. Then, we estimate the effect of the road detectors state on the agent selected phases, through the SHAP model-agnostic technique, using Shapley values recovered from a linear explanation model. The results show that it is possible to extract meaningful explanations on the decision taken by a complex policy, in relation to both the traffic volumes and the lanes occupancy.",,978-1-5386-7024-8,10.1109/ITSC.2019.8917519,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917519,,Detectors;Learning (artificial intelligence);Machine learning;Optimization;Neural networks;Training;Computational modeling,data analysis;learning (artificial intelligence);neural nets;optimisation;road traffic control;road vehicles;signal representation,traffic signal control;deep reinforcement learning;traffic light control optimization problem;nontrivial phases;traffic data analysis;road detectors state;SHAP model-agnostic technique;linear explanation model;neural network agents;policy gradient agent;circulatory traffic lights phase,,,26.0,,,,,IEEE,IEEE Conferences
319,Generalization of Deep Learning for Cyber-Physical System Security: A Survey,C. S. Wickramasinghe; D. L. Marino; K. Amarasinghe; M. Manic,"Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, USA",IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,,2018,,,745,751,"Cyber-Physical Systems (CPSs)have become ubiquitous in recent years and has become the core of modern critical infrastructure and industrial applications. Therefore, ensuring security is a prime concern. Due to the success of Deep Learning (DL)in a multitude of domains, development of DL based CPS security applications have received increased interest in the past few years. Developing generalized models is critical since the models have to perform well under threats that they havent trained on. However, despite the broad body of work on using DL for ensuring the security of CPSs, to our best knowledge very little work exists where the focus is on the generalization capabilities of these DL applications. In this paper, we intend to provide a concise survey of the regularization methods for DL algorithms used in security-related applications in CPSs and thus could be used to improve the generalization capability of DL based cyber-physical system based security applications. Further, we provide a brief insight into the current challenges and future directions as well.",2577-1647,978-1-5090-6684-1,10.1109/IECON.2018.8591773,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8591773,Generalization;Deep Neural networks;Regularization;Cyber- Physical Systems;Cyber Security,Neural networks;Malware;Cyber-physical systems;Data models,critical infrastructures;generalisation (artificial intelligence);learning (artificial intelligence);security of data,critical infrastructure;security-related applications;DL applications;generalization capability;generalized models;DL based CPS security applications;industrial applications;CPSs;cyber-physical system security;Deep Learning,,5.0,60.0,,,,,IEEE,IEEE Conferences
320,"Edge Intelligence: The Convergence of Humans, Things, and AI",T. Rausch; S. Dustdar,TU Wien; TU Wien,2019 IEEE International Conference on Cloud Engineering (IC2E),,2019,,,86,96,"Edge AI and Human Augmentation are two major technology trends, driven by recent advancements in edge computing, IoT, and AI accelerators. As humans, things, and AI continue to grow closer together, systems engineers and researchers are faced with new and unique challenges. In this paper, we analyze the role of edge computing and AI in the cyber-human evolution, and identify challenges that edge computing systems will consequently be faced with. We take a closer look at how a cyber-physical fabric will be complemented by AI operationalization to enable seamless end-to-end edge intelligence systems.",,978-1-7281-0218-4,10.1109/IC2E.2019.00022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789967,edge AI;human augmentation;edge intelligence;edge computing;AI systems;AI operations,Artificial intelligence;Cloud computing;Computational modeling;Sensors;Data models;Edge computing;Urban areas,artificial intelligence;Internet of Things;systems engineering,edge computing;AI accelerators;systems engineers;cyber-human evolution;AI operationalization;seamless end-to-end edge intelligence systems;edge AI;Human Augmentation,,,53.0,,,,,IEEE,IEEE Conferences
321,Ethics of Artificial Intelligence: Challenges,M. Piteira; M. Aparicio; C. J. Costa,"Instituto Universitário de Lisboa (ISCTE-IUL), ISTAR-IUL, Instituto Politécnico de Setúbal, IPS, Setúbal, Portugal; Information Management School(Nova IMS), Universidade Nova de Lisboa, Instituto Universitario de Lisboa (ISCTE-IUL) ISTAR-IUL, Portugal; Universidade de Lisboa, Advance/CSG, ISEG (Lisbon School of Economics & Management)",2019 14th Iberian Conference on Information Systems and Technologies (CISTI),,2019,,,1,6,"Artificial intelligence (AI) has in recent times assumed a relevant role in the most diverse sectors of our society. We are at a no return point, and our future will incorporate artificial intelligence into our everyday life, professional or personal. The idea of “thinking” machines existence, making decisions by Humans raises several ethical questions. It is fundamental to study and investigate the best approaches to their integration. This article identifies the guiding principles of ethics in the context of using intelligent and autonomous systems. Here we present a bibliometric study, reporting the main ACM and IEEE studies on Ethics and AI. Our results indicate various clusters of Ethics and AI, that the scientific community has been focused on.",2166-0727,978-9-8998-4349-3,10.23919/CISTI.2019.8760826,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8760826,Ethics;Artificial Intelligence;Framework;Bibliometric Study,Ethics;Standards;Bibliometrics;Machine learning;Monitoring;Machinery,artificial intelligence;ethical aspects;information analysis,AI;artificial intelligence;ethical questions;intelligent systems;autonomous systems;thinking machines;bibliometric study,,,,,,,,IEEE,IEEE Conferences
323,Latent Class Multi-Label Classification to Identify Subclasses of Disease for Improved Prediction,A. Alsaid Alyousef; S. Nihtyanova; C. P. Denton; P. Bosoni; R. Bellazzi; A. Tucker,"Brunel University London; UCL Royal Free Hospital, London, UK; UCL Royal Free Hospital, London, UK; University of Pavia, Italy; University of Pavia, Italy; Brunel University London",2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),,2019,,,535,538,"Disease subtyping can assist the development of precision medicine but remains a challenge in data analysis by reason of the many different methods to group individuals depending on their data. However, identification of subclasses of disease will help to produce better models which are more specific to patients and will improve prediction and interpretation of underlying characteristics of disease. This paper presents a novel algorithm that integrates latent class models with supervised learning. The new algorithm uses latent class models to cluster patients within groups that results in improved classification as well as aiding the understanding of the dissimilarities of the discovered groups. The methods are tested on data from patients with Systemic Sclerosis (SSc), a rare potentially fatal condition. Results show that the ""Latent Class Multi-Label Classification Model"" improves accuracy when compared with competitive similar methods.",2372-9198,978-1-7281-2286-1,10.1109/CBMS.2019.00109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787428,Multi-Label Classification;Latent Class Model;Naïve Bayes,Prediction algorithms;Classification algorithms;Predictive models;Clustering algorithms;Diseases;Skin;Medical diagnostic imaging,data analysis;diseases;learning (artificial intelligence);medicine;pattern classification;pattern clustering,disease subtyping;precision medicine;data analysis;latent class multilabel classification model;patients clustering;supervised learning,,,16.0,,,,,IEEE,IEEE Conferences
324,"AI, Connectivity and Cyber-Security in Avionics",M. GATTI; A. DAMIEN,"THALES Avionics, SAS, France; THALES Avionics, SAS, France",2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),,2019,,,35,38,"This paper introduces the three key pillars for the avionics platform of the future, and presents the associated challenges. The term “Avionics Platform” addresses here includes (1) all the embedded systems required by an aircraft to fly (whatever the aircraft is, commercial, fighter, transport, drone or flying cabs) and (2) all the embedded critical and less critical functions required for flying (flight command control, propulsion management), piloting (fuel management, anti-icing, TCAS, TAWS, etc.), utilities (doors and slides, toilets, air conditioning, etc.), maintenance, and connectivity. AI, Connectivity and Cyber-Security in Avionics are the three pillar and main challenges we face. In this paper, we present the current status of our work.",1946-0759,978-1-7281-0303-7,10.1109/ETFA.2019.8869381,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869381,avionics;artificial intelligence;connectivity;cyber-security,Aerospace electronics;Aircraft;Safety;Machine learning;Learning automata;Aerospace control,aerospace computing;artificial intelligence;avionics;security of data,cyber-security;avionics platform;aircraft;drone;embedded critical functions;flight command control;propulsion management;fuel management;anti-icing;AI,,,7.0,,,,,IEEE,IEEE Conferences
325,Multi-Channel Deep Feature Learning for Intrusion Detection,G. Andresini; A. Appice; N. D. Mauro; C. Loglisci; D. Malerba,"Department of Computer Science, University of Bari Aldo Moro, Bari, Italy; Department of Computer Science, University of Bari Aldo Moro, Bari, Italy; Department of Computer Science, University of Bari Aldo Moro, Bari, Italy; Department of Computer Science, University of Bari Aldo Moro, Bari, Italy; Department of Computer Science, University of Bari Aldo Moro, Bari, Italy",IEEE Access,,2020,8,,53346,53359,"Networks had an increasing impact on modern life since network cybersecurity has become an important research field. Several machine learning techniques have been developed to build network intrusion detection systems for correctly detecting unforeseen cyber-attacks at the network-level. For example, deep artificial neural network architectures have recently achieved state-of-the-art results. In this paper a novel deep neural network architecture is defined, in order to learn flexible and effective intrusion detection models, by combining an unsupervised stage for multi-channel feature learning with a supervised one exploiting feature dependencies on cross channels. The aim is to investigate whether class-specific features of the network flows could be learned and added to the original ones in order to increase the model accuracy. In particular, in the unsupervised stage, two autoencoders are separately learned on normal and attack flows, respectively. As the top layer in the decoder of these autoencoders reconstructs samples in the same space as the input one, they could be used to define two new feature vectors allowing the representation of each network flow as a multi-channel sample. In the supervised stage, a multi-channel parametric convolution is adopted, in order to learn the effect of each channel on the others. In particular, as the samples belong to two different distributions (normal and attack flows), the samples labelled as normal should be more similar to the representation reconstructed with the normal autoencoder than that of the attack one, and viceversa. This expected dependency will be exploited to better disentangle the differences between normal and attack flows. The proposed neural network architecture leads to better predictive accuracy when compared to competitive intrusion detection architectures on three benchmark datasets.",2169-3536,,10.1109/ACCESS.2020.2980937,research project “TALIsMan-Tecnologie di Assistenza personALizzata per il Miglioramento della quAlità della vitA”; Italian Ministry for Universities and Research (MIUR); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9036935,Cybersecurity;intrusion detection;machine learning;computer security,Feature extraction;Intrusion detection;Deep learning;Neural networks;Computer architecture;Decoding,neural nets;security of data;unsupervised learning,competitive intrusion detection architectures;multichannel deep feature learning;network cybersecurity;machine learning techniques;network intrusion detection systems;unforeseen cyber-attacks;network-level;deep artificial neural network architectures;unsupervised stage;multichannel feature;cross channels;class-specific features;model accuracy;feature vectors;network flow;multichannel sample;supervised stage;multichannel parametric convolution;normal autoencoder;intrusion detection models,,,65.0,CCBY,,,,IEEE,IEEE Journals
326,Explaining Explanations: An Overview of Interpretability of Machine Learning,L. H. Gilpin; D. Bau; B. Z. Yuan; A. Bajwa; M. Specter; L. Kagal,"Comput. Sci. & Artificial Intell. Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA; Comput. Sci. & Artificial Intell. Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA; Comput. Sci. & Artificial Intell. Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA; Comput. Sci. & Artificial Intell. Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA; Comput. Sci. & Artificial Intell. Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA; Comput. Sci. & Artificial Intell. Lab., Massachusetts Inst. of Technol., Cambridge, MA, USA",2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA),,2018,,,80,89,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",,978-1-5386-5090-5,10.1109/DSAA.2018.00018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8631448,Machine learning theories;Models and systems;Deep learning and deep analytics;Fairness and transparency in data science,Artificial intelligence;Computational modeling;Decision trees;Biological neural networks;Taxonomy;Complexity theory,artificial intelligence;data analysis;learning (artificial intelligence);neural nets,complex machines;algorithmic fairness;training data;suggested future research directions;explanatory artificial intelligence;explaining explanations;machine learning;XAI;potential bias-problems,,8.0,87.0,,,,,IEEE,IEEE Conferences
327,Shoplifting Smart Stores using Adversarial Machine Learning,M. Nassar; A. Itani; M. Karout; M. El Baba; O. A. S. Kaakaji,"Department of Computer Science, Faculty of Arts and Sciences, American University of Beirut (AUB), Beirut, Lebanon; Department of Computer Science, Faculty of Arts and Sciences, American University of Beirut (AUB), Beirut, Lebanon; Department of Computer Science, Faculty of Arts and Sciences, American University of Beirut (AUB), Beirut, Lebanon; Department of Computer Science, Faculty of Arts and Sciences, American University of Beirut (AUB), Beirut, Lebanon; Department of Computer Science, Faculty of Arts and Sciences, American University of Beirut (AUB), Beirut, Lebanon",2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA),,2019,,,1,6,"Smart stores cashier-less technology is partially based on camera-equipped object detection systems. Powerful machine learning algorithms are deployed at the back-end for classification. In this paper, we explore the usage of adversarial machine learning techniques to deceive the smart stores' classifiers. In particular, we experiment with printable adversarial patches and target making an expensive item classified as a cheaper one. By sticking patches to the objects and lifting them, a customer can make her customized discounts and alter the machine learning prediction. We discuss experiments, results, and possible countermeasures.",2161-5330,978-1-7281-5052-9,10.1109/AICCSA47632.2019.9035236,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035236,Smart Stores;Adversarial Machine Learning;Adversarial Patch;Deep Learning;Classification;Convolutional Neural Networks;Object Recognition,,cameras;convolutional neural nets;image classification;learning (artificial intelligence);object detection,printable adversarial patches;shoplifting smart stores;adversarial machine learning;smart stores cashier-less technology;camera-equipped object detection systems;convolutional neural networks;expensive item classification,,,20.0,,,,,IEEE,IEEE Conferences
329,Understanding the Student Dropout in Distance Learning,M. M. d. Oliveira; R. Barwaldt; M. R. Pias; D. B. Espíndola,"Federal University of Rio Grande (FURG),Center for Computational Sciences (C3),Rio Grande-RS,Brazil; Federal University of Rio Grande (FURG),Center for Computational Sciences (C3),Rio Grande-RS,Brazil; Federal University of Rio Grande (FURG),Center for Computational Sciences (C3),Rio Grande-RS,Brazil; Federal University of Rio Grande (FURG),Center for Computational Sciences (C3),Rio Grande-RS,Brazil",2019 IEEE Frontiers in Education Conference (FIE),,2019,,,1,7,"This Research to Practice Full Paper introduces an approach for the early identification of students at risk of dropping out of their distance learning courses. Students dropout in university courses have long been a major issue leading to social, academic and financial impacts. Predictive modelling analysis has been used as a tool to identify at an early stage probable cases of dropout. In distance learning, this type of analysis is made possible mostly because of the increasing adoption rate of Virtual Learning Environments (VLEs) where data on the student academic activities can be continuously recorded. This paper discusses the design and validation of a Learning Analytics system for early identification of students at risk of dropping out. The case study presented relies on data collected from two postgraduate courses as part of Brazils Open University. The methodology for the system design and validation comprises the steps to build an intelligent data pipeline. Jupyter Notebooks have been d used as the data science analysis environment in order to create data pipelines and have their performance evaluated. Results obtained from the validation of models built out of eight machine learning techniques show an average accuracy of 84% among the set of ML techniques tested. The highest accuracy is delivered by the Extra Trees classifier with 88%. Logistic Regression performed the worst performance with an accuracy of 79%.",2377-634X,978-1-7281-1746-1,10.1109/FIE43999.2019.9028433,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9028433,Predictive Modelling;Dropout;Distance Education;Machine Learning,Pipelines;Computer aided instruction;Predictive models;Task analysis;Data models;Machine learning;Logistics,distance learning;educational courses;educational institutions;probability;regression analysis,distance learning courses;students dropout;university courses;social impacts;academic impacts;financial impacts;predictive modelling analysis;student academic activities;learning analytics system;postgraduate courses;system design;intelligent data pipeline;data science analysis environment;data pipelines;machine learning techniques;Brazils open university;virtual learning environments;adoption rate,,,22.0,,,,,IEEE,IEEE Conferences
330,ByteWise: A case study in neural network obfuscation identification,L. Jones; D. Christman; S. Banescu; M. Carlisle,"United States Air Force Academy, Academy Center for Cyberspace Research, Colorado Springs, United States of America; United States Air Force Academy, Machine Autonomy Reasoning Vision and Learning Laboratory, Colorado Springs, United States of America; Technische Universität München, Fakultät für Informatik, Munich, Germany; Carnegie Mellon University, Information Networking Institute, Pittsburgh, United States of America",2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC),,2018,,,155,164,"Researchers taking advantage of recent advancements in neural networks have made leaps in many fields such as image recognition, natural language processing, and speech recognition. However, little work has been done with neural networks in the field of binary analysis. Recently, researchers have used neural networks to recognize function boundaries in binaries, using only the bytes of the programs as features. In this paper, we extend their work to detect the bytes of bogus basic blocks added in the dead branches of opaque predicates. We perform a case study using the bogus control flow transformation offered by Obfuscator-LLVM. We detect the bytes of bogus basic blocks with a 94% F1 score. This information can be used to prune code for static reverse engineering. We believe this line of research will yield optimized triage, reverse engineering tools, and malware detection based on obfuscation identification using neural networks.",,978-1-5386-4649-6,10.1109/CCWC.2018.8301720,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301720,obfuscation identification;neural networks;software obfuscation,Malware;Biological neural networks;Reverse engineering;Measurement;Computer crime;Semantics,invasive software;neural nets;program diagnostics;reverse engineering,neural network obfuscation identification;bogus basic blocks;ByteWise;image recognition;natural language processing;speech recognition;binary analysis;function boundaries;opaque predicates;Obfuscator-LLVM;F1 score;static reverse engineering;malware detection,,1.0,50.0,,,,,IEEE,IEEE Conferences
331,Detection of Research Front Topic Based on Data of NSF Artificial Intelligence Project,L. Wen; W. Zhang; X. Jin; Z. Liu,Beijing Science and Technology Information Research Institute; Beijing Science and Technology Information Research Institute; Beijing Science and Technology Information Research Institute; University of Chinese Academy of Sciences,2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM),,2019,,,82,86,"Artificial intelligence in the United States has been the world's leading, and the detection of research front topic based on the data of NSF artificial intelligent projects is helpful for us to obtain quantitative information for prioritized technologies that could be used for technology management and decision making for research funding and technology investment in the field of artificial intelligent. PLDA model was used to identify topic contained in the NSF project data in the field of artificial intelligence from 2013 to 2018. The funding time, funding amount and centrality indexes of the topic were taken as three indexes to judge research front topics. Threshold value was set according to Pareto Law. And 17 research front topics in the artificial intelligence field were determined. The results showed that 13 aspects, including intelligent system and intelligent community, quantum technology, molecular programming, intelligent simulation, underwater communication and positioning, big data analysis and so on, are the main research front contents concerned in the field of artificial intelligence in the United State.",,978-1-7281-4691-1,10.1109/AIAM48774.2019.00023,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950768,"National Science Foundation, Fund project, PLDA, Artificial intelligence, Research front",,artificial intelligence;Big Data;data analysis;decision making;information analysis;Internet;Pareto analysis;research and development;technology management;text analysis,NSF project data;centrality indexes;artificial intelligence field;intelligent system;intelligent community;intelligent simulation;main research front contents;United States;NSF artificial intelligence project;NSF artificial intelligent projects;technology management;decision making;research funding;technology investment;research front topic detection;PLDA model;Pareto Law;quantum technology;molecular programming;underwater positioning;big data analysis,,,15.0,,,,,IEEE,IEEE Conferences
333,Utilizing a Transparency-Driven Environment Toward Trusted Automatic Genre Classification: A Case Study in Journalism History,A. Bilgin; E. Tjong Kim Sang; K. Smeenk; L. Hollink; J. van Ossenbruggen; F. Harbers; M. Broersma,"Jacco van Ossenbruggen CWI, Amsterdam, NY, USA; Jacco van Ossenbruggen CWI, Amsterdam, NY, USA; Netherlands eScience Center, Amsterdam, Netherlands; Jacco van Ossenbruggen CWI, Amsterdam, NY, USA; Jacco van Ossenbruggen CWI, Amsterdam, NY, USA; Marcel Broersma, Univ. of Groningen, Groningen, Netherlands; Marcel Broersma, Univ. of Groningen, Groningen, Netherlands",2018 IEEE 14th International Conference on e-Science (e-Science),,2018,,,486,496,"With the growing abundance of unlabeled data in real-world tasks, researchers have to rely on the predictions given by black-boxed computational models. However, it is an often neglected fact that these models may be scoring high on accuracy for the wrong reasons. In this paper, we present a practical impact analysis of enabling model transparency by various presentation forms. For this purpose, we developed an environment that empowers non-computer scientists to become practicing data scientists in their own research field. We demonstrate the gradually increasing understanding of journalism historians through a real-world use case study on automatic genre classification of newspaper articles. This study is a first step towards trusted usage of machine learning pipelines in a responsible way.",,978-1-5386-9156-4,10.1109/eScience.2018.00137,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588756,machine learning;transparency;journalism history;genre classification,Pipelines;Machine learning;Data models;Task analysis;Predictive models;Feature extraction;Natural language processing,history;journalism;learning (artificial intelligence);pattern classification,journalism history;unlabeled data;black-boxed computational models;practical impact analysis;model transparency;presentation forms;noncomputer scientists;journalism historians;transparency-driven environment;trusted automatic genre classification;data scientists;machine learning pipelines,,,28.0,,,,,IEEE,IEEE Conferences
334,"Data-Driven Decision-Making (D3M): Framework, Methodology, and Directions",J. Lu; Z. Yan; J. Han; G. Zhang,"Decision Systems and e-Service Intelligence Lab, Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia; Decision Systems and e-Service Intelligence Lab, Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia; Decision Systems and e-Service Intelligence Lab, Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia; Decision Systems and e-Service Intelligence Lab, Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia",IEEE Transactions on Emerging Topics in Computational Intelligence,,2019,3,4.0,286,296,"A decision problem, according to traditional principles, is approached by finding an optimal solution to an analytical programming decision model, which is known as model-driven decision-making. The fidelity of the model determines the quality and reliability of the decision-making; however, the intrinsic complexity of many real-world decision problems leads to significant model mismatch or infeasibility in deriving a model using the first principle. To overcome the challenges that are present in the big data era, both researchers and practitioners emphasize the importance of making decisions that are backed up by data related to decision tasks, a process called data-driven decision-making (D3M). By building on data science, not only can decision models be predicted in the presence of uncertainty or unknown dynamics, but also inherent rules or knowledge can be extracted from data and directly utilized to generate decision solutions. This position paper systematically discusses the basic concepts and prevailing techniques in data-driven decision-making and clusters-related developments in technique into two main categories: programmable data-driven decision-making (P-D3M) and nonprogrammable data-driven decision-making (NP-D3M). This paper establishes a D3M technical framework, main methodologies, and approaches for both categories of D3M, as well as identifies potential methods and procedures for using data to support decision-making. It also provides examples of how D3M is implemented in practice and identifies five further research directions in the D3M area. We believe that this paper will directly support researchers and professionals in their understanding of the fundamentals of D3M and of the developments in technical methods",2471-285X,,10.1109/TETCI.2019.2915813,Australian Research Council; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8732997,Data-driven decision-making;decision support systems;computational intelligence,Decision making;Data models;Computational modeling;Predictive models;Optimization;Computational intelligence;Neural networks,Big Data;decision making;decision support systems,decision solutions;programmable data-driven decision-making;decision problem;analytical programming decision model;model-driven decision-making;real-world decision problems;decision tasks;decision models;D3M;data science,,,89.0,,,,,IEEE,IEEE Journals
335,Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neural Networks,D. Kumar; A. Wong; G. W. Taylor,"Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Guelph, Guelph, ON, Canada",2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,2017,,,1686,1694,"In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an approach to visualize and understand the decisions made by deep neural networks (DNNs) given a specific input. CLEAR facilitates the visualization of attentive regions and levels of interest of DNNs during the decision-making process. It also enables the visualization of the most dominant classes associated with these attentive regions of interest. As such, CLEAR can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity, and allows for better insights into the decision-making process of DNNs. Quantitative and qualitative experiments across three different datasets demonstrate the efficacy of CLEAR for gaining a better understanding of the inner workings of DNNs during the decision-making process.",2160-7516,978-1-5386-0733-6,10.1109/CVPRW.2017.215,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014949,,Decision making;Heating systems;Kernel;Visualization;Image color analysis;Convolution;Neural networks,data visualisation;decision making;image classification;neural nets,dominant class visualization;decision-making process;DNN;attentive region visualization;CLEAR;deep neural networks;class-enhanced attentive response,,7.0,19.0,,,,,IEEE,IEEE Conferences
336,Challenges and Research Directions for Blockchains in the Internet of Things,F. Golatowski; B. Butzin; T. Brockmann; T. Schulz; M. Kasparick; Y. Li; R. Rahmani; A. Haber; M. Sakalsız; Ö. Aydemir,"Institute of Applied Microelectronics and Computer Engineering, University of Rostock, Germany; Institute of Applied Microelectronics and Computer Engineering, University of Rostock, Germany; Institute of Applied Microelectronics and Computer Engineering, University of Rostock, Germany; Institute of Applied Microelectronics and Computer Engineering, University of Rostock, Germany; Institute of Applied Microelectronics and Computer Engineering, University of Rostock, Germany; Institutionen för data- och systemvetenskap, Stockholms Universitet, Kista, Sweden; Institutionen för data- och systemvetenskap, Stockholms Universitet, Kista, Sweden; Mavennet Systems Inc., Toronto, Canada; T2 Software, Ankara, Turkey; IOTIQ, Leipzig, Germany",2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),,2019,,,712,717,"In this paper, we analyze the state of the art in distributed ledger technologies and blockchains and investigate potential applications in the Internet of Things (IoT) domain. Afterwards, we discuss interoperability of blockchains, and their use in smart contracts, and artificial intelligence.",,978-1-5386-8500-6,10.1109/ICPHYS.2019.8780270,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8780270,blockchain;Internet of Things;distributed ledger technology;industry 4.0;artificial intelligence,Blockchain;Internet of Things;Distributed ledger;Security;Distributed databases;Smart contracts;Peer-to-peer computing,artificial intelligence;contracts;cryptocurrencies;distributed databases;Internet of Things;open systems,artificial intelligence;smart contracts;interoperability;IoT domain;Internet of Things domain;distributed ledger technologies;blockchains,,,29.0,,,,,IEEE,IEEE Conferences
338,Visual Interaction with Deep Learning Models through Collaborative Semantic Inference,S. Gehrmann; H. Strobelt; R. Krüger; H. Pfister; A. M. Rush,Harvard NLP group; IBM Research Cambridge; Harvard Visual Computing group; Harvard Visual Computing group; Harvard NLP group,IEEE Transactions on Visualization and Computer Graphics,,2020,26,1.0,884,894,"Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.",1941-0506,,10.1109/TVCG.2019.2934595,NIH; AWS; Google Faculty Research; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805457,Human-Computer Collaboration;Deep Learning;Neural Networks;Interaction Design;Human-Centered Design,Visualization;Collaboration;Semantics;Tools;Analytical models;Cognition;Predictive models,data visualisation;groupware;human computer interaction;inference mechanisms;learning (artificial intelligence),reasoning process;visual metaphors;semantic interactions;visual collaboration;interaction design;deep learning systems;visual interface;explainable reasoning;black-box approaches;collaborative semantic inference;deep learning models;visual interaction,,,92.0,,,,,IEEE,IEEE Journals
339,Intelligent Architecture for Mobile HetNet in B5G,W. Chien; H. Cho; C. Lai; F. Tseng; H. Chao; M. M. Hassan; A. Alelaiwi,"NA; Dept. of Appl. Math., Hsuan Chuang Univ., Hsinchu, Taiwan; NA; NA; NA; Inf. Syst. Dept., King Saud Univ., Riyadh, Saudi Arabia; NA",IEEE Network,,2019,33,3.0,34,41,"Since traffic in networks is growing rapidly, it is difficult for the existing network architecture to support the huge traffic requirement. This article proposes a novel intelligent architecture as a promising paradigm for B5G heterogeneous networks to optimize network resource usage and network performance. The main idea is to build a suitable network model through AI, and integrate edge computing and cloud computing to improve computing performance. In addition, this article gives appropriate recommendations of the deep learning method for different network issues. Since the deep learning method requires a large amount of computing resources, the network resource allocation needs to be paid attention to in this architecture. For complex environments of B5G heterogeneous networks, integrated packet forwarding is one potential technology to improve quality of service. Moreover, we discuss the challenges and open issues for B5G.",1558-156X,,10.1109/MNET.2019.1800364,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726070,,Computer architecture;Resource management;Servers;Cloud computing;Computational modeling;Edge computing;Data models;Machine learning,5G mobile communication;cloud computing;learning (artificial intelligence);mobile computing;quality of service;resource allocation;telecommunication computing;telecommunication traffic,B5G heterogeneous networks;network resource usage;network performance;suitable network model;edge computing;computing performance;deep learning method;computing resources;network resource allocation;intelligent architecture;mobile HetNet;huge traffic requirement;cloud computing;network issues;network architecture;integrated packet forwarding;quality of service,,,15.0,,,,,IEEE,IEEE Magazines
341,From logs to Stories: Human-Centred Data Mining for Cyber Threat Intelligence,N. Afzaliseresht; Y. Miao; S. Michalska; Q. Liu; H. Wang,"Institute for Sustainable Industries and Liveable cities, Victoria University, Melbourne, VIC, Australia; Institute for Sustainable Industries and Liveable cities, Victoria University, Melbourne, VIC, Australia; Institute for Sustainable Industries and Liveable cities, Victoria University, Melbourne, VIC, Australia; The Commonwealth Scientific and Industrial Research Organization (CSIRO), Hobart, TAS, Australia; Institute for Sustainable Industries and Liveable cities, Victoria University, Melbourne, VIC, Australia",IEEE Access,,2020,8,,19089,19099,"An average medium-sized organisation logs approx. 10 to 500 mln events per day on the system. Only less than 5% of threat alerts are being investigated by the specialised staff, leaving the security hole open for potential attacks. Insufficient information in alert message produced in machine-friendly rather than human-friendly format causes cognitive overload on currently limited cybersecurity resources. In this paper, the model that generates the report in natural language by means of applying novel storytelling techniques from security logs is proposed. The solution caters for different levels of reader expertise and preference by providing adjustable templates, filled from both local and global knowledge base. The validation is performed on case study from Security Operations Centre (SOC) at educational institution. The report generated proves superior to existing approach in terms of comprehension (increased cognition) and completeness (enriched context). The evaluation demonstrates power of storytelling in potential threats interpretation in cybersecurity context.",2169-3536,,10.1109/ACCESS.2020.2966760,Australian Government Research Training Program Scholarship; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8960350,Cybersecurity;storytelling;threat intelligence;human cognition;information extraction;knowledge Discovery,,cognition;data mining;natural language processing;security of data,human-centred data mining;cyber threat intelligence;threat alerts;alert message;cybersecurity resources;natural language;storytelling techniques;security logs;knowledge base;Security Operations Centre,,,34.0,CCBY,,,,IEEE,IEEE Journals
342,Towards the Integration of a Post-Hoc Interpretation Step into the Machine Learning Workflow for IoT Botnet Detection,S. Nõmm; A. Guerra-Manzanares; H. Bahsi,Tallinn University of Technology; Tallinn University of Technology; Tallinn University of Technology,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),,2019,,,1162,1169,"The analysis of the interplay between the feature selection and the post-hoc local interpretation steps in a machine learning workflow followed for IoT botnet detection constitutes the research scope of the present paper. While the application of machine learning-based techniques has become a trend in cyber security, the main focus has been almost on detection accuracy. However, providing the relevant explanation for a detection decision is a vital requirement in a tiered incident handling processes of the contemporary security operations centers. Moreover, the design of intrusion detection systems in IoT networks has to take the limitations of the computational resources into consideration. Therefore, resource limitations in addition to human element of incident handling necessitate considering feature selection and interpretability at the same time in machine learning workflows. In this paper, first, we analyzed the selection of features and its implication on the data accuracy. Second, we investigated the impact of feature selection on the explanations generated at the post-hoc interpretation phase. We utilized a filter method, Fisher's Score and Local Interpretable Model-Agnostic Explanation (LIME) at feature selection and post-hoc interpretation phases, respectively. To evaluate the quality of explanations, we proposed a metric that reflects the need of the security analysts. It is demonstrated that the application of both steps for the particular case of IoT botnet detection may result in highly accurate and interpretable learning models induced by fewer features. Our metric enables us to evaluate the detection accuracy and interpretability in an integrated way.",,978-1-7281-4550-1,10.1109/ICMLA.2019.00193,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999281,Botnet detection;machine learning;interpretation,Feature extraction;Machine learning;Measurement;Botnet;Intrusion detection;Computer security,computer network security;feature selection;Internet of Things;invasive software;learning (artificial intelligence),feature selection;machine learning workflow;Local Interpretable Model-Agnostic Explanation;IoT botnet detection;highly accurate learning models;interpretable learning models;detection accuracy;machine learning-based techniques;detection decision;security operations centers;intrusion detection systems;IoT networks;post-hoc local interpretation;Fisher score;post-hoc interpretation;cyber security;tiered incident handling process;security analysis;data accuracy;computational resources,,,23.0,,,,,IEEE,IEEE Conferences
343,Dermoscopy Image Analysis: Overview and Future Directions,M. E. Celebi; N. Codella; A. Halpern,"Department of Computer Science, University of Central Arkansas, Conway, AR, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Dermatology Service, Memorial Sloan Kettering Cancer Center, New York, NY, USA",IEEE Journal of Biomedical and Health Informatics,,2019,23,2.0,474,478,"Dermoscopy is a non-invasive skin imaging technique that permits visualization of features of pigmented melanocytic neoplasms that are not discernable by examination with the naked eye. While studies on the automated analysis of dermoscopy images date back to the late 1990s, because of various factors (lack of publicly available datasets, open-source software, computational power, etc.), the field progressed rather slowly in its first two decades. With the release of a large public dataset by the International Skin Imaging Collaboration in 2016, development of opensource software for convolutional neural networks, and the availability of inexpensive graphics processing units, dermoscopy image analysis has recently become a very active research field. In this paper, we present a brief overview of this exciting subfield of medical image analysis, primarily focusing on three aspects of it, namely, segmentation, feature extraction, and classification. We then provide future directions for researchers.",2168-2208,,10.1109/JBHI.2019.2895803,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8627921,Skin cancer;melanoma;dermoscopy;computer-aided diagnosis;dermoscopy image analysis,Lesions;Skin;Melanoma;Feature extraction;Biomedical imaging;Image segmentation,biomedical optical imaging;cancer;convolutional neural nets;feature extraction;image classification;image segmentation;medical image processing;skin,dermoscopy image analysis;noninvasive skin imaging technique;pigmented melanocytic neoplasms;automated analysis;dermoscopy images;open-source software;International Skin Imaging Collaboration;opensource software;medical image analysis;feature extraction;image segmentation;image classification;convolutional neural networks,,5.0,90.0,,,,,IEEE,IEEE Journals
344,The Fuzzy Emotion Recognition Framework Using Semantic-Linguistic Facial Features,D. Y. Liliana; T. Basaruddin,"State Polytechnic of Jakarta,Department of Informatics and Computer,Depok,Indonesia; Faculty of Computer Science, Universitas Indonesia,Depok,Indonesia",2019 IEEE R10 Humanitarian Technology Conference (R10-HTC)(47129),,2019,,,263,268,"Emotion recognition through facial expression analysis is an emerging research in Artificial Intelligence which faces many challenges. The problem is the variation of facial expressions that displays human emotions. Humans can subjectively express the same emotions in various ways. To overcome the problem of ambiguity in emotion expression, a fuzzy approach is developed to analyze the facial components in determining the type of emotion. In this study, we proposed a framework for fuzzy emotion recognition as a representation of the psychologist knowledge. Three stages in the fuzzy emotion recognition were facial feature extraction with Active Appearance Model; Semantic-linguistic facial features extraction; fuzzy emotion recognition with Fuzzy Emotion Classification. System performance testing provided the best results on extended Cohn Kanade (CK+) facial expression dataset, with the accuracy of linguistic facial component recognition 0.98, and accuracy of fuzzy emotion recognition 0.90. Testing was also performed on custom-made Indonesian Mixed Emotion Dataset (IMED) which resulted in accuracy of 0.87. The fuzzy emotion recognition has a potential to be applied in various real problems such as virtual counseling, stress detection, lie detection, and e-commerce.",2572-7621,978-1-7281-0834-6,10.1109/R10-HTC47129.2019.9042442,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042442,fuzzy emotion;facial expression;facial components;semantic features;linguistic features,,emotion recognition;face recognition;feature extraction;fuzzy set theory,facial expression analysis;human emotions;emotion expression;extended Cohn Kanade facial expression dataset;fuzzy emotion recognition framework;fuzzy emotion classification;linguistic facial component recognition;semantic-linguistic facial features extraction;Indonesian mixed emotion dataset;IMED,,,25.0,,,,,IEEE,IEEE Conferences
345,Cross-Domain Visual Exploration of Academic Corpora via the Latent Meaning of User-Authored Keywords,A. Benito-Santos; R. Therón Sánchez,"Department of Computer Science and Automation, Visual Analytics and Information Visualization Group, University of Salamanca, Salamanca, Spain; Department of Computer Science and Automation, Visual Analytics and Information Visualization Group, University of Salamanca, Salamanca, Spain",IEEE Access,,2019,7,,98144,98160,"Nowadays, scholars dedicate a substantial amount of their work to the querying and browsing of increasingly large collections of research papers on the Internet. In parallel, the recent surge of novel interdisciplinary approaches in science requires scholars to acquire competencies in new fields for which they may lack the necessary vocabulary to formulate adequate queries. This problem, together with the issue of information overload, poses new challenges in the fields of natural language processing (NLP) and visualization design that call for a rapid response from the scientific community. In this respect, we report on a novel visualization scheme that enables the exploration of research paper collections via the analysis of semantic proximity relationships found in author-assigned keywords. Our proposal replaces traditional string queries with a bag-of-words (BoW) extracted from a user-generated auxiliary corpus that captures the intentionality of the research. Continuing along the lines established by other authors in the fields of literature-based discovery (LBD), NLP, and visual analytics (VA), we combine novel advances in the fields of NLP with visual network analysis techniques to offer scholars a perspective of the target corpus that better fits their research interests. To highlight the advantages of our proposal, we conduct two experiments employing a collection of visualization research papers and an auxiliary cross-domain BoW. Here, we showcase how our visualization can be used to maximize the effectiveness of a browsing session by enhancing the language acquisition task, which allows for effectively extracting knowledge that is in line with the users' previous expectations.",2169-3536,,10.1109/ACCESS.2019.2929754,Ministerio de Economía y Competitividad; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8766090,Academic corpora;digital humanities;document exploration;human-computer interaction;knowledge elicitation;latent semantic analysis;literature-based discovery;visualization,Semantics;Task analysis;Analytical models;Data visualization;Visual analytics;Natural language processing,data mining;data visualisation;Internet;natural language processing;query processing;text analysis,language acquisition task;browsing session;auxiliary cross-domain BoW;visualization research papers;target corpus;visual network analysis techniques;user-generated;bag-of-words;author-assigned keywords;semantic proximity relationships;novel visualization scheme;scientific community;NLP;natural language processing;information overload;user-authored keywords;latent meaning;academic corpora;cross-domain visual exploration,,2.0,64.0,CCBY,,,,IEEE,IEEE Journals
348,An Automated Live Forensic and Postmortem Analysis Tool for Bitcoin on Windows Systems,S. Zollner; K. R. Choo; N. Le-Khac,"Bavarian Police Institute for Further Education, Training Institute of the Bavarian Police, Ainring, Germany; Department of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA; School of Computer Science, University College Dublin, Dublin 4, Ireland",IEEE Access,,2019,7,,158250,158263,"Bitcoin is popular not only with consumers, but also with cybercriminals (e.g., in ransomware and online extortion, and commercial online child exploitation). Given the potential of Bitcoin to be involved in a criminal investigation, the need to have an up-to-date and in-depth understanding on the forensic acquisition and analysis of Bitcoins is crucial. However, there has been limited forensic research of Bitcoin in the literature. The general focus of existing research is on postmortem analysis of specific locations (e.g. wallets on mobile devices), rather than a forensic approach that combines live data forensics and postmortem analysis to facilitate the identification, acquisition, and analysis of forensic traces relating to the use of Bitcoins on a system. Hence, the latter is the focus of this paper where we present an open source tool for live forensic and postmortem analysing automatically. Using this open source tool, we describe a list of target artifacts that can be obtained from a forensic investigation of popular Bitcoin clients and Web Wallets on different web browsers installed on Windows 7 and Windows 10 platforms.",2169-3536,,10.1109/ACCESS.2019.2948774,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8878085,Bitcoin forensics;bitcoin web wallet;cryptocurrency forensics;automated forensic process;digital forensics,Bitcoin;Forensics;Microsoft Windows;Public key;Cryptographic hash function;Browsers,computer crime;cryptocurrencies;digital forensics;Internet;Microsoft Windows (operating systems);online front-ends;public domain software,automated live forensic;postmortem analysis;Windows systems;forensic acquisition;forensic research;live data forensics;forensic traces;open source tool;Bitcoin clients;Web Wallets,,1.0,14.0,CCBY,,,,IEEE,IEEE Journals
349,Artificial Intelligence in Use by Multimodal Fusion,E. P. Blasch; U. Majumder; T. Rovito; A. K. Raz,"AFOSR,Rome,NY,13441; Air Force Research Laboratory,Rome,NY,13441; Air Force Research Laboratory,Dayton,OH,45433; Purdue University,West Lafayette,IN,47906",2019 22th International Conference on Information Fusion (FUSION),,2019,,,1,8,"The explosion of artificial intelligence (AI) methods present many opportunities for future technology, while at the same time, there are some unresolved challenges. One example is the ability to explain, reproduce, and leverage AI methods for practical applications. AI, along with many other pattern recognition and information fusion techniques can augment a user in decision making. However, when difficult decisions arise, contextual knowledge can help discern the merits of the results. In this paper, we highlight the ability of AI to support “usable” man-machine support as for methods that support data at rest, data in use, and data in motion (RUM). An example is shown for deep multi modal image fusion in support of simultaneous tracking and identification.",,978-0-9964527-8-6,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9011267,High-Level Information Fusion;Situation Assessment;Dynamic Data Driven Applications Systems;Video analysis,Data models;Cognition;Context modeling;Machine learning;Computational modeling;Sensors,artificial intelligence;decision making;image fusion;pattern recognition,artificial intelligence methods;AI methods;pattern recognition;information fusion techniques;decision making;deep multimodal image fusion;man-machine support,,,65.0,,,,,IEEE,IEEE Conferences
357,Towards Model-Driven Self-Explanation for Autonomous Decision-Making Systems,O. Reynolds,Aston University,2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C),,2019,,,624,628,"The ability for systems to make decisions by themselves is increasing with advances in different areas of AI such as machine learning and optimisation techniques for autonomous systems among other. Humans are handing over more decisions to systems that provide no explanations for their judgements unless they are enabled explicitly in their design. Trust based on a program being well written and tested correctly is not appropriate for AI-based autonomous systems. Unlike traditional software, this new software increasingly exhibit emergent behaviours making it unpredictable due to unexpected situations. Self-explanation is sometimes implemented, tracking decisions to give explanations to users. A more consistent, proven approach to self-explanation would be needed for making trustable systems. The paper proposes a research agenda to define an architecture to enable self-explanation for autonomous decision-making systems. The approach will be model-driven to facilitate reuse, the rapid development of tools and suitable abstractions for demonstrating concepts. The architecture will be informed by existing research in provenance ontology and model version research. The evaluation of the architecture is expected to be done using two case studies. The first will implement self-explanation as a primary concern in the building of a system. The second case will attempt to fit self-explanation to an existing system.",,978-1-7281-5125-0,10.1109/MODELS-C.2019.00095,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8904475,"self-explanation, model-driven, autonomous, decision-making",,decision making;learning (artificial intelligence);ontologies (artificial intelligence);program testing;trusted computing,tracking decisions;trustable systems;autonomous decision-making systems;provenance ontology;machine learning;AI-based autonomous systems;model-driven self-explanation;program testing,,,17.0,,,,,IEEE,IEEE Conferences
359,An Algorithm for Early Detection of Sepsis Using Traditional Statistical Regression Modeling,R. Pawar; J. Bone; J. M. Ansermino; M. Görges,"The University of British Columbia,Vancouver,Canada; The University of British Columbia,Vancouver,Canada; The University of British Columbia,Vancouver,Canada; The University of British Columbia,Vancouver,Canada",2019 Computing in Cardiology (CinC),,2019,,,Page 1,Page 4,"Sepsis is the final common pathway for many infections, whereby the body's immune response leads to organ failure, and eventually death. It is associated with high mortality rates and, if survived, significant morbidity. Early detection is imperative to improve outcomes. Yet, there is also a need to avoid a high false alarm rate. The aim of this study was to develop and evaluate a simple algorithm for early sepsis detection.Significant missing data were encountered in the dataset, which were forward-filled or substituted with population means. Clinically relevant variable combinations were added along with transformation features including dichotomization, z-scores, first derivative, and changes from baseline. A logistic regression model was used to identify candidate features and build the overall risk score function for prediction.The final candidate score had areas under the receiver operating characteristic curve of 0.747, 0.760, and 0.783 for the three test data sets. It had accuracies of 0.795, 0.889, 0.815, respectively, and an overall utility score for the full test set of 0.249 using a cutoff of 0.024.Evaluation indicated significant potential for further optimization, including reduction of false-positive predictions. Adding features capturing change over time is expected to provide scope for further investigation.",2325-887X,978-1-7281-6936-1,10.23919/CinC49843.2019.9005699,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005699,,Logistics;Heart rate;Predictive models;Biomedical monitoring;Indexes;Sociology,biological organs;data analysis;diseases;feature extraction;medical computing;patient diagnosis;regression analysis,statistical regression modeling;early sepsis detection;transformation features;logistic regression model;risk score function,,,18.0,,,,,IEEE,IEEE Conferences
360,Credible Agent-Based Simulation – An Illusion or Only A Step Awayƒ,B. S. Onggo; L. Yilmaz; F. Klügl; T. Terano; C. M. Macal,"University of Southampton,Centre for Operational Research,Southampton,UK,SO14 1BJ; Auburn University,Computer Science and Software Engineering,Auburn,AL,USA,36849; Örebro University,School of Science and Technology,Örebro,SWEDEN,70182; Chiba University of Commerce,Ichikawa, Chiba,JAPAN,272-8512; Argonne National Laboratory,Decision and Infrastructure Sciences,Argonne,IL,USA,60439",2019 Winter Simulation Conference (WSC),,2019,,,273,284,"During the World Café activity at the 2018 Winter Simulation Conference, we discussed Agent-based Simulation (ABS) credibility. The topic is important since credible ABS leads to an impact on society whereby ABS is implemented by users and they can benefit from it. This paper presents the perspective of three academic panelists and a practitioner on the credibility of ABS. The discussion reveals that the increasing use of ABS models to explain social phenomena or systems that exhibit emergent behavior pose a challenge for model credibility. Several points and suggestions are raised by the panelists, including evaluating ABS model credibility via its explanatory power, the multi-dimensionality of credibility and the role of software engineering approaches.",1558-4305,978-1-7281-3283-9,10.1109/WSC40007.2019.9004716,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9004716,,Adaptation models;Computational modeling;Predictive models;Software engineering;Industries;Cognition,software agents;software engineering,ABS model credibility;credible agent-based simulation;agent-based simulation credibility;software engineering approach,,,19.0,,,,,IEEE,IEEE Conferences
361,Knowledge Graphs and Knowledge Networks: The Story in Brief,A. Sheth; S. Padhee; A. Gyrard,University of South Carolina; Wright State University; Wright State University,IEEE Internet Computing,,2019,23,4.0,67,75,"Knowledge Graphs (KGs) represent real-world noisy raw information in a structured form, capturing relationships between entities. However, for dynamic real-world applications such as social networks, recommender systems, computational biology, relational knowledge representation has emerged as a challenging research problem where there is a need to represent the changing nodes, attributes, and edges over time. The evolution of search engine responses to user queries in the last few years is partly because of the role of KGs such as Google KG. KGs are significantly contributing to various AI applications from link prediction, entity relations prediction, node classification to recommendation and question answering systems. This article is an attempt to summarize the journey of KG for AI.",1941-0131,,10.1109/MIC.2019.2928449,kHealth NIH; Hazards SEES NSF; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8874979,,Photonic crystal fibers;Optical fiber losses;Propagation losses;Optical polarization;Network theory (graphs),graph theory;knowledge representation;query processing;question answering (information retrieval);recommender systems;search engines;social networking (online),knowledge networks;KGs;real-world noisy raw information;structured form;social networks;recommender systems;computational biology;relational knowledge representation;search engine responses;AI applications;entity relations prediction;question answering systems;knowledge graphs;link prediction;node classification,,,15.0,,,,,IEEE,IEEE Magazines
362,Motivations and Risks of Machine Ethics,S. Cave; R. Nyrup; K. Vold; A. Weller,"Leverhulme Centre for the Future of Intelligence, University of Cambridge, Cambridge, U.K.; Leverhulme Centre for the Future of Intelligence, University of Cambridge, Cambridge, U.K.; Leverhulme Centre for the Future of Intelligence, Faculty of Philosophy, University of Cambridge, Cambridge, U.K.; Leverhulme Centre for the Future of Intelligence, University of Cambridge, Cambridge, U.K.",Proceedings of the IEEE,,2019,107,3.0,562,574,"This paper surveys reasons for and against pursuing the field of machine ethics, understood as research aiming to build “ethical machines.” We clarify the nature of this goal, why it is worth pursuing, and the risks involved in its pursuit. First, we survey and clarify some of the philosophical issues surrounding the concept of an “ethical machine” and the aims of machine ethics. Second, we argue that while there are good prima facie reasons for pursuing machine ethics, including the potential to improve the ethical alignment of both humans and machines, there are also potential risks that must be considered. Third, we survey these potential risks and point to where research should be devoted to clarifying and managing potential risks. We conclude by making some recommendations about the questions that future work could address.",1558-2256,,10.1109/JPROC.2018.2865996,"Darwin College, University of Cambridge; Leverhulme Trust; Engineering and Physical Sciences Research Council; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456834,Ethical alignment;ethical reasoning;machine agency;machine ethics,Ethics;Social implications of technology;Biomedical monitoring;Biosensors;Stakeholders;Complexity theory;Cognitive processes,cognitive systems;ethical aspects;man-machine systems;philosophical aspects;risk management,ethical machine;machine ethics;potential risk management;philosophical issues;human-machine ethical alignment,,1.0,75.0,,,,,IEEE,IEEE Journals
363,SUDS: System for uncertainty decision support,M. de Boer; B. Nouwt; M. van Bekkum,"Data Science, TNO, The Hague, The Netherlands; Data Science, TNO, The Hague, The Netherlands; Data Science, TNO, The Hague, The Netherlands",2017 IEEE International Conference on Big Data (Big Data),,2017,,,2795,2803,"Big Data Applications (BDAs) are used to support a decision making process, but their developers and users are often unaware of the exact uncertainties that underlie the output of BDAs. In this paper we present a System for Uncertainty Decision Support (SUDS) as a generic plug-in for BDAs that aims to give both developers and users more insight in the location, types and propagation of uncertainties within a BDA. To achieve this, SUDS receives uncertainty information directly from the BDA and provides a separate front-end where the uncertainties can be explored and queried. SUDS combines state of the art methods in knowledge modelling, natural language processing and big data architectures.",,978-1-5386-2715-0,10.1109/BigData.2017.8258246,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258246,big data;uncertainty;semantics;ontology,Uncertainty;Data visualization;Ontologies;Big Data applications;Data models;Business,Big Data;decision making;decision support systems;natural language processing;uncertainty handling,SUDS;Big Data Applications;BDAs;decision making process;uncertainty information;big data architectures;system for uncertainty decision support;uncertainty propagation;knowledge modelling;natural language processing,,,35.0,,,,,IEEE,IEEE Conferences
372,Assessing Artificial Intelligence for Humanity: Will AI be the Our Biggest Ever Advance ? or the Biggest Threat [Opinion],A. Nowak; P. Lukowicz; P. Horodecki,"University of Warsaw, Poland; Technical University of Kaiserslautern, Germany; Gdańsk University of Technology, Poland",IEEE Technology and Society Magazine,,2018,37,4.0,26,34,"Recent rapid advancements in artificial intelligence (AI) are arguably the most important dimension of humanity's progress to date. As members of the human race, that is, homo sapiens, we are defined by our capacity for cognition. Until now, humans were the only species capable of higher cognitive functions. But today AI has advanced to a stage where on many cognitionrelated tasks it can match and even surpass the performance of humans. Examples include not only AI's spectacular successes in winning Go, chess, and other board games with humans, and in surpassing humans on fully defined world puzzles. But AI is also now achieving extremely high efficiency in practical applications such as speech and object recognition, self-driving cars, intelligent tutoring systems, efficient decision support systems, and in the capacity to detect patterns in Big Data and in constructing accurate models of social behavior. Thus, for the first time in history, we must ask ourselves: “has our monopoly on intelligence, however defined, been challenged?”",1937-416X,,10.1109/MTS.2018.2876105,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8558761,,Artificial intelligence;Human factors;Decision making;Cognition,artificial intelligence;cognition,cognitive functions;intelligent tutoring systems;fully defined world puzzles;AI's spectacular successes;human race;humanity;artificial intelligence,,4.0,34.0,,,,,IEEE,IEEE Magazines
384,Ender's game redux [computer games],M. Macedonia,"Georgia Tech. Res. Inst., Atlanta, GA, USA",Computer,,2005,38,2.0,95,97,"In this article the author explains the context of his brother's e-mail, who is a US Army surgeon in Iraq. The e-mail concisely summarizes the convergence of entertainment technology and military training. He also describes the influence of game technology in his training.",1558-0814,,10.1109/MC.2005.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1401785,,Game theory;Military computing;Computational modeling;Computer simulation;Eyes;Personnel;Production;Instruments;Costs;Personal communication networks,computer games;military computing;computer based training;digital simulation;Internet;entertainment,entertainment technology;military training;computer game,,,,,,,,IEEE,IEEE Magazines
391,Challenges Associated with Generative Forms of Multimedia Content (Keynote Talk),A. F. Smeaton,"Insight Centre for Data Analytics, Dublin City University Glasnevin, Dublin 9, Ireland",2019 International Conference on Content-Based Multimedia Indexing (CBMI),,2019,,,1,3,"This short paper presents what is currently the main challenge associated with using Generative Adversarial Networks (GANs) to generate visual media. That challenge is around automatically determining the authenticity of an image/video, which is of increasing importance as fake videos and images start to proliferate on social media, especially when associated with political campaigning. The paper introduces GANs, outlines how they are used to generate visual media and summarises this major challenge.",1949-3991,978-1-7281-4673-7,10.1109/CBMI.2019.8877439,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877439,Generative multimedia;GANs,,multimedia computing;neural nets;social networking (online);video signal processing,generative forms;multimedia content;generative adversarial networks;GANs;visual media;image authenticity;fake videos;social media;political campaigning;video authenticity,,,14.0,,,,,IEEE,IEEE Conferences
